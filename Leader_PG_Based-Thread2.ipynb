{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leader - Policy-Gradient Based\n",
    "\n",
    "In this notebook, we develop a policy-gradient based leader agent. The leader learns a policy to lead followers via a target zone to gather more apples. We modify the Crossing environment so that it recognizes multiple roles in a team of agents - leader and followers.\n",
    "\n",
    "We discover that tradition RL (based on policy-gradient and Q-learning) has severe limitations in strategic decision making. Both the follower agents with partial observation of the game space as well as leader agent with complete observation of the game space find it hard to overcome the problems of local optima and sparce reward.\n",
    "\n",
    "<img src=\"images/Leader_PG.png\" width=\"600\">\n",
    "\n",
    "Specifically, even though a much larger food pile exist on the right side of the river, both the leader agent and the follower agents remain fixated on the smaller food pile on the left side of the river closer to them. They fail completely to recognize the strategic fact that there is not enough apples in this smaller pile for the large number of agents competing for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from xteams_env import CrossingEnv\n",
    "from xteams_model3 import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenarios\n",
    "\n",
    "The agent models stored in various folders have been trained under different training scenarios based on:\n",
    "* team organization: 1 team of 10 agents, 2 teams of 5 agents\n",
    "* team roles: leader and followers, followers only\n",
    "* starting temps\n",
    "* team cultures: pacifist, cooperative\n",
    "* num of game steps per episode: 300, 600, 1200\n",
    "* leader reward: number of apples in the target zone, team reward gathered\n",
    "\n",
    "## 1 team of 10 Pacifist agents \n",
    "\n",
    "### map = food_d37\n",
    "\n",
    "| Scenario |      Temp     |  Game Steps |\n",
    "|----------|---------|------|\n",
    "| 1 |  0.4 | 300 |\n",
    "| 2 |  0.8 | 300 |\n",
    "| 3 |  1.0 | 300 |\n",
    "| 4 |  1.0 | 600 |\n",
    "| 5 |  1.25 | 300 |\n",
    "| 6 |  1.25 | 600 |\n",
    "| 7 |  1.5 | 300 |\n",
    "| 8 |  1.5 | 600 |\n",
    "| 9 |  1.5 | 1200 |\n",
    "| 10 |  2.0 | 300 |\n",
    "| 11 |  2.0 | 600 |\n",
    "| 12 |  2.0 | 1200 |\n",
    "| 13 |  4.0 | 300 |\n",
    "| 14 |  4.0 | 600 |\n",
    "| 15 |  4.0 | 1200 |\n",
    "| 16 |  8.0 | 300 |\n",
    "| 17 |  8.0 | 600 |\n",
    "| 18 |  8.0 | 1200 |\n",
    "\n",
    "### map = food_d37_river_w1_d25\n",
    "\n",
    "| Scenario |      Temp     |  Game Steps |\n",
    "|----------|---------|------|\n",
    "| 19 |  1.0 | 300 |\n",
    "| 20 |  1.25 | 300 |\n",
    "| 21 |  2.0 | 300 |\n",
    "| 22 |  4.0 | 300 |\n",
    "\n",
    "## 2 team of 5 agents each \n",
    "\n",
    "### No Leader in both teams (Start Temp = 1.25; Game Steps/Episode = 300)\n",
    "\n",
    "| Scenario | Cultures | Map  |\n",
    "|------|------|------|\n",
    "| 23 | Pac vs Pac  | food_d37 |\n",
    "| 24 | Pac vs Coop | food_d37_river_w1_d25 |\n",
    "| 25 | Pac vs Coop | food_d37_river_w1_d25 |\n",
    "\n",
    "\n",
    "### Team 1 (Leader + Followers) Team 2 (Leader-less)\n",
    "\n",
    "| Scenario | Temp | Cultures | Map  | Leader type | Leader Metric |\n",
    "|------|------|------|------|------|------|\n",
    "| 26 | 1.25 | Pac vs Coop  | food_d37_river_w1_d25 | crawler | team_reward |\n",
    "| 27 | 2.0 | Pac vs Coop | food_d37_river_w1_d25 | crawler | team_reward |\n",
    "| 28 | 1.5 | Pac vs Coop | food_d37_river_w1_d25 | drone | team_reward |\n",
    "| 29 | 1.5 | Pac vs Pac  | food_d37_river_w1_d25 | drone | team_reward |\n",
    "| 30 | 2.0 | Pac vs Pac  | food_d37_river_w1_d25 | drone | team_reward |\n",
    "| 31 | 2.0 | Pac vs Pac  | food_d37_river_w1_d25 | drone | apples |\n",
    "| 32 | 2.0 | Pac vs Pac  | food_d37_river_w1_d25 | drone | apples + team_reward |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    # Agents trained in map = food_d37\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/',   # scenario=1\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/',   # scenario=2\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=3\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/',   # scenario=4\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/',   # scenario=5\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/',   # scenario=6\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/',   # scenario=7\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/',   # scenario=8\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/',   # scenario=9\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/',   # scenario=10\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/',   # scenario=11\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/',   # scenario=12\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/',   # scenario=13\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/',   # scenario=14\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/',   # scenario=15\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/',   # scenario=16\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/',   # scenario=17\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/',   # scenario=18\n",
    "\n",
    "    # Agents trained in map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=19\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=20 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=21\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\",   # scenario=22\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37\n",
    "    \"models/2T-5L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/\",     # scenario=23\n",
    "    \"models/2T-5L/baseline/food_d37/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=24\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "    # Team Viking (Pacifist w/o leader) vs Team Frank (Cooperative)\n",
    "    \"models/2T-5L/baseline/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=25\n",
    "    \n",
    "    # Team Viking (Pacifist w/ leader) vs Team Frank (No Leader)\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\", # scenario=26\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t2.0_rp-1.0_300gs/\",   # scenario=27\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_coop/t1.5_rp-1.0_300gs/\",   # scenario=28\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t1.5_rp-1.0_300gs/\",   # scenario=29 \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t2.0_rp-1.0_300gs/\",   # scenario=30  \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples/t2.0_rp-1.0_300gs/\",   # scenario=31\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples_teamreward/t2.0_rp-1.0_300gs/\"   # scenario=32\n",
    "    \n",
    "]\n",
    "\n",
    "# Parameter sets pertaining to the trained models in the folders above (not used in the code)\n",
    "parameters =[ \n",
    "        # Temperature for explore/exploit; penalty per step in river; game steps per episode\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37\n",
    "            {'temp_start':0.4, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':0.8, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37_river_w1_d25    \n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "        # Team Viking (Pacifist w/ leader) vs Team Frank (Cooperative)\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},    \n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},   \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300} \n",
    "            ]\n",
    "\n",
    "print (len(parameters))\n",
    "print (len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teams + Agents --> Env\n",
    "\n",
    "The new Crossing environment accepts a list of parameters of the participating agents and teams.\n",
    "\n",
    "**Team parameters**\n",
    "* name - name of team\n",
    "* color - color of the team (can be over-ridden by agent colors)\n",
    "* culture - use to calculate and dole out team rewards to agents during training\n",
    "* roles - a list of possible roles in the team\n",
    "* target_zone - an area where agents are rewarded to assemble within\n",
    "* banned_zone - an area where agents are penalized for staying within\n",
    "\n",
    "**Agent parameters**\n",
    "* id - id of the agent\n",
    "* team - the team the agent is attached to\n",
    "* color - color of the agent when rendering (can over-ride default team color\n",
    "* type - 'crawler' or 'drone' - affecting the agent's policy NN, action and obs spaces\n",
    "* role - the agent's role in the team\n",
    "\n",
    "The code below plays a game using the new environment using Agents previously trained in Crossing_Baseline. Agent 0 of Team Viking has the 'leader' role. \n",
    "\n",
    "If an agent is a 'leader', the Environment sets the target_zone for its team based on the agent's location. During rendering, the target zone is represented by a green box.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone starts at (50, 18) in map food_d37_river_w1_d25_v4.\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 4\n",
      "Agent0 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent1 reward is 0\n",
      "Agent1 aggressiveness is 0.01\n",
      "US agents hit = 1\n",
      "THEM agents hit = 0\n",
      "Agent2 reward is 7\n",
      "Agent2 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent3 reward is 0\n",
      "Agent3 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent4 reward is 0\n",
      "Agent5 reward is 34\n",
      "Agent5 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent6 reward is 0\n",
      "Agent6 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent7 reward is 0\n",
      "Agent7 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent8 reward is 4\n",
      "Agent8 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent9 reward is 5\n",
      "Agent9 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Total rewards gathered = 54\n",
      "Av. rewards per agent = 5.40\n",
      "Num laser fired = 6\n",
      "Total US Hit (friendly fire) = 1\n",
      "Total THEM Hit = 0\n",
      "friendly fire (%) = 1.000\n",
      "Num agents gathering from 2nd food pile: 1\n",
      "\n",
      "Statistics by Team\n",
      "===================\n",
      "Team Vikings has total reward of 11\n",
      "Team Franks has total reward of 43\n",
      "Dominating Team: Franks\n",
      "Team dominance: 3.91x\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "maps = [\n",
    "    \"food_d37_river_w1_d25\",\n",
    "    \"food_d37_river_w1_d25_v2\",\n",
    "    \"food_d37_river_w1_d25_v3\",\n",
    "    \"food_d37_river_w1_d25_v4\",\n",
    "]\n",
    "starts = [\n",
    "    (3,9),\n",
    "    (3,18),\n",
    "    (20,9),\n",
    "    (20,18),\n",
    "    (50,9),\n",
    "    (50,18),\n",
    "]\n",
    "\n",
    "droneleader_start = starts[random.randint(0,len(starts)-1)]\n",
    "map_name = maps[random.randint(0,len(maps)-1)]\n",
    "\n",
    "print(\"Drone starts at {} in map {}.\".format(droneleader_start, map_name))\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "scenario = 31\n",
    "dir_name = folders[scenario-1]\n",
    "parameter = parameters[scenario-1]\n",
    "episodes = 2000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "# Scenario 26-32\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},        \n",
    "        {'name': 'Franks', 'color': 'red',\n",
    "         'culture': {'name':'pacifist','laser_penalty':-1.0},\n",
    "         # 'culture': {'name':'cooperative','coop_factor':5.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "# Scenario 1-25\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist','laser_penalty':-1.0},\n",
    "         # 'culture': {'name':'cooperative','coop_factor':5.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "        {'name': 'Franks', 'color': 'red',\n",
    "         'culture': {'name':'pacifist','laser_penalty':-1.0},\n",
    "         # 'culture': {'name':'cooperative','coop_factor':5.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',     \\\n",
    "         'role': 'follower', 'start': (1,7)},  # Use a different color for Leader\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9)},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8)},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7)},\n",
    "        # Leader of Team Viking is a drone and has a different color\n",
    "        {'id': 4, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': droneleader_start},\n",
    "        # Leader of Team Viking is a crawler and has a different color\n",
    "        #{'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "        # 'role': 'follower', 'start': (3,9)},\n",
    "    \n",
    "        {'id': 5, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,1)},\n",
    "        {'id': 6, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,3)},\n",
    "        {'id': 7, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,2)},\n",
    "        {'id': 8, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,1)},\n",
    "        {'id': 9, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,3)}\n",
    "]\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 300\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, 0)\n",
    "                elif agents_params[i]['type'] is 'drone':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, 0)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0], agents[1], agents[2], agents[3], agents[4]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:5]]))\n",
    "# Team Franks\n",
    "teams.append(Team(name=teams_params[1]['name'],color=teams_params[1]['color'],   \\\n",
    "                  culture=teams_params[1]['culture'], roles=teams_params[1]['roles'], \\\n",
    "                  agent_policies=[agents[5], agents[6], agents[7], agents[8], agents[9]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[5:10]]))\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/frame))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_ai_agents))\n",
    "print ('Num laser fired = {}'.format(total_tags))\n",
    "print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_team = None\n",
    "top_team_reward = 0\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    if team.name is not 'Crazies':\n",
    "        reward = sum(team.sum_rewards())\n",
    "        print ('Team {} has total reward of {}'.format(team.name, reward))\n",
    "                           \n",
    "        if reward > top_team_reward:   # Keep track of dominating team\n",
    "            top_team_reward = reward\n",
    "            top_team = team.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(teams) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_team))\n",
    "    dominance = top_team_reward/((total_rewards-top_team_reward+1.1e-7)/(len(teams)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (agents[4].apples_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0bb362c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.sum(env.initial_food[30:40, 20:30]))\n",
    "\n",
    "plt.imshow(env.initial_food)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 60)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(100, 60, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 100, 60])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "100\n",
      "60\n",
      "[(35, 24), (22, 39), (35, 27), (36, 25), (33, 25), (22, 21), (34, 25), (20, 21), (35, 25), (74, 39)]\n",
      "['Vikings', 'Franks']\n",
      "[0, 0, 0, 0, None, 0, 0, 0, 0, 0]\n",
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0b40a5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ae150f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADKCAYAAACFWKrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAC6JJREFUeJzt3W+sZPVdx/H3x71ghRJYZFO3u6SAaZqoiQI3hFYkjVSk2LDVNGYbG2lrsmkUBRPTrDZpGpM+qP/in5iaFVFUUoiUKmmoBW2rPmFl2S5/dpfCFrHsusAqBqo+oMi3D+bc9nJ3Zu652zlz74+8X8nkzpzzm5lPfnv43DO/mbmkqpAkteO71juAJGltLG5JaozFLUmNsbglqTEWtyQ1xuKWpMZY3JLUGItbkhpjcUtSYxaGeNAkfh1TktaoqtJnnGfcktQYi1uSGmNxS1JjLG5Jakyv4k5yTZKvJDmSZPfQoSRJk2W1v8edZBPwOPATwFHgAeC9VXVoyn38VIkkrdEsP1VyGXCkqp6sqpeA24Ed30k4SdKp61Pc24Cnl90+2m2TJK2DmX0BJ8kuYNesHk+SNF6f4j4GnL/s9vZu26tU1R5gD7jGLUlD6rNU8gDw5iQXJjkd2AncPWwsSdIkq55xV9XLSW4APg9sAm6pqoODJ5MkjbXqxwFP6UFdKpGkNfOPTEnSa5TFLUmNsbglqTEWtyQ1xuKWpMZY3JLUGItbkhpjcUtSYyxuSWqMxS1JjbG4JakxFrckNcbilqTGWNyS1BiLW5IaY3FLUmMsbklqjMUtSY2xuCWpMRa3JDXG4pakxljcktQYi1uSGmNxS1JjLG5JasyqxZ3k/CRfTHIoycEkN84jmCRpvFTV9AHJVmBrVe1PchbwIPDuqjo05T7TH1SSdJKqSp9xq55xV9XxqtrfXf86cBjY9p3FkySdqoW1DE5yAXAxsHfMvl3ArpmkkiRNtOpSybcGJq8H/gn4eFXdtcpYl0okaY1mtlQCkOQ04NPAbauVtiRpWH3enAxwK/B8Vd3U60E945akNet7xt2nuK8A/gV4BHil2/wbVXXPlPtY3JK0RjMr7lNhcUvS2s10jVuStHFY3JLUmEGK+9JLL6Wqmr9I0kbkGbckNcbilqTGWNyS1BiLW5IaY3FLUmMsbklqjMUtSY2xuCWpMRa3JDXG4pakxljcktQYi1uSGmNxS1JjLG5JaozFLUmNsbglqTEWtyQ1xuKWpMZY3JLUGItbkhpjcUtSY3oXd5JNSb6c5LNDBpIkTbeWM+4bgcNDBZEk9dOruJNsB34KuHnYOJKk1fQ94/594MPAKwNmkST1sGpxJ3kX8FxVPbjKuF1J9iXZd+LEiZkFlCS9Wp8z7h8FrkvyFHA78ONJ/nrloKraU1WLVbW4ZcuWGceUJC1Ztbir6terantVXQDsBL5QVe8bPJkkaSw/xy1JjVlYy+Cq+hLwpUGSSJJ68YxbkhpjcUtSYyxuSWqMxS1JjbG4JakxFrckNcbilqTGWNyS1BiLW5IaY3FLUmMsbklqjMUtSY2xuCWpMRa3JDXG4pakxljcktQYi1uSGmNxS1JjLG5JaozFLUmNsbglqTEWtyQ1xuKWpMZY3JLUGItbkhrTq7iTnJPkziSPJTmc5K1DB5MkjbfQc9wfAH9fVe9JcjpwxoCZJElTrFrcSc4GrgTeD1BVLwEvDRtLkjRJn6WSC4ETwJ8n+XKSm5OcuXJQkl1J9iXZd+LEiZkHlSSN9CnuBeAS4JNVdTHwv8DulYOqak9VLVbV4pYtW2YcU5K0pE9xHwWOVtXe7vadjIpckrQOVi3uqnoGeDrJW7pNVwGHBk0lSZqo76dKfhm4rftEyZPAB4aLJEmapldxV9UBYHHgLJKkHvzmpCQ1xuKWpMZY3JLUGItbkhpjcUtSYyxuSWqMxS1JjbG4JakxFrckNcbilqTGWNyS1BiLW5IaY3FLUmMsbklqjMUtSY2xuCWpMRa3JDXG4pakxljcktSYVNXsHzSZ/YNK0mtcVaXPOM+4JakxFrckNcbilqTGWNyS1JhexZ3kV5McTPJokk8led3QwSRJ461a3Em2Ab8CLFbVDwGbgJ1DB5Mkjdd3qWQB+J4kC8AZwH8MF0mSNM2qxV1Vx4DfAb4GHAdeqKp7hw4mSRqvz1LJZmAHcCHwRuDMJO8bM25Xkn1J9s0+piRpSZ+lkncA/1ZVJ6rqG8BdwNtWDqqqPVW1WFWLsw4pSfq2PsX9NeDyJGckCXAVcHjYWJKkSfqsce8F7gT2A49099kzcC5J0gT+kSlJ2iD8I1OS9BplcUtSYyxuSWqMxS1JjbG4JakxFrckNcbilqTGWNyS1BiLW5IaY3FLUmMsbklqjMUtSY2xuCWpMRa3JDXG4pakxljcktQYi1uSGmNxS1JjLG5JaozFLUmNWRjocf8T+Pcp+8/rxmx05pydFjKCOWfNnP29qe/AQf4v76s+abKvqhbn/sRrZM7ZaSEjmHPWzDkMl0okqTEWtyQ1Zr2Ke886Pe9amXN2WsgI5pw1cw5gXda4JUmnzqUSSWrMoMWd5JokX0lyJMnuMfu/O8kd3f69SS4YMs+EjOcn+WKSQ0kOJrlxzJi3J3khyYHu8tF55+xyPJXkkS7DvjH7k+QPu/l8OMklc873lmVzdCDJi0luWjFmXeYyyS1Jnkvy6LJt5ya5L8kT3c/NE+57fTfmiSTXr0PO307yWPdv+pkk50y479TjYw45P5bk2LJ/22sn3HdqL8wh5x3LMj6V5MCE+85tPtesqga5AJuArwIXAacDDwE/sGLMLwJ/0l3fCdwxVJ4pObcCl3TXzwIeH5Pz7cBn551tTNangPOm7L8W+BwQ4HJg7zpm3QQ8A7xpI8wlcCVwCfDosm2/Bezuru8GPjHmfucCT3Y/N3fXN88559XAQnf9E+Ny9jk+5pDzY8Cv9TgupvbC0DlX7P9d4KPrPZ9rvQx5xn0ZcKSqnqyql4DbgR0rxuwAbu2u3wlclSQDZjpJVR2vqv3d9a8Dh4Ft88wwQzuAv6yR+4FzkmxdpyxXAV+tqmlfxJqbqvpn4PkVm5cff7cC7x5z158E7quq56vqv4H7gGvmmbOq7q2ql7ub9wPbh3r+vibMZx99emFmpuXsuuZngU8N9fxDGbK4twFPL7t9lJML8VtjugPzBeB7B8w0VbdUczGwd8zutyZ5KMnnkvzgXIN9WwH3Jnkwya4x+/vM+bzsZPJ/EBthLgHeUFXHu+vPAG8YM2YjzSnABxm9qhpnteNjHm7olnRumbD0tJHm88eAZ6vqiQn7N8J8juWbk50krwc+DdxUVS+u2L2f0Uv+Hwb+CPjbeefrXFFVlwDvBH4pyZXrlGOqJKcD1wF/M2b3RpnLV6nRa+MN/RGrJB8BXgZumzBkvY+PTwLfD/wIcJzRMsRG9l6mn22v93xONGRxHwPOX3Z7e7dt7JgkC8DZwH8NmGmsJKcxKu3bququlfur6sWq+p/u+j3AaUnOm3NMqupY9/M54DOMXnYu12fO5+GdwP6qenbljo0yl51nl5aSup/PjRmzIeY0yfuBdwE/1/2SOUmP42NQVfVsVf1/Vb0C/OmE598o87kA/Axwx6Qx6z2f0wxZ3A8Ab05yYXcGthO4e8WYu4Gld+nfA3xh0kE5lG6d68+Aw1X1exPGfN/S2nuSyxjN21x/wSQ5M8lZS9cZvWH16IphdwM/33265HLghWVLAfM08UxmI8zlMsuPv+uBvxsz5vPA1Uk2dy/9r+62zU2Sa4APA9dV1f9NGNPn+BjUivdTfnrC8/fphXl4B/BYVR0dt3MjzOdUQ77zyehTDo8zehf5I92232R0AAK8jtHL6SPAvwIXzfvdWeAKRi+RHwYOdJdrgQ8BH+rG3AAcZPQO+P3A29Yh50Xd8z/UZVmaz+U5A/xxN9+PAIvrkPNMRkV89rJt6z6XjH6RHAe+wWhd9RcYvZ/yj8ATwD8A53ZjF4Gbl933g90xegT4wDrkPMJoXXjp+Fz6JNYbgXumHR9zzvlX3XH3MKMy3royZ3f7pF6YZ85u+18sHZPLxq7bfK714jcnJakxvjkpSY2xuCWpMRa3JDXG4pakxljcktQYi1uSGmNxS1JjLG5Jasw3Ab5fRk/yHS1rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ae13edd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ae06f5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 60, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0adff6e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ac37f5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ac2517b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ac3494a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ae0f0c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADKCAYAAACFWKrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAC4pJREFUeJzt3X/sXfVdx/Hny5aKMIQiyewK2cAsS9TErWkIm7iQMJEhWadZTBenuJk0ixLBZFmIi8tiYsz8FacxmoooGrIRGVOyMAfqov5DpXTlR1sGHeJoLTBlgRn/2JC3f9zT8eXb++Pccs+93w95PpKb77nnvM+573x6+vqe+zn3tqkqJEnt+K5VNyBJmo/BLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWrM5iEOmsSvY0rSnKoqfeq84pakxhjcktQYg1uSGmNwS1JjegV3kquTfCXJ0SQ3Dd2UJGmyzPr3uJNsAh4Dfhw4BtwPvL+qDk/Zx0+VSNKcFvmpkkuBo1X1RFV9C/gMsOvVNCdJOn19gns78NSa58e6dZKkFVjYF3CS7AH2LOp4kqTx+gT3ceCiNc8v7Na9QlXtBfaCc9ySNKQ+UyX3A29OcnGSLcBu4K5h25IkTTLziruqXkxyPfBFYBNwS1UdGrwzSdJYMz8OeFoHdapEkubmPzIlSa9RBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxM4M7yUVJvpTkcJJDSW5YRmOSpPFSVdMLkm3Atqo6kOQc4AHgvVV1eMo+0w8qSTpFVaVP3cwr7qo6UVUHuuVvAkeA7a+uPUnS6do8T3GSNwFvA/aN2bYH2LOQriRJE82cKvlOYfI64J+B36yqO2fUOlUiSXNa2FQJQJIzgM8Ct80KbUnSsPrcnAxwK/BcVd3Y66BecUvS3PpecfcJ7suBfwUeBl7qVv9aVd09ZR+DW5LmtLDgPh0GtyTNb6Fz3JKkjcPglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxvYM7yaYkX07y+SEbkiRNN88V9w3AkaEakST10yu4k1wI/CRw87DtSJJm6XvF/QfAR4GXBuxFktTDzOBOci3wbFU9MKNuT5L9SfYvrDtJ0ilSVdMLkt8Cfg54ETgT+F7gzqr6wJR9ph9UknSKqkqfupnB/Yri5ArgI1V17Yw6g1uS5tQ3uP0ctyQ1Zq4r7t4H9YpbkubmFbckvUYZ3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMb0Cu4k5yW5I8mjSY4kefvQjUmSxtvcs+5TwN9X1fuSbAHOGrAnSdIUqarpBcm5wEHgkppV/PI+veokSS+rqvSp6zNVcjHwdeAvknw5yc1Jzl5flGRPkv1J9s/ZqyRpDn2uuHcC9wE/WlX7knwKeKGqfn3KPl5xS9KcFnnFfQw4VlX7uud3ADtOtzFJ0qszM7ir6mngqSRv6VZdCRwetCtJ0kQzp0oAkrwVuBnYAjwBfLCqvjGl3qkSSZpT36mSXsE9L4Nbkua3yDluSdIGYnBLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1JhewZ3kV5McSvJIkk8nOXPoxiRJ480M7iTbgV8BdlbVDwObgN1DNyZJGq/vVMlm4HuSbAbOAv5zuJYkSdPMDO6qOg78LvA14ATwfFXdM3RjkqTx+kyVbAV2ARcDbwDOTvKBMXV7kuxPsn/xbUqSTuozVfIu4N+r6utV9W3gTuAd64uqam9V7ayqnYtuUpL0sj7B/TXgsiRnJQlwJXBk2LYkSZP0mePeB9wBHAAe7vbZO3BfkqQJUlWLP2iy+INK0mtcVaVPnd+clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYzYPdNz/Av5jyvYLupqNzj4Xp4UewT4XzT77e2PfwkH+l/eZL5rsr6qdS3/hOdnn4rTQI9jnotnnMJwqkaTGGNyS1JhVBffeFb3uvOxzcVroEexz0exzACuZ45YknT6nSiSpMYMGd5Krk3wlydEkN43Z/t1Jbu+270vypiH7mdDjRUm+lORwkkNJbhhTc0WS55Mc7B4fX3afXR9PJnm462H/mO1J8ofdeD6UZMeS+3vLmjE6mOSFJDeuq1nJWCa5JcmzSR5Zs+78JPcmebz7uXXCvtd1NY8nuW4Fff5Okke7P9PPJTlvwr5Tz48l9PmJJMfX/NleM2HfqbmwhD5vX9Pjk0kOTth3aeM5t6oa5AFsAr4KXAJsAR4EfnBdzS8Bf9ot7wZuH6qfKX1uA3Z0y+cAj43p8wrg88vubUyvTwIXTNl+DfAFIMBlwL4V9roJeBp440YYS+CdwA7gkTXrfhu4qVu+CfjkmP3OB57ofm7tlrcuuc+rgM3d8ifH9dnn/FhCn58APtLjvJiaC0P3uW777wEfX/V4zvsY8or7UuBoVT1RVd8CPgPsWlezC7i1W74DuDJJBuzpFFV1oqoOdMvfBI4A25fZwwLtAv6qRu4DzkuybUW9XAl8taqmfRFraarqX4Dn1q1ee/7dCrx3zK4/AdxbVc9V1TeAe4Grl9lnVd1TVS92T+8DLhzq9fuaMJ599MmFhZnWZ5c1PwN8eqjXH8qQwb0deGrN82OcGojfqelOzOeB7xuwp6m6qZq3AfvGbH57kgeTfCHJDy21sZcVcE+SB5LsGbO9z5gvy24m/4XYCGMJ8PqqOtEtPw28fkzNRhpTgA8xelc1zqzzYxmu76Z0bpkw9bSRxvPHgGeq6vEJ2zfCeI7lzclOktcBnwVurKoX1m0+wOgt/48AfwT87bL761xeVTuAdwO/nOSdK+pjqiRbgPcAfzNm80YZy1eo0XvjDf0RqyQfA14EbptQsurz40+AHwDeCpxgNA2xkb2f6Vfbqx7PiYYM7uPARWueX9itG1uTZDNwLvDfA/Y0VpIzGIX2bVV15/rtVfVCVf1Pt3w3cEaSC5bcJlV1vPv5LPA5Rm871+oz5svwbuBAVT2zfsNGGcvOMyenkrqfz46p2RBjmuQXgGuBn+1+yZyix/kxqKp6pqr+r6peAv5swutvlPHcDPw0cPukmlWP5zRDBvf9wJuTXNxdge0G7lpXcxdw8i79+4B/mnRSDqWb5/pz4EhV/f6Emu8/Ofee5FJG47bUXzBJzk5yzsllRjesHllXdhfw892nSy4Dnl8zFbBME69kNsJYrrH2/LsO+LsxNV8ErkqytXvrf1W3bmmSXA18FHhPVf3vhJo+58eg1t1P+akJr98nF5bhXcCjVXVs3MaNMJ5TDXnnk9GnHB5jdBf5Y92632B0AgKcyejt9FHg34BLln13Fric0Vvkh4CD3eMa4MPAh7ua64FDjO6A3we8YwV9XtK9/oNdLyfHc22fAf64G++HgZ0r6PNsRkF87pp1Kx9LRr9ITgDfZjSv+ouM7qf8I/A48A/A+V3tTuDmNft+qDtHjwIfXEGfRxnNC588P09+EusNwN3Tzo8l9/nX3Xn3EKMw3ra+z+75KbmwzD679X958pxcU7uy8Zz34TcnJakx3pyUpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNeb/Afg7uy9hD/E/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ac2d9860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.food.shape)\n",
    "for obs in env_obs:\n",
    "    print(obs.shape)\n",
    "for obs in agents_obs:\n",
    "    print(obs.shape)\n",
    "print(env.gamespace_width)\n",
    "print(env.gamespace_height)\n",
    "print(env.agent_locations)\n",
    "print(env.teams)\n",
    "print(env.tagged)\n",
    "\n",
    "for state in env.state_n:\n",
    "    print(state.shape)\n",
    "    plt.imshow(state[:,:,6])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Vikings\n",
      "name Vikings\n",
      "color deepskyblue\n",
      "culture {'name': 'pacifist_leadfollow', 'laser_penalty': -1.0, 'target_reward': 2.0}\n",
      "roles ['leader', 'follower']\n",
      "target_zone None\n",
      "banned_zone None\n",
      "\n",
      "\n",
      "Team Franks\n",
      "name Franks\n",
      "color red\n",
      "culture {'name': 'pacifist', 'laser_penalty': -1.0}\n",
      "roles ['leader', 'follower']\n",
      "target_zone None\n",
      "banned_zone None\n",
      "\n",
      "\n",
      "Agent 0\n",
      "id 0\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 7)\n",
      "(35, 24)\n",
      "\n",
      "\n",
      "Agent 1\n",
      "id 1\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 9)\n",
      "(22, 39)\n",
      "\n",
      "\n",
      "Agent 2\n",
      "id 2\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (2, 8)\n",
      "(35, 27)\n",
      "\n",
      "\n",
      "Agent 3\n",
      "id 3\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 7)\n",
      "(36, 25)\n",
      "\n",
      "\n",
      "Agent 4\n",
      "id 4\n",
      "team Vikings\n",
      "color royalblue\n",
      "type drone\n",
      "role leader\n",
      "start (3, 9)\n",
      "(33, 25)\n",
      "\n",
      "\n",
      "Agent 5\n",
      "id 5\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 1)\n",
      "(22, 21)\n",
      "\n",
      "\n",
      "Agent 6\n",
      "id 6\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 3)\n",
      "(34, 25)\n",
      "\n",
      "\n",
      "Agent 7\n",
      "id 7\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (2, 2)\n",
      "(20, 21)\n",
      "\n",
      "\n",
      "Agent 8\n",
      "id 8\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 1)\n",
      "(35, 25)\n",
      "\n",
      "\n",
      "Agent 9\n",
      "id 9\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 3)\n",
      "(74, 39)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for team in teams_params:\n",
    "    print (\"Team {}\".format(team['name']) )\n",
    "    for k, v in team.items():\n",
    "        print (k, v)\n",
    "    print ('\\n')\n",
    "\n",
    "for i, agent in enumerate(agents_params):\n",
    "    print (\"Agent {}\".format(agent['id']) )\n",
    "    for k, v in agent.items():\n",
    "        print (k, v)\n",
    "    print (env.agent_locations[i])\n",
    "    print ('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - DroneLeader_PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Load Drone Leader.\n",
      "..........\n",
      "Episode 10 complete\n",
      "\n",
      "Episode 10 complete\tDelta total:7\tRunning mean: 54.54\n",
      "Max Norms =  ['139.23']\n",
      "..........\n",
      "Episode 20 complete\n",
      "\n",
      "Episode 20 complete\tDelta total:7\tRunning mean: 50.96\n",
      "Max Norms =  ['89.19']\n",
      "..........\n",
      "Episode 30 complete\n",
      "\n",
      "Episode 30 complete\tDelta total:37\tRunning mean: 47.29\n",
      "Max Norms =  ['108.86']\n",
      "..........\n",
      "Episode 40 complete\n",
      "\n",
      "Episode 40 complete\tDelta total:45\tRunning mean: 46.56\n",
      "Max Norms =  ['120.65']\n",
      "..........\n",
      "Episode 50 complete\n",
      "\n",
      "Episode 50 complete\tDelta total:44\tRunning mean: 46.09\n",
      "Max Norms =  ['78.81']\n",
      "..........\n",
      "Episode 60 complete\n",
      "\n",
      "Episode 60 complete\tDelta total:43\tRunning mean: 45.72\n",
      "Max Norms =  ['74.12']\n",
      "..........\n",
      "Episode 70 complete\n",
      "\n",
      "Episode 70 complete\tDelta total:42\tRunning mean: 45.27\n",
      "Max Norms =  ['128.72']\n",
      "..........\n",
      "Episode 80 complete\n",
      "\n",
      "Episode 80 complete\tDelta total:43\tRunning mean: 44.97\n",
      "Max Norms =  ['51.43']\n",
      "..........\n",
      "Episode 90 complete\n",
      "\n",
      "Episode 90 complete\tDelta total:43\tRunning mean: 44.78\n",
      "Max Norms =  ['35.21']\n",
      "..........\n",
      "Episode 100 complete\n",
      "\n",
      "Episode 100 complete\tDelta total:42\tRunning mean: 44.51\n",
      "Max Norms =  ['78.71']\n",
      "..........\n",
      "Episode 110 complete\n",
      "\n",
      "Episode 110 complete\tDelta total:42\tRunning mean: 44.29\n",
      "Max Norms =  ['25.43']\n",
      "..........\n",
      "Episode 120 complete\n",
      "\n",
      "Episode 120 complete\tDelta total:43\tRunning mean: 44.06\n",
      "Max Norms =  ['33.94']\n",
      "..........\n",
      "Episode 130 complete\n",
      "\n",
      "Episode 130 complete\tDelta total:39\tRunning mean: 43.68\n",
      "Max Norms =  ['42.91']\n",
      "..........\n",
      "Episode 140 complete\n",
      "\n",
      "Episode 140 complete\tDelta total:40\tRunning mean: 43.33\n",
      "Max Norms =  ['45.17']\n",
      "..........\n",
      "Episode 150 complete\n",
      "\n",
      "Episode 150 complete\tDelta total:40\tRunning mean: 43.01\n",
      "Max Norms =  ['26.51']\n",
      "..........\n",
      "Episode 160 complete\n",
      "\n",
      "Episode 160 complete\tDelta total:43\tRunning mean: 42.86\n",
      "Max Norms =  ['39.61']\n",
      "..........\n",
      "Episode 170 complete\n",
      "\n",
      "Episode 170 complete\tDelta total:40\tRunning mean: 42.64\n",
      "Max Norms =  ['29.24']\n",
      "..........\n",
      "Episode 180 complete\n",
      "\n",
      "Episode 180 complete\tDelta total:41\tRunning mean: 42.44\n",
      "Max Norms =  ['9.80']\n",
      "..........\n",
      "Episode 190 complete\n",
      "\n",
      "Episode 190 complete\tDelta total:41\tRunning mean: 42.37\n",
      "Max Norms =  ['21.55']\n",
      "..........\n",
      "Episode 200 complete\n",
      "\n",
      "Episode 200 complete\tDelta total:41\tRunning mean: 42.33\n",
      "Max Norms =  ['9.89']\n",
      "..........\n",
      "Episode 210 complete\n",
      "\n",
      "Episode 210 complete\tDelta total:44\tRunning mean: 42.34\n",
      "Max Norms =  ['19.27']\n",
      "..........\n",
      "Episode 220 complete\n",
      "\n",
      "Episode 220 complete\tDelta total:42\tRunning mean: 42.37\n",
      "Max Norms =  ['18.56']\n",
      "..........\n",
      "Episode 230 complete\n",
      "\n",
      "Episode 230 complete\tDelta total:42\tRunning mean: 42.35\n",
      "Max Norms =  ['6.39']\n",
      "..........\n",
      "Episode 240 complete\n",
      "\n",
      "Episode 240 complete\tDelta total:42\tRunning mean: 42.3\n",
      "Max Norms =  ['21.83']\n",
      "..........\n",
      "Episode 250 complete\n",
      "\n",
      "Episode 250 complete\tDelta total:42\tRunning mean: 42.38\n",
      "Max Norms =  ['9.09']\n",
      "..........\n",
      "Episode 260 complete\n",
      "\n",
      "Episode 260 complete\tDelta total:44\tRunning mean: 42.59\n",
      "Max Norms =  ['15.39']\n",
      "..........\n",
      "Episode 270 complete\n",
      "\n",
      "Episode 270 complete\tDelta total:42\tRunning mean: 42.67\n",
      "Max Norms =  ['26.67']\n",
      "..........\n",
      "Episode 280 complete\n",
      "\n",
      "Episode 280 complete\tDelta total:40\tRunning mean: 42.51\n",
      "Max Norms =  ['23.90']\n",
      "..........\n",
      "Episode 290 complete\n",
      "\n",
      "Episode 290 complete\tDelta total:42\tRunning mean: 42.39\n",
      "Max Norms =  ['26.17']\n",
      "..........\n",
      "Episode 300 complete\n",
      "\n",
      "Episode 300 complete\tDelta total:41\tRunning mean: 42.27\n",
      "Max Norms =  ['3.56']\n",
      "..........\n",
      "Episode 310 complete\n",
      "\n",
      "Episode 310 complete\tDelta total:41\tRunning mean: 42.13\n",
      "Max Norms =  ['5.27']\n",
      "..........\n",
      "Episode 320 complete\n",
      "\n",
      "Episode 320 complete\tDelta total:41\tRunning mean: 42.05\n",
      "Max Norms =  ['17.46']\n",
      "..........\n",
      "Episode 330 complete\n",
      "\n",
      "Episode 330 complete\tDelta total:42\tRunning mean: 42.02\n",
      "Max Norms =  ['11.96']\n",
      "..........\n",
      "Episode 340 complete\n",
      "\n",
      "Episode 340 complete\tDelta total:42\tRunning mean: 42.05\n",
      "Max Norms =  ['14.45']\n",
      "..........\n",
      "Episode 350 complete\n",
      "\n",
      "Episode 350 complete\tDelta total:42\tRunning mean: 42.04\n",
      "Max Norms =  ['2.50']\n",
      "..........\n",
      "Episode 360 complete\n",
      "\n",
      "Episode 360 complete\tDelta total:43\tRunning mean: 42.05\n",
      "Max Norms =  ['10.05']\n",
      "..........\n",
      "Episode 370 complete\n",
      "\n",
      "Episode 370 complete\tDelta total:42\tRunning mean: 42.11\n",
      "Max Norms =  ['5.34']\n",
      "..........\n",
      "Episode 380 complete\n",
      "\n",
      "Episode 380 complete\tDelta total:42\tRunning mean: 42.17\n",
      "Max Norms =  ['6.93']\n",
      "..........\n",
      "Episode 390 complete\n",
      "\n",
      "Episode 390 complete\tDelta total:43\tRunning mean: 42.24\n",
      "Max Norms =  ['2.25']\n",
      "..........\n",
      "Episode 400 complete\n",
      "\n",
      "Episode 400 complete\tDelta total:43\tRunning mean: 42.31\n",
      "Max Norms =  ['18.84']\n",
      "..........\n",
      "Episode 410 complete\n",
      "\n",
      "Episode 410 complete\tDelta total:42\tRunning mean: 42.31\n",
      "Max Norms =  ['14.02']\n",
      "..........\n",
      "Episode 420 complete\n",
      "\n",
      "Episode 420 complete\tDelta total:44\tRunning mean: 42.37\n",
      "Max Norms =  ['10.54']\n",
      "..........\n",
      "Episode 430 complete\n",
      "\n",
      "Episode 430 complete\tDelta total:43\tRunning mean: 42.47\n",
      "Max Norms =  ['12.18']\n",
      "..........\n",
      "Episode 440 complete\n",
      "\n",
      "Episode 440 complete\tDelta total:44\tRunning mean: 42.58\n",
      "Max Norms =  ['12.48']\n",
      "..........\n",
      "Episode 450 complete\n",
      "\n",
      "Episode 450 complete\tDelta total:43\tRunning mean: 42.65\n",
      "Max Norms =  ['18.30']\n",
      "..........\n",
      "Episode 460 complete\n",
      "\n",
      "Episode 460 complete\tDelta total:43\tRunning mean: 42.68\n",
      "Max Norms =  ['3.28']\n",
      "..........\n",
      "Episode 470 complete\n",
      "\n",
      "Episode 470 complete\tDelta total:41\tRunning mean: 42.58\n",
      "Max Norms =  ['2.82']\n",
      "..........\n",
      "Episode 480 complete\n",
      "\n",
      "Episode 480 complete\tDelta total:40\tRunning mean: 42.48\n",
      "Max Norms =  ['25.18']\n",
      "..........\n",
      "Episode 490 complete\n",
      "\n",
      "Episode 490 complete\tDelta total:41\tRunning mean: 42.31\n",
      "Max Norms =  ['13.53']\n",
      "..........\n",
      "Episode 500 complete\n",
      "\n",
      "Episode 500 complete\tDelta total:40\tRunning mean: 42.08\n",
      "Max Norms =  ['22.50']\n",
      "..........\n",
      "Episode 510 complete\n",
      "\n",
      "Episode 510 complete\tDelta total:39\tRunning mean: 41.82\n",
      "Max Norms =  ['49.72']\n",
      "..........\n",
      "Episode 520 complete\n",
      "\n",
      "Episode 520 complete\tDelta total:41\tRunning mean: 41.63\n",
      "Max Norms =  ['17.09']\n",
      "..........\n",
      "Episode 530 complete\n",
      "\n",
      "Episode 530 complete\tDelta total:41\tRunning mean: 41.57\n",
      "Max Norms =  ['15.50']\n",
      "..........\n",
      "Episode 540 complete\n",
      "\n",
      "Episode 540 complete\tDelta total:41\tRunning mean: 41.47\n",
      "Max Norms =  ['13.14']\n",
      "..........\n",
      "Episode 550 complete\n",
      "\n",
      "Episode 550 complete\tDelta total:41\tRunning mean: 41.42\n",
      "Max Norms =  ['15.40']\n",
      "..........\n",
      "Episode 560 complete\n",
      "\n",
      "Episode 560 complete\tDelta total:41\tRunning mean: 41.4\n",
      "Max Norms =  ['11.54']\n",
      "..........\n",
      "Episode 570 complete\n",
      "\n",
      "Episode 570 complete\tDelta total:42\tRunning mean: 41.47\n",
      "Max Norms =  ['10.88']\n",
      "..........\n",
      "Episode 580 complete\n",
      "\n",
      "Episode 580 complete\tDelta total:41\tRunning mean: 41.47\n",
      "Max Norms =  ['10.35']\n",
      "..........\n",
      "Episode 590 complete\n",
      "\n",
      "Episode 590 complete\tDelta total:41\tRunning mean: 41.43\n",
      "Max Norms =  ['19.96']\n",
      "..........\n",
      "Episode 600 complete\n",
      "\n",
      "Episode 600 complete\tDelta total:42\tRunning mean: 41.44\n",
      "Max Norms =  ['2.23']\n",
      "..........\n",
      "Episode 610 complete\n",
      "\n",
      "Episode 610 complete\tDelta total:42\tRunning mean: 41.5\n",
      "Max Norms =  ['13.13']\n",
      "..........\n",
      "Episode 620 complete\n",
      "\n",
      "Episode 620 complete\tDelta total:41\tRunning mean: 41.53\n",
      "Max Norms =  ['12.05']\n",
      "..........\n",
      "Episode 630 complete\n",
      "\n",
      "Episode 630 complete\tDelta total:41\tRunning mean: 41.48\n",
      "Max Norms =  ['20.99']\n",
      "..........\n",
      "Episode 640 complete\n",
      "\n",
      "Episode 640 complete\tDelta total:41\tRunning mean: 41.48\n",
      "Max Norms =  ['14.87']\n",
      "..........\n",
      "Episode 650 complete\n",
      "\n",
      "Episode 650 complete\tDelta total:41\tRunning mean: 41.48\n",
      "Max Norms =  ['39.75']\n",
      "..........\n",
      "Episode 660 complete\n",
      "\n",
      "Episode 660 complete\tDelta total:42\tRunning mean: 41.52\n",
      "Max Norms =  ['8.28']\n",
      "..........\n",
      "Episode 670 complete\n",
      "\n",
      "Episode 670 complete\tDelta total:42\tRunning mean: 41.57\n",
      "Max Norms =  ['0.52']\n",
      "..........\n",
      "Episode 680 complete\n",
      "\n",
      "Episode 680 complete\tDelta total:41\tRunning mean: 41.54\n",
      "Max Norms =  ['13.97']\n",
      "..........\n",
      "Episode 690 complete\n",
      "\n",
      "Episode 690 complete\tDelta total:41\tRunning mean: 41.49\n",
      "Max Norms =  ['3.58']\n",
      "..........\n",
      "Episode 700 complete\n",
      "\n",
      "Episode 700 complete\tDelta total:40\tRunning mean: 41.4\n",
      "Max Norms =  ['7.43']\n",
      "..........\n",
      "Episode 710 complete\n",
      "\n",
      "Episode 710 complete\tDelta total:40\tRunning mean: 41.27\n",
      "Max Norms =  ['5.11']\n",
      "..........\n",
      "Episode 720 complete\n",
      "\n",
      "Episode 720 complete\tDelta total:40\tRunning mean: 41.15\n",
      "Max Norms =  ['7.04']\n",
      "..........\n",
      "Episode 730 complete\n",
      "\n",
      "Episode 730 complete\tDelta total:39\tRunning mean: 41.01\n",
      "Max Norms =  ['22.02']\n",
      "..........\n",
      "Episode 740 complete\n",
      "\n",
      "Episode 740 complete\tDelta total:40\tRunning mean: 40.89\n",
      "Max Norms =  ['16.60']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Episode 750 complete\n",
      "\n",
      "Episode 750 complete\tDelta total:42\tRunning mean: 40.95\n",
      "Max Norms =  ['0.57']\n",
      "..........\n",
      "Episode 760 complete\n",
      "\n",
      "Episode 760 complete\tDelta total:43\tRunning mean: 41.11\n",
      "Max Norms =  ['14.20']\n",
      "..........\n",
      "Episode 770 complete\n",
      "\n",
      "Episode 770 complete\tDelta total:44\tRunning mean: 41.38\n",
      "Max Norms =  ['12.17']\n",
      "..........\n",
      "Episode 780 complete\n",
      "\n",
      "Episode 780 complete\tDelta total:44\tRunning mean: 41.63\n",
      "Max Norms =  ['8.32']\n",
      "..........\n",
      "Episode 790 complete\n",
      "\n",
      "Episode 790 complete\tDelta total:44\tRunning mean: 41.86\n",
      "Max Norms =  ['84.17']\n",
      "..........\n",
      "Episode 800 complete\n",
      "\n",
      "Episode 800 complete\tDelta total:45\tRunning mean: 42.15\n",
      "Max Norms =  ['36.75']\n",
      "..........\n",
      "Episode 810 complete\n",
      "\n",
      "Episode 810 complete\tDelta total:45\tRunning mean: 42.42\n",
      "Max Norms =  ['2.01']\n",
      "..........\n",
      "Episode 820 complete\n",
      "\n",
      "Episode 820 complete\tDelta total:44\tRunning mean: 42.58\n",
      "Max Norms =  ['1.73']\n",
      "..........\n",
      "Episode 830 complete\n",
      "\n",
      "Episode 830 complete\tDelta total:44\tRunning mean: 42.7\n",
      "Max Norms =  ['5.93']\n",
      "..........\n",
      "Episode 840 complete\n",
      "\n",
      "Episode 840 complete\tDelta total:44\tRunning mean: 42.82\n",
      "Max Norms =  ['0.10']\n",
      "..........\n",
      "Episode 850 complete\n",
      "\n",
      "Episode 850 complete\tDelta total:44\tRunning mean: 42.93\n",
      "Max Norms =  ['0.61']\n",
      "..........\n",
      "Episode 860 complete\n",
      "\n",
      "Episode 860 complete\tDelta total:43\tRunning mean: 43.01\n",
      "Max Norms =  ['48.07']\n",
      "..........\n",
      "Episode 870 complete\n",
      "\n",
      "Episode 870 complete\tDelta total:43\tRunning mean: 43.06\n",
      "Max Norms =  ['1.50']\n",
      "..........\n",
      "Episode 880 complete\n",
      "\n",
      "Episode 880 complete\tDelta total:43\tRunning mean: 43.11\n",
      "Max Norms =  ['14.58']\n",
      "..........\n",
      "Episode 890 complete\n",
      "\n",
      "Episode 890 complete\tDelta total:43\tRunning mean: 43.11\n",
      "Max Norms =  ['0.90']\n",
      "..........\n",
      "Episode 900 complete\n",
      "\n",
      "Episode 900 complete\tDelta total:42\tRunning mean: 43.08\n",
      "Max Norms =  ['10.99']\n",
      "..........\n",
      "Episode 910 complete\n",
      "\n",
      "Episode 910 complete\tDelta total:42\tRunning mean: 42.97\n",
      "Max Norms =  ['3.89']\n",
      "..........\n",
      "Episode 920 complete\n",
      "\n",
      "Episode 920 complete\tDelta total:42\tRunning mean: 42.88\n",
      "Max Norms =  ['11.16']\n",
      "..........\n",
      "Episode 930 complete\n",
      "\n",
      "Episode 930 complete\tDelta total:43\tRunning mean: 42.85\n",
      "Max Norms =  ['0.15']\n",
      "..........\n",
      "Episode 940 complete\n",
      "\n",
      "Episode 940 complete\tDelta total:43\tRunning mean: 42.86\n",
      "Max Norms =  ['0.81']\n",
      "..........\n",
      "Episode 950 complete\n",
      "\n",
      "Episode 950 complete\tDelta total:43\tRunning mean: 42.88\n",
      "Max Norms =  ['0.12']\n",
      "..........\n",
      "Episode 960 complete\n",
      "\n",
      "Episode 960 complete\tDelta total:43\tRunning mean: 42.89\n",
      "Max Norms =  ['2.26']\n",
      "..........\n",
      "Episode 970 complete\n",
      "\n",
      "Episode 970 complete\tDelta total:43\tRunning mean: 42.9\n",
      "Max Norms =  ['0.22']\n",
      "..........\n",
      "Episode 980 complete\n",
      "\n",
      "Episode 980 complete\tDelta total:43\tRunning mean: 42.91\n",
      "Max Norms =  ['0.26']\n",
      "..........\n",
      "Episode 990 complete\n",
      "\n",
      "Episode 990 complete\tDelta total:43\tRunning mean: 42.92\n",
      "Max Norms =  ['0.16']\n",
      "..........\n",
      "Episode 1000 complete\n",
      "\n",
      "Episode 1000 complete\tDelta total:43\tRunning mean: 42.93\n",
      "Max Norms =  ['0.08']\n",
      "..........\n",
      "Episode 1010 complete\n",
      "\n",
      "Episode 1010 complete\tDelta total:43\tRunning mean: 42.93\n",
      "Max Norms =  ['0.06']\n",
      "..........\n",
      "Episode 1020 complete\n",
      "\n",
      "Episode 1020 complete\tDelta total:43\tRunning mean: 42.94\n",
      "Max Norms =  ['0.05']\n",
      "..........\n",
      "Episode 1030 complete\n",
      "\n",
      "Episode 1030 complete\tDelta total:43\tRunning mean: 42.95\n",
      "Max Norms =  ['0.05']\n",
      "..........\n",
      "Episode 1040 complete\n",
      "\n",
      "Episode 1040 complete\tDelta total:43\tRunning mean: 42.95\n",
      "Max Norms =  ['0.04']\n",
      "..........\n",
      "Episode 1050 complete\n",
      "\n",
      "Episode 1050 complete\tDelta total:43\tRunning mean: 42.96\n",
      "Max Norms =  ['0.04']\n",
      "..........\n",
      "Episode 1060 complete\n",
      "\n",
      "Episode 1060 complete\tDelta total:43\tRunning mean: 42.96\n",
      "Max Norms =  ['0.04']\n",
      "..........\n",
      "Episode 1070 complete\n",
      "\n",
      "Episode 1070 complete\tDelta total:43\tRunning mean: 42.96\n",
      "Max Norms =  ['0.04']\n",
      "..........\n",
      "Episode 1080 complete\n",
      "\n",
      "Episode 1080 complete\tDelta total:43\tRunning mean: 42.97\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1090 complete\n",
      "\n",
      "Episode 1090 complete\tDelta total:43\tRunning mean: 42.97\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1100 complete\n",
      "\n",
      "Episode 1100 complete\tDelta total:43\tRunning mean: 42.97\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1110 complete\n",
      "\n",
      "Episode 1110 complete\tDelta total:43\tRunning mean: 42.98\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1120 complete\n",
      "\n",
      "Episode 1120 complete\tDelta total:43\tRunning mean: 42.98\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1130 complete\n",
      "\n",
      "Episode 1130 complete\tDelta total:43\tRunning mean: 42.98\n",
      "Max Norms =  ['0.03']\n",
      "..........\n",
      "Episode 1140 complete\n",
      "\n",
      "Episode 1140 complete\tDelta total:43\tRunning mean: 42.98\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1150 complete\n",
      "\n",
      "Episode 1150 complete\tDelta total:43\tRunning mean: 42.98\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1160 complete\n",
      "\n",
      "Episode 1160 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1170 complete\n",
      "\n",
      "Episode 1170 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1180 complete\n",
      "\n",
      "Episode 1180 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1190 complete\n",
      "\n",
      "Episode 1190 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1200 complete\n",
      "\n",
      "Episode 1200 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1210 complete\n",
      "\n",
      "Episode 1210 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1220 complete\n",
      "\n",
      "Episode 1220 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1230 complete\n",
      "\n",
      "Episode 1230 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1240 complete\n",
      "\n",
      "Episode 1240 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.02']\n",
      "..........\n",
      "Episode 1250 complete\n",
      "\n",
      "Episode 1250 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1260 complete\n",
      "\n",
      "Episode 1260 complete\tDelta total:43\tRunning mean: 42.99\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1270 complete\n",
      "\n",
      "Episode 1270 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1280 complete\n",
      "\n",
      "Episode 1280 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1290 complete\n",
      "\n",
      "Episode 1290 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1300 complete\n",
      "\n",
      "Episode 1300 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1310 complete\n",
      "\n",
      "Episode 1310 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1320 complete\n",
      "\n",
      "Episode 1320 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1330 complete\n",
      "\n",
      "Episode 1330 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1340 complete\n",
      "\n",
      "Episode 1340 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1350 complete\n",
      "\n",
      "Episode 1350 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1360 complete\n",
      "\n",
      "Episode 1360 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1370 complete\n",
      "\n",
      "Episode 1370 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1380 complete\n",
      "\n",
      "Episode 1380 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1390 complete\n",
      "\n",
      "Episode 1390 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1400 complete\n",
      "\n",
      "Episode 1400 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1410 complete\n",
      "\n",
      "Episode 1410 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1420 complete\n",
      "\n",
      "Episode 1420 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1430 complete\n",
      "\n",
      "Episode 1430 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1440 complete\n",
      "\n",
      "Episode 1440 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1450 complete\n",
      "\n",
      "Episode 1450 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1460 complete\n",
      "\n",
      "Episode 1460 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1470 complete\n",
      "\n",
      "Episode 1470 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Episode 1480 complete\n",
      "\n",
      "Episode 1480 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1490 complete\n",
      "\n",
      "Episode 1490 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1500 complete\n",
      "\n",
      "Episode 1500 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1510 complete\n",
      "\n",
      "Episode 1510 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1520 complete\n",
      "\n",
      "Episode 1520 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1530 complete\n",
      "\n",
      "Episode 1530 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.01']\n",
      "..........\n",
      "Episode 1540 complete\n",
      "\n",
      "Episode 1540 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1550 complete\n",
      "\n",
      "Episode 1550 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1560 complete\n",
      "\n",
      "Episode 1560 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1570 complete\n",
      "\n",
      "Episode 1570 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1580 complete\n",
      "\n",
      "Episode 1580 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1590 complete\n",
      "\n",
      "Episode 1590 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1600 complete\n",
      "\n",
      "Episode 1600 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1610 complete\n",
      "\n",
      "Episode 1610 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1620 complete\n",
      "\n",
      "Episode 1620 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1630 complete\n",
      "\n",
      "Episode 1630 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1640 complete\n",
      "\n",
      "Episode 1640 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1650 complete\n",
      "\n",
      "Episode 1650 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1660 complete\n",
      "\n",
      "Episode 1660 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1670 complete\n",
      "\n",
      "Episode 1670 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1680 complete\n",
      "\n",
      "Episode 1680 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1690 complete\n",
      "\n",
      "Episode 1690 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1700 complete\n",
      "\n",
      "Episode 1700 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1710 complete\n",
      "\n",
      "Episode 1710 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1720 complete\n",
      "\n",
      "Episode 1720 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1730 complete\n",
      "\n",
      "Episode 1730 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1740 complete\n",
      "\n",
      "Episode 1740 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1750 complete\n",
      "\n",
      "Episode 1750 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1760 complete\n",
      "\n",
      "Episode 1760 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1770 complete\n",
      "\n",
      "Episode 1770 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1780 complete\n",
      "\n",
      "Episode 1780 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1790 complete\n",
      "\n",
      "Episode 1790 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1800 complete\n",
      "\n",
      "Episode 1800 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1810 complete\n",
      "\n",
      "Episode 1810 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1820 complete\n",
      "\n",
      "Episode 1820 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1830 complete\n",
      "\n",
      "Episode 1830 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1840 complete\n",
      "\n",
      "Episode 1840 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1850 complete\n",
      "\n",
      "Episode 1850 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1860 complete\n",
      "\n",
      "Episode 1860 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1870 complete\n",
      "\n",
      "Episode 1870 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1880 complete\n",
      "\n",
      "Episode 1880 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1890 complete\n",
      "\n",
      "Episode 1890 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1900 complete\n",
      "\n",
      "Episode 1900 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1910 complete\n",
      "\n",
      "Episode 1910 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1920 complete\n",
      "\n",
      "Episode 1920 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1930 complete\n",
      "\n",
      "Episode 1930 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1940 complete\n",
      "\n",
      "Episode 1940 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1950 complete\n",
      "\n",
      "Episode 1950 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1960 complete\n",
      "\n",
      "Episode 1960 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1970 complete\n",
      "\n",
      "Episode 1970 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1980 complete\n",
      "\n",
      "Episode 1980 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 1990 complete\n",
      "\n",
      "Episode 1990 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "..........\n",
      "Episode 2000 complete\n",
      "\n",
      "Episode 2000 complete\tDelta total:43\tRunning mean: 43.0\n",
      "Max Norms =  ['0.00']\n",
      "\n",
      "Training time: 750.62 sec\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize environment\n",
    "game = \"Crossing\"\n",
    "num_crawler_actions = 8                     # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                      # Drones are capable of 12 actions\n",
    "num_goal_params = 2    # Goal has 2 parameter\n",
    "\n",
    "experiment = '1T-1L/pg_droneleader_2000/'    # 1 team of 1 policy gradient based drone leader \n",
    "\n",
    "# Map and Parameter sets\n",
    "map_name = \"food_d37_river_w1_d25\"  \n",
    "parameters =[ \n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "]\n",
    "\n",
    "temp_end = 1.2   # temp parameter is annealed from the value stored in parameters['temp_start'] to 1.0 \n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 7      # environ observation consists of a list of stacked frames per agent\n",
    "max_episodes = 2000\n",
    "\n",
    "render = True    # This turns on rendering every save so that agents' behavior can be observed\n",
    "SPEED = 1/30\n",
    "second_pile_x = 50  # x-coordinate of the 2nd food pile\n",
    "\n",
    "log_interval = 10\n",
    "save_interval = 20\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-2\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   1 agents - 1 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 1\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    start = time.clock()  # time the training\n",
    "    \n",
    "    torch.manual_seed(parameter['seed'])\n",
    "    situation = 'pg_droneleader_seed_'+str(parameter['seed'])\n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "    \n",
    "    # Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "    teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "    ]\n",
    "    agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)},\n",
    "    ]\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    deltas = []   # 6-2-2019 delta coordinates\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            \n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            \n",
    "            # Initialize agent policy based on type\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                print(\"Load Drone Leader.\")\n",
    "                agents.append(Drone_Policy(num_frames, num_drone_actions, i))  \n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "            \n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "        deltas = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "        \n",
    "        # 6-2-2019 Keep track of distance from goal achieved by droneleader\n",
    "        episode_delta = 0   # distance from goal for an episode\n",
    "        running_delta = None   # running distance from goal\n",
    "        running_deltas = []    # history of running distance from goal\n",
    "        best_delta = 0    # best running distance from goal (for storing best_model)        \n",
    "        \n",
    "        # Keep track of num learners who has crossed over to the 2nd food pile\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "        running_crossed = None         # running average\n",
    "        running_crossed_hist = []   # history of running averages\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Crawler_Policy(input_channels=num_frames, num_actions=num_crawler_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "    # Attach agents to their teams\n",
    "    # 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "    teams = []\n",
    "    # Team Vikings\n",
    "    teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'], \\\n",
    "                  culture=teams_params[0]['culture'], roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:1]]))\n",
    "    \n",
    "    # 5-30-2019  Strategist accepts directorship of a team\n",
    "    suntzu = Strategist()\n",
    "    suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "    \n",
    "    env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = False)   \n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with Crawler_Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        \n",
    "        # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_norm_deltas(goals[0], env.agent_locations[0])\n",
    "        # agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize reward and agents crossed counters\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        episode_delta = 0                               # distance from goal for an episode\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                else:    \n",
    "                    actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                \n",
    "                # Only crawlers can fire lasers\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                        \n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with Crawler_Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            \n",
    "            load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "            \n",
    "            # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "            game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "            goals, topology = suntzu.generate_goals(game_space)\n",
    "            deltas = calc_norm_deltas(goals[0], env.agent_locations[0])\n",
    "            # agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "            \n",
    "            if render and (ep % save_interval == 0):   # render 1 episode every save\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "\n",
    "        # Keep track num of agents who gather from 2nd food pile. Note that env.consumption tracks the \n",
    "        # agent index and location of apple gathered\n",
    "        for (i, loc) in env.consumption:\n",
    "            if loc[0] > second_pile_x:   # If x-cood of gathered apple is beyond a preset value, it is\n",
    "                                         # in the 2nd pile\n",
    "                crossed[i] = 1\n",
    "        episode_crossed = sum(crossed)   # sum up the num agents who crossed to 2nd pile for the episode\n",
    "                \n",
    "        # Update reward and crossed statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "            \n",
    "        if running_crossed is None:\n",
    "            running_crossed = episode_crossed\n",
    "        running_crossed = running_crossed * 0.99 + episode_crossed * 0.01\n",
    "        running_crossed_hist.append(running_crossed)\n",
    "        \n",
    "        # 6-02-2019 Update distance from goal for droneleader\n",
    "        target_x, target_y = goals[0]\n",
    "        current_x, current_y = env.agent_locations[0]\n",
    "        episode_delta = abs(target_x - current_x) + abs(target_y - current_y)\n",
    "        \n",
    "        if running_delta is None:\n",
    "            running_delta = episode_delta\n",
    "        running_delta = running_delta * 0.99 + episode_delta * 0.01\n",
    "        running_deltas.append(running_delta)\n",
    "        \n",
    "                \n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                # verbose_str = 'Learner:{}'.format(i)\n",
    "                # verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                # verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                # verbose_str += '\\tNum agents crossed: {}'.format(episode_crossed)\n",
    "                # verbose_str += '\\tRunning mean: {:.4}'.format(running_crossed)\n",
    "                verbose_str += '\\tDelta total:{}'.format(episode_delta)\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_delta)\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(teams, agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "\n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/' + experiment + map_name\n",
    "                results_dir = 'results/' + experiment + map_name\n",
    "\n",
    "                model_file = model_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    save_model(f, ep, agents[i], optimizers[i])\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "             \n",
    "            crossed_file = results_dir+'/{}/t{}_rp{}_{}gs/Crossed.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(crossed_file), exist_ok=True)\n",
    "            with open(crossed_file, 'wb') as f:\n",
    "                    pickle.dump(running_crossed_hist, f)\n",
    "\n",
    "            delta_file = results_dir+'/{}/t{}_rp{}_{}gs/Delta.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(delta_file), exist_ok=True)\n",
    "            with open(delta_file, 'wb') as f:\n",
    "                    pickle.dump(running_deltas, f)\n",
    "\n",
    "    end = time.clock()\n",
    "    print('\\nTraining time: {:.2f} sec'.format((end-start)/60.0))\n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].apples_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agents with New Env\n",
    "\n",
    "The code below run training on 2 teams of 5 Agents each using the new environment.\n",
    "\n",
    "Team Viking has a Pacifist culture and they are unagressive (do not fire their lasers). Team Franks has a Cooperative culture and their agents specialize into 'shooters' and 'gatherers' even though these roles are not formally defined. Thus the Team Viking is highly disadvantaged in this situation. \n",
    "\n",
    "We will investigate how the introduction of a 'leader' can change the dynamic of this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "Learner agent 5\n",
      "Learner agent 6\n",
      "Learner agent 7\n",
      "Learner agent 8\n",
      "Learner agent 9\n",
      "....................\n",
      "Episode 20 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02504\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.00951\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['154.26', '134.75', '218.74', '102.35', '103.89', '252.86', '310.64', '143.60', '158.70', '189.66']\n",
      "....................\n",
      "Episode 40 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02048\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01737\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01941\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.007778\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['128.95', '117.36', '115.89', '83.53', '0.00', '107.70', '121.43', '117.56', '95.61', '116.16']\n",
      "....................\n",
      "Episode 60 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: -0.02129\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01421\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01587\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03406\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.006362\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['136.24', '121.97', '97.28', '83.58', '118.44', '85.05', '95.69', '0.00', '131.12', '183.30']\n",
      "....................\n",
      "Episode 80 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: -0.01741\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01162\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03544\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01298\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:2\tRunning mean: 0.02\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.02786\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.08068\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['114.49', '137.12', '96.90', '118.61', '144.07', '98.53', '110.44', '101.96', '104.37', '0.00']\n",
      "....................\n",
      "Episode 100 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0344\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.009507\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02899\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.02731\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.04606\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.04035\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.1474\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['117.32', '118.83', '91.01', '106.58', '174.63', '103.48', '0.00', '0.00', '0.00', '0.00']\n",
      "....................\n",
      "Episode 120 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07237\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.05589\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01691\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1013\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.04852\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.03767\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.03842\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.07221\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.1379\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['121.93', '207.10', '103.30', '118.20', '139.69', '113.77', '94.07', '0.00', '0.00', '116.03']\n",
      "....................\n",
      "Episode 140 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07774\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:2\tRunning mean: -0.06266\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02235\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.1026\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.02732\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:3\tRunning mean: 0.06081\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.04133\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.05906\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.3166\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['117.67', '141.23', '99.20', '89.25', '173.73', '0.00', '109.55', '0.00', '0.00', '0.00']\n",
      "....................\n",
      "Episode 160 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1732\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:2\tRunning mean: -0.0229\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02788\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:1\tRunning mean: -0.1338\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:2\tRunning mean: 0.1087\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1014\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:-13.0\tRunning mean: -0.007737\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.06568\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.08557\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['0.00', '118.89', '148.34', '101.31', '149.79', '109.63', '0.00', '143.01', '0.00', '0.00']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 180 complete\n",
      "Learner:0\tReward total:4\tRunning mean: 0.2804\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1489\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05688\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.06421\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:7\tRunning mean: 0.2042\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1106\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: -0.006328\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.07255\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.2038\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['113.33', '0.00', '103.13', '94.92', '198.36', '133.99', '0.00', '0.00', '0.00', '102.13']\n",
      "....................\n",
      "Episode 200 complete\n",
      "Learner:0\tReward total:2\tRunning mean: 0.3902\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1125\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.09431\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.3896\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.1733\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.09999\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:1\tRunning mean: 0.004824\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.05934\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0.0\tRunning mean: 0.3933\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['127.03', '135.29', '89.86', '100.38', '171.11', '0.00', '0.00', '102.80', '0.00', '145.10']\n",
      "....................\n",
      "Episode 220 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4859\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:1\tReward total:0\tRunning mean: -0.297\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:2\tReward total:0\tRunning mean: 0.09461\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:3\tReward total:2\tRunning mean: -0.2203\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:5\tReward total:0\tRunning mean: 0.2862\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:6\tReward total:0\tRunning mean: 0.08178\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:7\tReward total:3\tRunning mean: 0.05278\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:8\tReward total:0\tRunning mean: 0.09197\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Learner:9\tReward total:0\tRunning mean: -0.04896\tNum agents crossed: 0\tRunning mean: 0.01859\n",
      "Max Norms =  ['141.44', '118.74', '132.99', '127.00', '155.72', '0.00', '0.00', '142.75', '0.00', '0.00']\n",
      "....................\n",
      "Episode 240 complete\n",
      "Learner:0\tReward total:4\tRunning mean: 0.6266\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:1\tReward total:0\tRunning mean: -0.3649\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:2\tReward total:0\tRunning mean: 0.278\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:3\tReward total:0\tRunning mean: -0.5457\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:5\tReward total:0\tRunning mean: -0.05338\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:6\tReward total:0\tRunning mean: 0.06689\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:7\tReward total:-4.0\tRunning mean: -0.08919\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:8\tReward total:0\tRunning mean: 0.07522\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Learner:9\tReward total:9\tRunning mean: 0.08151\tNum agents crossed: 0\tRunning mean: 0.02416\n",
      "Max Norms =  ['145.67', '130.37', '116.01', '134.40', '216.43', '98.37', '0.00', '110.74', '0.00', '89.16']\n",
      "....................\n",
      "Episode 260 complete\n",
      "Learner:0\tReward total:8\tRunning mean: 0.8886\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:1\tReward total:0\tRunning mean: -0.2271\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:2\tReward total:0\tRunning mean: 0.6401\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:3\tReward total:4\tRunning mean: -1.495\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:5\tReward total:-1.0\tRunning mean: -0.1401\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:6\tReward total:0\tRunning mean: 0.06349\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01798\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:8\tReward total:0\tRunning mean: 0.07113\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Learner:9\tReward total:1\tRunning mean: -0.09599\tNum agents crossed: 0\tRunning mean: 0.09287\n",
      "Max Norms =  ['122.72', '0.00', '113.61', '100.86', '144.57', '94.55', '0.00', '0.00', '0.00', '103.26']\n",
      "....................\n",
      "Episode 280 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 0.9822\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:1\tReward total:2\tRunning mean: -0.04975\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:2\tReward total:12\tRunning mean: 1.619\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:3\tReward total:0\tRunning mean: -1.594\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:5\tReward total:4\tRunning mean: -0.07165\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:6\tReward total:0\tRunning mean: 0.07981\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:7\tReward total:0\tRunning mean: 0.2106\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0675\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Learner:9\tReward total:12.0\tRunning mean: 0.2138\tNum agents crossed: 1\tRunning mean: 0.176\n",
      "Max Norms =  ['117.32', '149.50', '113.41', '103.12', '191.05', '107.96', '0.00', '0.00', '0.00', '87.52']\n",
      "....................\n",
      "Episode 300 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.476\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:1\tReward total:6\tRunning mean: 0.8956\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:2\tReward total:6\tRunning mean: 1.805\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:3\tReward total:3.0\tRunning mean: -1.494\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:5\tReward total:4\tRunning mean: 0.1874\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:6\tReward total:0\tRunning mean: 0.06528\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:7\tReward total:0\tRunning mean: 0.2954\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:8\tReward total:0\tRunning mean: 0.07207\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Learner:9\tReward total:4\tRunning mean: 0.3183\tNum agents crossed: 1\tRunning mean: 0.2353\n",
      "Max Norms =  ['0.00', '161.88', '100.91', '120.01', '157.22', '128.80', '0.00', '0.00', '0.00', '117.66']\n",
      "....................\n",
      "Episode 320 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.444\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:1\tReward total:11\tRunning mean: 1.669\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:2\tReward total:5\tRunning mean: 2.267\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:3\tReward total:0\tRunning mean: -1.355\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:5\tReward total:-10.0\tRunning mean: 0.6109\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:6\tReward total:0\tRunning mean: 0.05339\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:7\tReward total:0\tRunning mean: 0.2767\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:8\tReward total:0\tRunning mean: 0.1042\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Learner:9\tReward total:0\tRunning mean: 0.3569\tNum agents crossed: 0\tRunning mean: 0.2561\n",
      "Max Norms =  ['0.00', '101.75', '135.68', '106.93', '145.35', '107.34', '0.00', '0.00', '0.00', '0.00']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 340 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.343\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:1\tReward total:7\tRunning mean: 2.863\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:2\tReward total:-8.0\tRunning mean: 2.602\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:3\tReward total:6\tRunning mean: -0.2264\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:5\tReward total:7\tRunning mean: 0.8488\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:6\tReward total:0\tRunning mean: 0.07118\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:7\tReward total:2\tRunning mean: 0.3094\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:8\tReward total:0\tRunning mean: 0.09363\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Learner:9\tReward total:0\tRunning mean: 0.5233\tNum agents crossed: 0\tRunning mean: 0.2354\n",
      "Max Norms =  ['0.00', '140.21', '138.56', '121.57', '233.42', '110.62', '0.00', '97.62', '0.00', '0.00']\n",
      "....................\n",
      "Episode 360 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.321\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:1\tReward total:12\tRunning mean: 4.092\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:2\tReward total:1\tRunning mean: 3.133\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:3\tReward total:2.0\tRunning mean: 0.5949\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:5\tReward total:3\tRunning mean: 1.056\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:6\tReward total:0\tRunning mean: 0.05822\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:7\tReward total:0\tRunning mean: 0.496\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:8\tReward total:0\tRunning mean: 0.1284\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Learner:9\tReward total:2\tRunning mean: 0.321\tNum agents crossed: 0\tRunning mean: 0.2564\n",
      "Max Norms =  ['0.00', '83.17', '111.99', '130.77', '172.26', '101.39', '0.00', '0.00', '0.00', '101.08']\n",
      "....................\n",
      "Episode 380 complete\n",
      "Learner:0\tReward total:15\tRunning mean: 2.619\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:1\tReward total:4\tRunning mean: 4.764\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:2\tReward total:4\tRunning mean: 3.586\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:3\tReward total:7\tRunning mean: 1.677\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:5\tReward total:3\tRunning mean: 1.118\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:6\tReward total:-15.0\tRunning mean: -0.06534\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:7\tReward total:0\tRunning mean: 0.5189\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:8\tReward total:0\tRunning mean: 0.1133\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Learner:9\tReward total:0\tRunning mean: 0.2717\tNum agents crossed: 0\tRunning mean: 0.2462\n",
      "Max Norms =  ['112.88', '119.94', '126.60', '107.26', '223.33', '92.28', '103.02', '0.00', '0.00', '0.00']\n",
      "....................\n",
      "Episode 400 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 2.951\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:1\tReward total:2\tRunning mean: 3.716\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:2\tReward total:13\tRunning mean: 4.479\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:3\tReward total:8\tRunning mean: 2.806\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:5\tReward total:9\tRunning mean: 1.409\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:6\tReward total:0\tRunning mean: -0.0451\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:7\tReward total:0\tRunning mean: 0.9832\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:8\tReward total:0\tRunning mean: 0.09271\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Learner:9\tReward total:0.0\tRunning mean: 0.2863\tNum agents crossed: 1\tRunning mean: 0.248\n",
      "Max Norms =  ['147.10', '87.00', '112.81', '124.36', '171.73', '101.44', '0.00', '0.00', '117.54', '103.15']\n",
      "....................\n",
      "Episode 420 complete\n",
      "Learner:0\tReward total:9\tRunning mean: 3.304\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:1\tReward total:3\tRunning mean: 2.555\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:2\tReward total:7\tRunning mean: 5.233\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:3\tReward total:9\tRunning mean: 3.046\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:5\tReward total:2\tRunning mean: 1.045\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:6\tReward total:0\tRunning mean: 0.000819\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:7\tReward total:9\tRunning mean: 1.655\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:8\tReward total:0\tRunning mean: 0.08524\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Learner:9\tReward total:-4.0\tRunning mean: 0.2609\tNum agents crossed: 0\tRunning mean: 0.2599\n",
      "Max Norms =  ['133.77', '146.97', '135.09', '83.99', '196.90', '119.74', '0.00', '92.45', '0.00', '116.70']\n",
      "....................\n",
      "Episode 440 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 3.324\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:1\tReward total:8\tRunning mean: 2.239\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:2\tReward total:13\tRunning mean: 5.802\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:3\tReward total:6\tRunning mean: 4.067\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:5\tReward total:-3.0\tRunning mean: 1.399\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0006698\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:7\tReward total:5\tRunning mean: 2.772\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:8\tReward total:1\tRunning mean: 0.07972\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Learner:9\tReward total:1.0\tRunning mean: 0.6783\tNum agents crossed: 1\tRunning mean: 0.3405\n",
      "Max Norms =  ['76.78', '139.31', '119.53', '91.95', '195.35', '147.74', '0.00', '105.79', '110.45', '84.54']\n",
      "....................\n",
      "Episode 460 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 3.667\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:1\tReward total:7\tRunning mean: 2.74\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:2\tReward total:6\tRunning mean: 6.069\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:3\tReward total:10\tRunning mean: 4.701\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:5\tReward total:2\tRunning mean: 1.162\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0005479\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:7\tReward total:11\tRunning mean: 4.033\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:8\tReward total:0\tRunning mean: -0.339\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Learner:9\tReward total:-9.0\tRunning mean: 1.05\tNum agents crossed: 0\tRunning mean: 0.3597\n",
      "Max Norms =  ['80.10', '87.65', '180.67', '136.68', '134.18', '121.79', '0.00', '131.86', '0.00', '75.60']\n",
      "....................\n",
      "Episode 480 complete\n",
      "Learner:0\tReward total:-33.0\tRunning mean: 3.818\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:1\tReward total:-46.0\tRunning mean: 2.66\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:2\tReward total:5\tRunning mean: 6.065\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:3\tReward total:12\tRunning mean: 4.859\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:5\tReward total:1\tRunning mean: 1.337\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0004481\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:7\tReward total:22\tRunning mean: 5.682\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:8\tReward total:0\tRunning mean: -0.2504\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Learner:9\tReward total:6.0\tRunning mean: 1.418\tNum agents crossed: 1\tRunning mean: 0.3779\n",
      "Max Norms =  ['179.92', '143.71', '165.50', '119.35', '146.09', '85.85', '0.00', '168.88', '0.00', '74.83']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 500 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 4.235\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:1\tReward total:5\tRunning mean: 2.991\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:2\tReward total:9\tRunning mean: 5.877\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:3\tReward total:2\tRunning mean: 4.633\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:5\tReward total:3\tRunning mean: 1.565\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:6\tReward total:0\tRunning mean: 0.03516\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:7\tReward total:12\tRunning mean: 8.076\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:8\tReward total:0\tRunning mean: -0.1751\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Learner:9\tReward total:0.0\tRunning mean: 1.92\tNum agents crossed: 0\tRunning mean: 0.41\n",
      "Max Norms =  ['127.59', '115.55', '133.68', '74.55', '168.11', '122.32', '0.00', '62.04', '0.00', '80.22']\n",
      "....................\n",
      "Episode 520 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 4.553\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:1\tReward total:2\tRunning mean: 3.274\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:2\tReward total:8\tRunning mean: 5.678\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:3\tReward total:7\tRunning mean: 4.46\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:5\tReward total:3\tRunning mean: 1.707\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:6\tReward total:0\tRunning mean: 0.04623\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:7\tReward total:24\tRunning mean: 10.47\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:8\tReward total:0\tRunning mean: -0.1258\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Learner:9\tReward total:13.0\tRunning mean: 2.628\tNum agents crossed: 1\tRunning mean: 0.4434\n",
      "Max Norms =  ['135.92', '95.05', '227.23', '98.38', '174.52', '147.10', '0.00', '71.35', '0.00', '76.93']\n",
      "....................\n",
      "Episode 540 complete\n",
      "Learner:0\tReward total:8\tRunning mean: 4.828\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:1\tReward total:8\tRunning mean: 3.483\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:2\tReward total:11\tRunning mean: 5.559\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:3\tReward total:3\tRunning mean: 4.475\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:5\tReward total:3\tRunning mean: 1.878\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:6\tReward total:0\tRunning mean: 0.03781\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:7\tReward total:14\tRunning mean: 12.56\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:8\tReward total:0\tRunning mean: -0.07496\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Learner:9\tReward total:0\tRunning mean: 3.256\tNum agents crossed: 0\tRunning mean: 0.4609\n",
      "Max Norms =  ['99.61', '88.82', '151.39', '92.86', '175.83', '132.20', '0.00', '111.13', '0.00', '0.00']\n",
      "....................\n",
      "Episode 560 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 5.17\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:1\tReward total:4\tRunning mean: 2.732\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:2\tReward total:3\tRunning mean: 5.692\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:3\tReward total:5\tRunning mean: 4.632\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:5\tReward total:8\tRunning mean: 1.765\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:6\tReward total:0\tRunning mean: 0.04985\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:7\tReward total:19\tRunning mean: 13.98\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:8\tReward total:0\tRunning mean: -0.06131\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Learner:9\tReward total:2.0\tRunning mean: 4.379\tNum agents crossed: 0\tRunning mean: 0.4583\n",
      "Max Norms =  ['95.18', '73.46', '129.87', '138.32', '198.86', '99.27', '0.00', '50.66', '0.00', '75.05']\n",
      "....................\n",
      "Episode 580 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 5.223\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:1\tReward total:3\tRunning mean: 2.99\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:2\tReward total:4\tRunning mean: 5.679\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:3\tReward total:1\tRunning mean: 4.722\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:5\tReward total:0\tRunning mean: 1.992\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:6\tReward total:0\tRunning mean: 0.07606\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:7\tReward total:24\tRunning mean: 15.07\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:8\tReward total:0\tRunning mean: -0.04073\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Learner:9\tReward total:8\tRunning mean: 5.404\tNum agents crossed: 0\tRunning mean: 0.4662\n",
      "Max Norms =  ['160.77', '85.34', '146.47', '114.27', '164.30', '0.00', '0.00', '180.74', '0.00', '118.44']\n",
      "....................\n",
      "Episode 600 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 5.228\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:1\tReward total:0\tRunning mean: 2.875\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:2\tReward total:8\tRunning mean: 5.975\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:3\tReward total:3\tRunning mean: 4.89\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:5\tReward total:5\tRunning mean: 2.164\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1185\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:7\tReward total:21\tRunning mean: 15.67\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:8\tReward total:0\tRunning mean: -0.02497\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Learner:9\tReward total:0.0\tRunning mean: 6.189\tNum agents crossed: 0\tRunning mean: 0.4522\n",
      "Max Norms =  ['93.76', '99.11', '106.14', '105.49', '168.67', '124.84', '0.00', '46.11', '0.00', '134.05']\n",
      "....................\n",
      "Episode 620 complete\n",
      "Learner:0\tReward total:3\tRunning mean: 5.298\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:1\tReward total:0\tRunning mean: 2.898\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:2\tReward total:3\tRunning mean: 5.668\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:3\tReward total:3\tRunning mean: 4.852\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:5\tReward total:6\tRunning mean: 2.515\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:6\tReward total:0\tRunning mean: 0.02377\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:7\tReward total:31\tRunning mean: 16.72\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:8\tReward total:0\tRunning mean: -0.02042\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Learner:9\tReward total:5\tRunning mean: 7.609\tNum agents crossed: 0\tRunning mean: 0.4347\n",
      "Max Norms =  ['57.42', '26.81', '135.07', '88.21', '157.95', '110.91', '0.00', '69.72', '0.00', '107.33']\n",
      "....................\n",
      "Episode 640 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 4.729\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:1\tReward total:3\tRunning mean: 3.02\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:2\tReward total:5\tRunning mean: 5.39\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:3\tReward total:4\tRunning mean: 4.548\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:5\tReward total:0\tRunning mean: 2.603\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0463\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:7\tReward total:27\tRunning mean: 18.08\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:8\tReward total:0\tRunning mean: -0.0946\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Learner:9\tReward total:1.0\tRunning mean: 10.06\tNum agents crossed: 0\tRunning mean: 0.4652\n",
      "Max Norms =  ['70.30', '115.18', '117.45', '62.18', '169.54', '0.00', '0.00', '55.48', '0.00', '95.67']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 660 complete\n",
      "Learner:0\tReward total:9\tRunning mean: 5.078\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:1\tReward total:7\tRunning mean: 3.095\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:2\tReward total:5\tRunning mean: 5.383\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:3\tReward total:5\tRunning mean: 4.206\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:5\tReward total:4\tRunning mean: 2.829\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:6\tReward total:0\tRunning mean: 0.05767\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:7\tReward total:14\tRunning mean: 18.78\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:8\tReward total:2\tRunning mean: -0.1497\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Learner:9\tReward total:14.0\tRunning mean: 10.58\tNum agents crossed: 1\tRunning mean: 0.4745\n",
      "Max Norms =  ['96.06', '75.18', '97.51', '60.33', '169.71', '128.83', '0.00', '49.15', '97.18', '130.76']\n",
      "....................\n",
      "Episode 680 complete\n",
      "Learner:0\tReward total:8\tRunning mean: 5.44\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:1\tReward total:1\tRunning mean: 2.93\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:2\tReward total:8\tRunning mean: 5.174\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:3\tReward total:1\tRunning mean: 4.354\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:5\tReward total:3\tRunning mean: 3.057\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1134\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:7\tReward total:27\tRunning mean: 18.95\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:8\tReward total:-1.0\tRunning mean: -0.6836\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Learner:9\tReward total:49.0\tRunning mean: 13.19\tNum agents crossed: 1\tRunning mean: 0.5257\n",
      "Max Norms =  ['116.17', '87.76', '121.29', '101.38', '226.73', '103.04', '0.00', '41.16', '85.32', '87.25']\n",
      "....................\n",
      "Episode 700 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 5.68\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:1\tReward total:2\tRunning mean: 2.756\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:2\tReward total:4\tRunning mean: 5.084\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:3\tReward total:2\tRunning mean: 4.394\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:5\tReward total:6\tRunning mean: 3.076\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1785\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:7\tReward total:29\tRunning mean: 19.19\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:8\tReward total:0\tRunning mean: -0.7272\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Learner:9\tReward total:5\tRunning mean: 16.52\tNum agents crossed: 0\tRunning mean: 0.5746\n",
      "Max Norms =  ['85.69', '161.39', '134.18', '67.72', '269.26', '94.04', '0.00', '60.02', '0.00', '77.46']\n",
      "....................\n",
      "Episode 720 complete\n",
      "Learner:0\tReward total:5\tRunning mean: 5.979\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:1\tReward total:11\tRunning mean: 2.89\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:2\tReward total:1\tRunning mean: 4.93\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:3\tReward total:11\tRunning mean: 4.481\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:5\tReward total:2\tRunning mean: 3.296\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:6\tReward total:3\tRunning mean: 0.1432\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:7\tReward total:16\tRunning mean: 19.4\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:8\tReward total:-12.0\tRunning mean: -0.9075\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Learner:9\tReward total:55.0\tRunning mean: 20.0\tNum agents crossed: 1\tRunning mean: 0.6253\n",
      "Max Norms =  ['72.81', '88.22', '71.37', '54.66', '142.58', '118.53', '124.59', '54.77', '86.09', '131.15']\n",
      "....................\n",
      "Episode 740 complete\n",
      "Learner:0\tReward total:4\tRunning mean: 6.148\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:1\tReward total:4\tRunning mean: 2.34\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:2\tReward total:5\tRunning mean: 4.833\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:3\tReward total:7\tRunning mean: 4.543\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:5\tReward total:6\tRunning mean: 3.247\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:6\tReward total:0\tRunning mean: 0.2218\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:7\tReward total:18\tRunning mean: 19.36\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:8\tReward total:0\tRunning mean: -0.9958\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Learner:9\tReward total:22.0\tRunning mean: 23.74\tNum agents crossed: 1\tRunning mean: 0.6557\n",
      "Max Norms =  ['105.59', '81.65', '114.91', '103.85', '156.28', '94.25', '0.00', '56.80', '0.00', '122.98']\n",
      "....................\n",
      "Episode 760 complete\n",
      "Learner:0\tReward total:6\tRunning mean: 6.222\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:1\tReward total:0\tRunning mean: 2.509\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:2\tReward total:6\tRunning mean: 4.606\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:3\tReward total:5\tRunning mean: 4.442\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:5\tReward total:4\tRunning mean: 3.877\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1307\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:7\tReward total:21\tRunning mean: 19.43\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:8\tReward total:0\tRunning mean: -0.7597\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Learner:9\tReward total:8\tRunning mean: 26.61\tNum agents crossed: 0\tRunning mean: 0.7432\n",
      "Max Norms =  ['70.39', '122.39', '82.16', '90.55', '137.74', '160.84', '0.00', '44.58', '0.00', '84.15']\n",
      "....................\n",
      "Episode 780 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 6.004\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:1\tReward total:7\tRunning mean: 2.488\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:2\tReward total:5\tRunning mean: 4.82\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:3\tReward total:0\tRunning mean: 4.526\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:5\tReward total:-4.0\tRunning mean: 4.011\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:6\tReward total:3.0\tRunning mean: 0.2343\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:7\tReward total:21\tRunning mean: 19.48\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:8\tReward total:0\tRunning mean: -0.5946\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Learner:9\tReward total:61.0\tRunning mean: 32.22\tNum agents crossed: 2\tRunning mean: 0.7996\n",
      "Max Norms =  ['115.46', '61.84', '105.17', '59.05', '160.64', '129.69', '62.53', '31.01', '0.00', '131.72']\n",
      "....................\n",
      "Episode 800 complete\n",
      "Learner:0\tReward total:2\tRunning mean: 5.763\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:1\tReward total:5\tRunning mean: 2.61\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:2\tReward total:4\tRunning mean: 4.668\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:3\tReward total:0\tRunning mean: 4.511\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:5\tReward total:5\tRunning mean: 4.568\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:6\tReward total:0\tRunning mean: 0.5367\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:7\tReward total:26\tRunning mean: 20.15\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:8\tReward total:0\tRunning mean: -0.4302\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Learner:9\tReward total:74.0\tRunning mean: 36.7\tNum agents crossed: 1\tRunning mean: 0.9186\n",
      "Max Norms =  ['82.72', '49.13', '155.48', '64.56', '197.08', '152.05', '0.00', '88.79', '0.00', '137.13']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 820 complete\n",
      "Learner:0\tReward total:4\tRunning mean: 5.321\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:1\tReward total:8\tRunning mean: 2.688\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:2\tReward total:5\tRunning mean: 4.534\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:3\tReward total:0\tRunning mean: 4.405\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:5\tReward total:2\tRunning mean: 4.919\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:6\tReward total:0\tRunning mean: 0.9429\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:7\tReward total:27\tRunning mean: 21.08\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:8\tReward total:0\tRunning mean: -0.2763\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Learner:9\tReward total:73.0\tRunning mean: 40.18\tNum agents crossed: 1\tRunning mean: 0.9992\n",
      "Max Norms =  ['191.02', '99.42', '128.86', '58.09', '205.43', '98.94', '0.00', '45.82', '0.00', '93.07']\n",
      "....................\n",
      "Episode 840 complete\n",
      "Learner:0\tReward total:3\tRunning mean: 5.304\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:1\tReward total:11\tRunning mean: 2.887\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:2\tReward total:13\tRunning mean: 4.598\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:3\tReward total:3\tRunning mean: 4.483\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:5\tReward total:45.0\tRunning mean: 6.977\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:6\tReward total:5.0\tRunning mean: 1.696\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:7\tReward total:15\tRunning mean: 20.98\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:8\tReward total:0\tRunning mean: -0.1883\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Learner:9\tReward total:18.0\tRunning mean: 42.89\tNum agents crossed: 3\tRunning mean: 1.182\n",
      "Max Norms =  ['148.10', '166.84', '142.27', '37.83', '157.36', '102.34', '81.52', '74.47', '0.00', '115.01']\n",
      "....................\n",
      "Episode 860 complete\n",
      "Learner:0\tReward total:4\tRunning mean: 4.515\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:1\tReward total:8\tRunning mean: 3.121\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:2\tReward total:2\tRunning mean: 4.511\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:3\tReward total:5\tRunning mean: 4.405\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:5\tReward total:5\tRunning mean: 7.648\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:6\tReward total:8\tRunning mean: 2.425\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:7\tReward total:18\tRunning mean: 20.54\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:8\tReward total:0\tRunning mean: -0.08246\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:9\tReward total:61.0\tRunning mean: 46.79\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Max Norms =  ['57.09', '114.75', '99.14', '61.95', '148.29', '129.61', '80.50', '21.52', '0.00', '121.09']\n",
      "....................\n",
      "Episode 880 complete\n",
      "Learner:0\tReward total:3\tRunning mean: 4.507\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:1\tReward total:2\tRunning mean: 3.33\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:2\tReward total:8\tRunning mean: 4.246\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:3\tReward total:9\tRunning mean: 4.2\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:5\tReward total:23.0\tRunning mean: 8.239\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:6\tReward total:5.0\tRunning mean: 3.015\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:7\tReward total:24\tRunning mean: 20.4\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:8\tReward total:0\tRunning mean: -0.04947\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Learner:9\tReward total:60.0\tRunning mean: 49.59\tNum agents crossed: 3\tRunning mean: 1.331\n",
      "Max Norms =  ['95.72', '116.40', '70.94', '54.57', '124.50', '112.84', '97.08', '40.10', '0.00', '100.16']\n",
      "....................\n",
      "Episode 900 complete\n",
      "Learner:0\tReward total:5\tRunning mean: 4.477\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:1\tReward total:7\tRunning mean: 3.648\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:2\tReward total:1\tRunning mean: 4.074\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:3\tReward total:4\tRunning mean: 3.972\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:5\tReward total:10.0\tRunning mean: 8.699\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:6\tReward total:15.0\tRunning mean: 4.081\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:7\tReward total:21\tRunning mean: 20.52\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:8\tReward total:1\tRunning mean: 0.004225\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Learner:9\tReward total:47.0\tRunning mean: 49.94\tNum agents crossed: 3\tRunning mean: 1.379\n",
      "Max Norms =  ['63.85', '106.79', '70.94', '48.83', '107.84', '133.48', '62.69', '36.06', '32.33', '113.44']\n",
      "....................\n",
      "Episode 920 complete\n",
      "Learner:0\tReward total:12\tRunning mean: 4.423\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:1\tReward total:1\tRunning mean: 3.572\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:2\tReward total:2\tRunning mean: 4.192\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:3\tReward total:0\tRunning mean: 3.577\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:5\tReward total:6\tRunning mean: 7.984\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:6\tReward total:2.0\tRunning mean: 4.293\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:7\tReward total:25\tRunning mean: 20.65\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:8\tReward total:0\tRunning mean: 0.04983\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Learner:9\tReward total:99.0\tRunning mean: 54.68\tNum agents crossed: 2\tRunning mean: 1.393\n",
      "Max Norms =  ['83.91', '93.76', '64.05', '45.91', '191.24', '123.07', '74.10', '58.37', '0.00', '116.05']\n",
      "....................\n",
      "Episode 940 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 4.305\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:1\tReward total:0\tRunning mean: 3.503\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:2\tReward total:2\tRunning mean: 3.828\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:3\tReward total:0\tRunning mean: 3.51\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:5\tReward total:12\tRunning mean: 8.075\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:6\tReward total:18\tRunning mean: 5.006\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:7\tReward total:16\tRunning mean: 20.5\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:8\tReward total:0\tRunning mean: 0.1487\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Learner:9\tReward total:42.0\tRunning mean: 58.31\tNum agents crossed: 1\tRunning mean: 1.367\n",
      "Max Norms =  ['64.48', '0.00', '71.99', '45.95', '124.28', '90.25', '22.21', '53.06', '0.00', '144.31']\n",
      "....................\n",
      "Episode 960 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 4.164\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:1\tReward total:5\tRunning mean: 3.418\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:2\tReward total:0\tRunning mean: 3.535\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:3\tReward total:0\tRunning mean: 3.242\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:5\tReward total:7\tRunning mean: 8.264\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:6\tReward total:0\tRunning mean: 5.651\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:7\tReward total:24\tRunning mean: 20.85\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:8\tReward total:0\tRunning mean: 0.2219\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Learner:9\tReward total:90.0\tRunning mean: 59.25\tNum agents crossed: 1\tRunning mean: 1.318\n",
      "Max Norms =  ['91.42', '104.48', '87.24', '44.05', '148.17', '94.42', '0.00', '40.32', '0.00', '140.95']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 980 complete\n",
      "Learner:0\tReward total:5\tRunning mean: 3.973\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:1\tReward total:8\tRunning mean: 3.295\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:2\tReward total:6\tRunning mean: 3.283\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:3\tReward total:2\tRunning mean: 3.216\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:5\tReward total:3\tRunning mean: 8.12\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:6\tReward total:12.0\tRunning mean: 6.733\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:7\tReward total:20\tRunning mean: 20.16\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:8\tReward total:0\tRunning mean: 0.3057\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Learner:9\tReward total:91.0\tRunning mean: 61.81\tNum agents crossed: 2\tRunning mean: 1.307\n",
      "Max Norms =  ['107.67', '82.81', '89.21', '48.80', '206.10', '74.20', '99.84', '43.82', '0.00', '147.98']\n",
      "....................\n",
      "Episode 1000 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 3.914\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:1\tReward total:0\tRunning mean: 3.308\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:2\tReward total:0\tRunning mean: 2.982\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:3\tReward total:0\tRunning mean: 3.067\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:5\tReward total:-2.0\tRunning mean: 7.851\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:6\tReward total:10\tRunning mean: 7.453\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:7\tReward total:21\tRunning mean: 20.07\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:8\tReward total:1\tRunning mean: 0.4742\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Learner:9\tReward total:100.0\tRunning mean: 63.73\tNum agents crossed: 1\tRunning mean: 1.304\n",
      "Max Norms =  ['107.32', '59.79', '144.68', '35.95', '165.10', '105.97', '97.13', '19.48', '31.57', '140.91']\n",
      "....................\n",
      "Episode 1020 complete\n",
      "Learner:0\tReward total:7\tRunning mean: 3.921\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:1\tReward total:1\tRunning mean: 2.947\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:2\tReward total:0\tRunning mean: 2.854\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:3\tReward total:0\tRunning mean: 2.899\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:5\tReward total:7\tRunning mean: 7.718\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:6\tReward total:6.0\tRunning mean: 8.584\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:7\tReward total:16\tRunning mean: 19.83\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:8\tReward total:2\tRunning mean: 0.6363\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:9\tReward total:94.0\tRunning mean: 67.09\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Max Norms =  ['76.57', '98.51', '80.19', '63.10', '158.59', '101.54', '123.11', '38.82', '147.02', '150.81']\n",
      "....................\n",
      "Episode 1040 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 3.32\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:1\tReward total:5\tRunning mean: 2.974\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:2\tReward total:4\tRunning mean: 2.884\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:3\tReward total:0\tRunning mean: 2.786\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:5\tReward total:1\tRunning mean: 7.273\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:6\tReward total:14.0\tRunning mean: 9.247\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:7\tReward total:14\tRunning mean: 20.54\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:8\tReward total:4\tRunning mean: 0.8517\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Learner:9\tReward total:84.0\tRunning mean: 67.95\tNum agents crossed: 1\tRunning mean: 1.347\n",
      "Max Norms =  ['64.07', '56.14', '118.16', '78.50', '181.83', '153.99', '35.48', '47.66', '168.56', '221.54']\n",
      "....................\n",
      "Episode 1060 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 2.797\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:1\tReward total:0\tRunning mean: 2.784\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:2\tReward total:3\tRunning mean: 2.654\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:3\tReward total:0\tRunning mean: 2.58\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:5\tReward total:0\tRunning mean: 7.146\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:6\tReward total:27.0\tRunning mean: 9.821\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:7\tReward total:34\tRunning mean: 21.3\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:8\tReward total:6\tRunning mean: 1.307\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Learner:9\tReward total:64.0\tRunning mean: 68.67\tNum agents crossed: 2\tRunning mean: 1.365\n",
      "Max Norms =  ['109.36', '49.01', '65.90', '46.91', '267.18', '0.00', '102.51', '64.37', '78.33', '171.10']\n",
      "....................\n",
      "Episode 1080 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 2.409\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:1\tReward total:0\tRunning mean: 2.551\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:2\tReward total:2\tRunning mean: 2.317\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:3\tReward total:0\tRunning mean: 2.218\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:5\tReward total:6\tRunning mean: 7.024\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:6\tReward total:22.0\tRunning mean: 10.05\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:7\tReward total:20\tRunning mean: 21.45\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:8\tReward total:17\tRunning mean: 2.284\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Learner:9\tReward total:62.0\tRunning mean: 67.89\tNum agents crossed: 2\tRunning mean: 1.327\n",
      "Max Norms =  ['35.02', '0.00', '39.66', '0.00', '139.28', '106.07', '129.62', '16.20', '47.55', '137.22']\n",
      "....................\n",
      "Episode 1100 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 2.043\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:1\tReward total:0\tRunning mean: 2.287\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:2\tReward total:0\tRunning mean: 1.981\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:3\tReward total:0\tRunning mean: 1.944\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:5\tReward total:14\tRunning mean: 7.082\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:6\tReward total:16\tRunning mean: 10.25\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:7\tReward total:16\tRunning mean: 21.0\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:8\tReward total:4\tRunning mean: 3.938\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Learner:9\tReward total:75.0\tRunning mean: 71.13\tNum agents crossed: 1\tRunning mean: 1.295\n",
      "Max Norms =  ['88.65', '80.36', '29.44', '79.32', '122.18', '39.50', '10.61', '11.34', '51.50', '171.10']\n",
      "....................\n",
      "Episode 1120 complete\n",
      "Learner:0\tReward total:2\tRunning mean: 1.779\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:1\tReward total:0\tRunning mean: 2.061\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:2\tReward total:0\tRunning mean: 1.62\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:3\tReward total:1\tRunning mean: 1.686\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:5\tReward total:1\tRunning mean: 6.775\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:6\tReward total:6\tRunning mean: 10.21\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:7\tReward total:21\tRunning mean: 20.46\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:8\tReward total:11\tRunning mean: 5.199\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Learner:9\tReward total:65.0\tRunning mean: 70.19\tNum agents crossed: 1\tRunning mean: 1.226\n",
      "Max Norms =  ['56.32', '52.23', '69.16', '75.38', '136.87', '128.04', '70.84', '7.30', '49.03', '202.42']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Episode 1140 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.649\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:1\tReward total:0\tRunning mean: 1.875\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:2\tReward total:0\tRunning mean: 1.402\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:3\tReward total:1\tRunning mean: 1.569\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:5\tReward total:3\tRunning mean: 7.383\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:6\tReward total:1\tRunning mean: 8.969\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:7\tReward total:40\tRunning mean: 20.33\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:8\tReward total:5\tRunning mean: 6.673\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Learner:9\tReward total:93.0\tRunning mean: 69.32\tNum agents crossed: 1\tRunning mean: 1.219\n",
      "Max Norms =  ['64.11', '51.16', '82.94', '150.75', '143.84', '110.06', '127.52', '12.14', '23.32', '203.53']\n",
      "....................\n",
      "Episode 1160 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.392\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:1\tReward total:3\tRunning mean: 1.626\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:2\tReward total:0\tRunning mean: 1.146\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:3\tReward total:-3.0\tRunning mean: -0.258\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:5\tReward total:0\tRunning mean: 7.349\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:6\tReward total:16\tRunning mean: 8.308\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:7\tReward total:18\tRunning mean: 19.75\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:8\tReward total:17\tRunning mean: 8.439\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Learner:9\tReward total:108.0\tRunning mean: 70.24\tNum agents crossed: 1\tRunning mean: 1.197\n",
      "Max Norms =  ['41.33', '77.20', '87.67', '133.55', '147.49', '0.00', '44.38', '7.22', '11.01', '142.54']\n",
      "....................\n",
      "Episode 1180 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.164\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:1\tReward total:14\tRunning mean: 1.542\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9936\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:3\tReward total:0\tRunning mean: -0.1356\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:5\tReward total:0\tRunning mean: 7.575\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:6\tReward total:4\tRunning mean: 7.548\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:7\tReward total:18\tRunning mean: 19.48\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:8\tReward total:18\tRunning mean: 9.789\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:9\tReward total:89.0\tRunning mean: 70.28\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Max Norms =  ['64.47', '51.75', '98.79', '119.33', '112.17', '0.00', '81.45', '5.20', '4.53', '133.32']\n",
      "....................\n",
      "Episode 1200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.023\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:1\tReward total:0\tRunning mean: 1.467\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9582\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1278\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:5\tReward total:17\tRunning mean: 7.875\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:6\tReward total:0\tRunning mean: 6.861\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:7\tReward total:17\tRunning mean: 19.16\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:8\tReward total:17\tRunning mean: 10.64\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Learner:9\tReward total:79.0\tRunning mean: 72.45\tNum agents crossed: 1\tRunning mean: 1.152\n",
      "Max Norms =  ['114.97', '44.93', '150.42', '107.93', '92.77', '17.77', '0.00', '0.90', '6.40', '182.32']\n",
      "....................\n",
      "Episode 1220 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.017\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:1\tReward total:3\tRunning mean: 1.485\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9703\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:3\tReward total:10\tRunning mean: 0.5298\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:5\tReward total:1\tRunning mean: 8.113\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:6\tReward total:4.0\tRunning mean: 6.578\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:7\tReward total:17\tRunning mean: 18.61\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:8\tReward total:17\tRunning mean: 11.71\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Learner:9\tReward total:85.0\tRunning mean: 70.75\tNum agents crossed: 2\tRunning mean: 1.163\n",
      "Max Norms =  ['75.64', '68.09', '78.60', '78.63', '242.88', '106.82', '78.03', '1.95', '8.70', '112.08']\n",
      "....................\n",
      "Episode 1240 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.35\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:1\tReward total:0\tRunning mean: 1.536\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:2\tReward total:0\tRunning mean: 1.012\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:3\tReward total:2\tRunning mean: 0.8252\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:5\tReward total:2\tRunning mean: 8.484\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:6\tReward total:15\tRunning mean: 6.958\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:7\tReward total:17\tRunning mean: 18.05\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:8\tReward total:18\tRunning mean: 12.07\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Learner:9\tReward total:0\tRunning mean: 67.48\tNum agents crossed: 0\tRunning mean: 1.179\n",
      "Max Norms =  ['57.46', '17.32', '105.49', '104.36', '104.53', '46.69', '23.13', '0.70', '10.55', '0.00']\n",
      "....................\n",
      "Episode 1260 complete\n",
      "Learner:0\tReward total:17\tRunning mean: 1.482\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:1\tReward total:5\tRunning mean: 1.08\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:2\tReward total:0\tRunning mean: 1.126\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:3\tReward total:6\tRunning mean: 0.7997\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:5\tReward total:0\tRunning mean: 9.523\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:6\tReward total:0\tRunning mean: 7.699\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:7\tReward total:0\tRunning mean: 17.56\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:8\tReward total:17\tRunning mean: 12.48\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:9\tReward total:76.0\tRunning mean: 63.95\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Max Norms =  ['106.50', '160.66', '60.85', '82.72', '185.30', '0.00', '0.00', '0.00', '6.29', '71.79']\n",
      "....................\n",
      "Episode 1280 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.365\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:1\tReward total:0\tRunning mean: 0.8718\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:2\tReward total:1\tRunning mean: 1.04\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:3\tReward total:0\tRunning mean: 0.7404\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:5\tReward total:0\tRunning mean: 10.36\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:6\tReward total:17\tRunning mean: 8.942\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:7\tReward total:16\tRunning mean: 17.3\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:8\tReward total:17\tRunning mean: 12.79\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Learner:9\tReward total:83.0\tRunning mean: 65.76\tNum agents crossed: 1\tRunning mean: 1.289\n",
      "Max Norms =  ['96.56', '34.64', '103.57', '89.55', '138.80', '0.00', '6.39', '8.13', '11.85', '159.04']\n",
      "....................\n",
      "Episode 1300 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.234\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:1\tReward total:17\tRunning mean: 1.356\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:2\tReward total:0\tRunning mean: 1.036\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:3\tReward total:0\tRunning mean: 0.6515\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:5\tReward total:0\tRunning mean: 10.23\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:6\tReward total:17\tRunning mean: 9.064\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:7\tReward total:3\tRunning mean: 16.67\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:8\tReward total:14\tRunning mean: 13.3\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Learner:9\tReward total:86.0\tRunning mean: 68.91\tNum agents crossed: 1\tRunning mean: 1.29\n",
      "Max Norms =  ['53.88', '22.97', '49.59', '101.99', '187.11', '0.00', '14.92', '4.63', '33.03', '167.85']\n",
      "....................\n",
      "Episode 1320 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.141\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:1\tReward total:0\tRunning mean: 1.329\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9232\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:3\tReward total:0\tRunning mean: 0.6081\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:5\tReward total:14\tRunning mean: 10.21\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:6\tReward total:7.0\tRunning mean: 8.853\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:7\tReward total:17\tRunning mean: 16.73\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:8\tReward total:17\tRunning mean: 13.4\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Learner:9\tReward total:77.0\tRunning mean: 71.19\tNum agents crossed: 2\tRunning mean: 1.283\n",
      "Max Norms =  ['102.32', '49.26', '133.80', '35.52', '246.23', '38.66', '111.51', '6.44', '32.65', '177.72']\n",
      "....................\n",
      "Episode 1340 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.124\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:1\tReward total:0\tRunning mean: 1.474\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:2\tReward total:0\tRunning mean: 0.8792\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:3\tReward total:0\tRunning mean: 0.6622\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:5\tReward total:7\tRunning mean: 9.687\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:6\tReward total:10\tRunning mean: 8.358\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:7\tReward total:17\tRunning mean: 16.64\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:8\tReward total:17\tRunning mean: 13.62\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Learner:9\tReward total:84.0\tRunning mean: 72.91\tNum agents crossed: 1\tRunning mean: 1.26\n",
      "Max Norms =  ['32.65', '108.92', '61.94', '79.07', '109.77', '72.20', '54.12', '2.34', '1.03', '164.45']\n",
      "....................\n",
      "Episode 1360 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.109\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:1\tReward total:0\tRunning mean: 1.272\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:2\tReward total:0\tRunning mean: 0.808\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:3\tReward total:0\tRunning mean: 0.7994\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:5\tReward total:10\tRunning mean: 9.259\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:6\tReward total:7\tRunning mean: 8.157\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:7\tReward total:17\tRunning mean: 16.59\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:8\tReward total:17\tRunning mean: 14.15\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Learner:9\tReward total:101.0\tRunning mean: 75.0\tNum agents crossed: 1\tRunning mean: 1.242\n",
      "Max Norms =  ['94.11', '81.82', '71.33', '77.01', '105.03', '60.36', '44.93', '9.43', '1.31', '183.53']\n",
      "....................\n",
      "Episode 1380 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.296\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:1\tReward total:1\tRunning mean: 1.248\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:2\tReward total:0\tRunning mean: 0.7868\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:3\tReward total:0\tRunning mean: 0.822\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:5\tReward total:50.0\tRunning mean: 10.85\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:6\tReward total:11\tRunning mean: 7.631\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:7\tReward total:18\tRunning mean: 16.34\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:8\tReward total:18\tRunning mean: 14.39\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Learner:9\tReward total:68.0\tRunning mean: 77.89\tNum agents crossed: 2\tRunning mean: 1.272\n",
      "Max Norms =  ['65.55', '119.38', '56.93', '57.55', '181.84', '158.23', '73.12', '2.61', '0.88', '263.47']\n",
      "....................\n",
      "Episode 1400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.069\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:1\tReward total:0\tRunning mean: 1.048\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:2\tReward total:0\tRunning mean: 0.7134\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9495\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:5\tReward total:17\tRunning mean: 10.64\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:6\tReward total:17\tRunning mean: 7.396\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:7\tReward total:17\tRunning mean: 16.37\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:8\tReward total:0\tRunning mean: 14.56\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Learner:9\tReward total:118.0\tRunning mean: 83.2\tNum agents crossed: 1\tRunning mean: 1.232\n",
      "Max Norms =  ['72.95', '67.59', '68.86', '41.72', '76.04', '24.21', '27.46', '0.97', '0.00', '141.76']\n",
      "....................\n",
      "Episode 1420 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.9806\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:1\tReward total:0\tRunning mean: 0.9653\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:2\tReward total:0\tRunning mean: 0.627\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:3\tReward total:0\tRunning mean: 0.8481\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:5\tReward total:0\tRunning mean: 11.32\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:6\tReward total:17\tRunning mean: 7.52\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:7\tReward total:17\tRunning mean: 16.45\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:8\tReward total:17\tRunning mean: 14.65\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Learner:9\tReward total:111.0\tRunning mean: 87.16\tNum agents crossed: 1\tRunning mean: 1.251\n",
      "Max Norms =  ['66.02', '93.16', '85.93', '21.53', '122.67', '0.00', '17.10', '0.68', '0.70', '126.61']\n",
      "....................\n",
      "Episode 1440 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.893\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:1\tReward total:0\tRunning mean: 1.167\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9481\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9696\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:5\tReward total:10\tRunning mean: 11.15\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:6\tReward total:7\tRunning mean: 7.597\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:7\tReward total:17\tRunning mean: 16.53\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:8\tReward total:17\tRunning mean: 14.82\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Learner:9\tReward total:103.0\tRunning mean: 89.64\tNum agents crossed: 1\tRunning mean: 1.259\n",
      "Max Norms =  ['132.96', '64.69', '71.66', '21.07', '51.54', '78.99', '39.49', '0.81', '0.70', '220.87']\n",
      "....................\n",
      "Episode 1460 complete\n",
      "Learner:0\tReward total:8\tRunning mean: 0.939\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:1\tReward total:0\tRunning mean: 1.121\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:2\tReward total:2\tRunning mean: 1.068\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9433\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:5\tReward total:6\tRunning mean: 10.53\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:6\tReward total:0\tRunning mean: 7.467\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:7\tReward total:17\tRunning mean: 16.59\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:8\tReward total:17\tRunning mean: 14.99\tNum agents crossed: 1\tRunning mean: 1.238\n",
      "Learner:9\tReward total:112.0\tRunning mean: 90.68\tNum agents crossed: 1\tRunning mean: 1.238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['34.83', '92.20', '37.51', '29.80', '227.36', '122.80', '0.00', '26.93', '1.29', '214.08']\n",
      "....................\n",
      "Episode 1480 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.8161\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:1\tReward total:1\tRunning mean: 0.9664\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:2\tReward total:1\tRunning mean: 0.8836\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:3\tReward total:0\tRunning mean: 0.8654\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:5\tReward total:5\tRunning mean: 11.81\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:6\tReward total:0\tRunning mean: 7.272\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:7\tReward total:17\tRunning mean: 16.71\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:8\tReward total:17\tRunning mean: 15.07\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Learner:9\tReward total:100.0\tRunning mean: 91.17\tNum agents crossed: 1\tRunning mean: 1.222\n",
      "Max Norms =  ['0.00', '76.10', '60.86', '86.03', '169.97', '113.13', '0.00', '9.04', '53.33', '216.23']\n",
      "....................\n",
      "Episode 1500 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.6849\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:1\tReward total:0\tRunning mean: 0.8722\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:2\tReward total:0\tRunning mean: 0.8225\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:3\tReward total:0\tRunning mean: 1.175\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:5\tReward total:17\tRunning mean: 11.47\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:6\tReward total:0\tRunning mean: 6.618\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:7\tReward total:17\tRunning mean: 16.45\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:8\tReward total:17\tRunning mean: 15.27\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Learner:9\tReward total:110.0\tRunning mean: 92.15\tNum agents crossed: 1\tRunning mean: 1.181\n",
      "Max Norms =  ['32.48', '37.76', '131.89', '56.04', '70.62', '28.01', '0.00', '12.96', '1.05', '226.51']\n",
      "....................\n",
      "Episode 1520 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.5602\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:1\tReward total:0\tRunning mean: 1.016\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:2\tReward total:0\tRunning mean: 0.8014\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9981\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:5\tReward total:17\tRunning mean: 11.06\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:6\tReward total:20.0\tRunning mean: 6.369\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:7\tReward total:19\tRunning mean: 16.51\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:8\tReward total:18\tRunning mean: 15.27\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Learner:9\tReward total:71.0\tRunning mean: 91.7\tNum agents crossed: 2\tRunning mean: 1.148\n",
      "Max Norms =  ['32.89', '22.37', '104.25', '24.02', '187.36', '54.94', '110.03', '23.58', '10.97', '254.15']\n",
      "....................\n",
      "Episode 1540 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.5018\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:1\tReward total:0\tRunning mean: 0.8926\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:2\tReward total:0\tRunning mean: 0.712\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:3\tReward total:0\tRunning mean: 0.902\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:5\tReward total:17\tRunning mean: 10.53\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:6\tReward total:0\tRunning mean: 7.127\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:7\tReward total:17\tRunning mean: 16.71\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:8\tReward total:17\tRunning mean: 15.02\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Learner:9\tReward total:109.0\tRunning mean: 91.77\tNum agents crossed: 1\tRunning mean: 1.121\n",
      "Max Norms =  ['22.63', '50.11', '126.71', '9.97', '92.86', '52.21', '0.00', '2.56', '0.33', '117.95']\n",
      "....................\n",
      "Episode 1560 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4807\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:1\tReward total:0\tRunning mean: 0.9716\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:2\tReward total:0\tRunning mean: 0.61\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9238\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:5\tReward total:3\tRunning mean: 10.5\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:6\tReward total:14\tRunning mean: 7.22\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:7\tReward total:17\tRunning mean: 16.59\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:8\tReward total:17\tRunning mean: 15.31\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Learner:9\tReward total:76.0\tRunning mean: 92.13\tNum agents crossed: 1\tRunning mean: 1.118\n",
      "Max Norms =  ['20.37', '101.42', '38.59', '54.56', '143.36', '58.46', '45.63', '5.06', '0.29', '198.05']\n",
      "....................\n",
      "Episode 1580 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4521\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:1\tReward total:0\tRunning mean: 0.8812\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:2\tReward total:0\tRunning mean: 0.5181\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:3\tReward total:0\tRunning mean: 0.8821\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:5\tReward total:17\tRunning mean: 9.931\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:6\tReward total:0\tRunning mean: 7.999\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:7\tReward total:17\tRunning mean: 16.35\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:8\tReward total:17\tRunning mean: 15.28\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Learner:9\tReward total:3\tRunning mean: 92.02\tNum agents crossed: 0\tRunning mean: 1.104\n",
      "Max Norms =  ['30.10', '155.49', '51.15', '27.38', '101.83', '32.45', '0.00', '1.23', '0.25', '264.18']\n",
      "....................\n",
      "Episode 1600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3698\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:1\tReward total:0\tRunning mean: 0.9859\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:2\tReward total:0\tRunning mean: 0.5462\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:3\tReward total:0\tRunning mean: 0.8083\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:5\tReward total:17\tRunning mean: 10.18\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:6\tReward total:0\tRunning mean: 7.257\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:7\tReward total:17\tRunning mean: 15.8\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:8\tReward total:17\tRunning mean: 15.57\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Learner:9\tReward total:119.0\tRunning mean: 95.05\tNum agents crossed: 1\tRunning mean: 1.094\n",
      "Max Norms =  ['28.36', '78.91', '112.34', '21.40', '110.99', '23.59', '0.00', '1.91', '2.05', '94.06']\n",
      "....................\n",
      "Episode 1620 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3025\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:1\tReward total:2\tRunning mean: 1.025\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:2\tReward total:0\tRunning mean: 0.4285\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:3\tReward total:0\tRunning mean: 0.6339\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:5\tReward total:4\tRunning mean: 9.306\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:6\tReward total:0\tRunning mean: 6.284\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:7\tReward total:17\tRunning mean: 15.98\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:8\tReward total:17\tRunning mean: 15.81\tNum agents crossed: 1\tRunning mean: 1.105\n",
      "Learner:9\tReward total:110.0\tRunning mean: 97.52\tNum agents crossed: 1\tRunning mean: 1.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['10.27', '68.69', '26.15', '3.22', '123.51', '122.15', '0.00', '1.88', '2.41', '201.12']\n",
      "....................\n",
      "Episode 1640 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4633\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:1\tReward total:0\tRunning mean: 1.173\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:2\tReward total:0\tRunning mean: 0.6447\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:3\tReward total:0\tRunning mean: 0.5185\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:5\tReward total:40.0\tRunning mean: 10.69\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:6\tReward total:0\tRunning mean: 5.453\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:7\tReward total:17\tRunning mean: 15.37\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:8\tReward total:17\tRunning mean: 15.97\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Learner:9\tReward total:73.0\tRunning mean: 95.79\tNum agents crossed: 2\tRunning mean: 1.151\n",
      "Max Norms =  ['37.24', '60.26', '11.77', '15.64', '242.00', '157.99', '0.00', '4.99', '1.38', '153.12']\n",
      "....................\n",
      "Episode 1660 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4676\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:1\tReward total:0\tRunning mean: 2.182\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:2\tReward total:0\tRunning mean: 0.5551\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:3\tReward total:0\tRunning mean: 0.4241\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:5\tReward total:41.0\tRunning mean: 12.28\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:6\tReward total:0\tRunning mean: 4.46\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:7\tReward total:17\tRunning mean: 15.23\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:8\tReward total:17\tRunning mean: 16.17\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Learner:9\tReward total:77.0\tRunning mean: 92.45\tNum agents crossed: 2\tRunning mean: 1.177\n",
      "Max Norms =  ['17.15', '4.10', '21.58', '14.32', '207.45', '100.99', '0.00', '2.16', '0.22', '146.11']\n",
      "....................\n",
      "Episode 1680 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4745\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:1\tReward total:0\tRunning mean: 2.1\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:2\tReward total:0\tRunning mean: 0.4541\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:3\tReward total:0\tRunning mean: 0.3468\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:5\tReward total:10\tRunning mean: 10.96\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:6\tReward total:7\tRunning mean: 3.746\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:7\tReward total:13\tRunning mean: 14.9\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:8\tReward total:9\tRunning mean: 16.27\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Learner:9\tReward total:0\tRunning mean: 84.67\tNum agents crossed: 0\tRunning mean: 1.078\n",
      "Max Norms =  ['11.36', '32.58', '35.80', '13.84', '197.72', '57.71', '131.67', '3.51', '17.75', '0.00']\n",
      "....................\n",
      "Episode 1700 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.5404\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:1\tReward total:1\tRunning mean: 1.957\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:2\tReward total:0\tRunning mean: 0.5512\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:3\tReward total:0\tRunning mean: 0.2837\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:5\tReward total:1\tRunning mean: 9.963\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:6\tReward total:0\tRunning mean: 3.064\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:7\tReward total:17\tRunning mean: 14.53\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:8\tReward total:18\tRunning mean: 16.46\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Learner:9\tReward total:0\tRunning mean: 75.48\tNum agents crossed: 0\tRunning mean: 0.9424\n",
      "Max Norms =  ['29.49', '21.66', '33.90', '15.45', '239.86', '131.26', '0.00', '6.36', '0.41', '0.00']\n",
      "....................\n",
      "Episode 1720 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4507\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:1\tReward total:0\tRunning mean: 1.728\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:2\tReward total:4\tRunning mean: 0.8073\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:3\tReward total:0\tRunning mean: 0.232\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:5\tReward total:0\tRunning mean: 9.897\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:6\tReward total:0\tRunning mean: 2.506\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:7\tReward total:17\tRunning mean: 14.54\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:8\tReward total:17\tRunning mean: 16.64\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Learner:9\tReward total:100.0\tRunning mean: 70.23\tNum agents crossed: 1\tRunning mean: 0.8797\n",
      "Max Norms =  ['12.42', '66.40', '81.22', '12.75', '205.42', '0.00', '0.00', '40.06', '0.19', '121.53']\n",
      "....................\n",
      "Episode 1740 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3686\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:1\tReward total:0\tRunning mean: 1.422\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:2\tReward total:0\tRunning mean: 1.415\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1898\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:5\tReward total:0\tRunning mean: 11.09\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:6\tReward total:0\tRunning mean: 2.365\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:7\tReward total:17\tRunning mean: 13.89\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:8\tReward total:17\tRunning mean: 16.74\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Learner:9\tReward total:108.0\tRunning mean: 70.3\tNum agents crossed: 1\tRunning mean: 0.8756\n",
      "Max Norms =  ['21.51', '36.33', '112.82', '78.66', '88.03', '0.00', '0.00', '0.70', '0.17', '75.16']\n",
      "....................\n",
      "Episode 1760 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.5087\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:1\tReward total:0\tRunning mean: 1.163\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:2\tReward total:1\tRunning mean: 1.777\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1552\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:5\tReward total:0\tRunning mean: 10.31\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:6\tReward total:0\tRunning mean: 2.061\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:7\tReward total:16\tRunning mean: 14.1\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:8\tReward total:17\tRunning mean: 16.79\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Learner:9\tReward total:93.0\tRunning mean: 74.44\tNum agents crossed: 1\tRunning mean: 0.9084\n",
      "Max Norms =  ['37.99', '40.26', '118.80', '4.35', '84.56', '0.00', '0.00', '49.85', '0.15', '153.98']\n",
      "....................\n",
      "Episode 1780 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4249\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:1\tReward total:0\tRunning mean: 0.9807\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:2\tReward total:0\tRunning mean: 2.77\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:3\tReward total:0\tRunning mean: 0.127\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:5\tReward total:17\tRunning mean: 9.902\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:6\tReward total:0\tRunning mean: 1.696\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:7\tReward total:0\tRunning mean: 13.67\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:8\tReward total:17\tRunning mean: 16.83\tNum agents crossed: 1\tRunning mean: 0.9523\n",
      "Learner:9\tReward total:113.0\tRunning mean: 77.56\tNum agents crossed: 1\tRunning mean: 0.9523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['18.56', '124.74', '45.21', '1.09', '187.27', '18.57', '0.00', '0.00', '0.16', '73.28']\n",
      "....................\n",
      "Episode 1800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3475\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:1\tReward total:2\tRunning mean: 0.9542\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:2\tReward total:0\tRunning mean: 3.809\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1038\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:5\tReward total:0\tRunning mean: 8.781\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:6\tReward total:0\tRunning mean: 1.557\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:7\tReward total:17\tRunning mean: 13.81\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:8\tReward total:17\tRunning mean: 16.86\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Learner:9\tReward total:121.0\tRunning mean: 83.4\tNum agents crossed: 1\tRunning mean: 0.9798\n",
      "Max Norms =  ['33.26', '135.72', '60.43', '1.40', '89.89', '0.00', '0.00', '0.71', '0.12', '100.86']\n",
      "....................\n",
      "Episode 1820 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3898\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:1\tReward total:2\tRunning mean: 1.263\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:2\tReward total:0\tRunning mean: 3.929\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:3\tReward total:0\tRunning mean: 0.08493\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:5\tReward total:0\tRunning mean: 8.642\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:6\tReward total:0\tRunning mean: 1.273\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:7\tReward total:17\tRunning mean: 13.86\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:8\tReward total:17\tRunning mean: 16.89\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Learner:9\tReward total:105.0\tRunning mean: 88.06\tNum agents crossed: 1\tRunning mean: 1.03\n",
      "Max Norms =  ['16.60', '154.82', '27.10', '2.19', '107.39', '0.00', '0.00', '0.58', '0.11', '132.46']\n",
      "....................\n",
      "Episode 1840 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3188\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:1\tReward total:0\tRunning mean: 1.663\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:2\tReward total:2\tRunning mean: 4.299\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:3\tReward total:0\tRunning mean: 0.06947\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:5\tReward total:0\tRunning mean: 7.748\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:6\tReward total:0\tRunning mean: 1.042\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:7\tReward total:17\tRunning mean: 14.09\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:8\tReward total:17\tRunning mean: 16.91\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Learner:9\tReward total:108.0\tRunning mean: 90.28\tNum agents crossed: 1\tRunning mean: 1.016\n",
      "Max Norms =  ['29.03', '74.14', '31.81', '54.23', '93.20', '0.00', '0.00', '0.61', '0.12', '134.36']\n",
      "....................\n",
      "Episode 1860 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.2776\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:1\tReward total:0\tRunning mean: 1.792\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:2\tReward total:4\tRunning mean: 4.689\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:3\tReward total:0\tRunning mean: 0.05682\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:5\tReward total:12\tRunning mean: 7.311\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:6\tReward total:0\tRunning mean: 1.046\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:7\tReward total:17\tRunning mean: 14.45\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:8\tReward total:17\tRunning mean: 16.93\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Learner:9\tReward total:110.0\tRunning mean: 93.39\tNum agents crossed: 1\tRunning mean: 1.023\n",
      "Max Norms =  ['48.25', '81.07', '35.13', '39.37', '91.19', '93.86', '0.00', '0.63', '0.10', '130.07']\n",
      "....................\n",
      "Episode 1880 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.2562\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:1\tReward total:0\tRunning mean: 1.61\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:2\tReward total:10\tRunning mean: 4.973\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04647\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:5\tReward total:0\tRunning mean: 6.986\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:6\tReward total:0\tRunning mean: 0.8555\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:7\tReward total:17\tRunning mean: 14.73\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:8\tReward total:17\tRunning mean: 16.95\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Learner:9\tReward total:109.0\tRunning mean: 96.43\tNum agents crossed: 1\tRunning mean: 1.018\n",
      "Max Norms =  ['93.68', '40.67', '115.54', '41.85', '147.27', '0.00', '0.00', '0.57', '0.09', '209.51']\n",
      "....................\n",
      "Episode 1900 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.2095\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:1\tReward total:0\tRunning mean: 1.572\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:2\tReward total:12\tRunning mean: 5.341\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03801\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:5\tReward total:0\tRunning mean: 6.018\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:6\tReward total:0\tRunning mean: 0.6997\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:7\tReward total:17\tRunning mean: 14.85\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:8\tReward total:17\tRunning mean: 16.96\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Learner:9\tReward total:113.0\tRunning mean: 98.82\tNum agents crossed: 1\tRunning mean: 1.006\n",
      "Max Norms =  ['77.85', '125.41', '33.44', '16.21', '74.30', '0.00', '0.00', '0.57', '0.10', '77.05']\n",
      "....................\n",
      "Episode 1920 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3989\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:1\tReward total:0\tRunning mean: 1.398\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:2\tReward total:4\tRunning mean: 5.476\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03109\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:5\tReward total:0\tRunning mean: 5.547\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:6\tReward total:0\tRunning mean: 0.5723\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:7\tReward total:17\tRunning mean: 14.78\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:8\tReward total:17\tRunning mean: 16.99\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Learner:9\tReward total:116.0\tRunning mean: 99.03\tNum agents crossed: 1\tRunning mean: 0.9787\n",
      "Max Norms =  ['0.00', '140.60', '48.29', '0.00', '73.03', '0.00', '0.00', '0.59', '0.54', '77.58']\n",
      "....................\n",
      "Episode 1940 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.484\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:1\tReward total:0\tRunning mean: 1.225\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:2\tReward total:2\tRunning mean: 5.371\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02543\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:5\tReward total:0\tRunning mean: 5.128\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:6\tReward total:0\tRunning mean: 0.6882\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:7\tReward total:17\tRunning mean: 14.72\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:8\tReward total:17\tRunning mean: 16.85\tNum agents crossed: 1\tRunning mean: 0.9911\n",
      "Learner:9\tReward total:119.0\tRunning mean: 101.2\tNum agents crossed: 1\tRunning mean: 0.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['26.12', '100.24', '194.01', '18.40', '39.50', '0.00', '0.00', '0.51', '0.09', '147.54']\n",
      "....................\n",
      "Episode 1960 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 0.4158\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:1\tReward total:1\tRunning mean: 1.141\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:2\tReward total:1\tRunning mean: 4.846\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0208\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:5\tReward total:0\tRunning mean: 5.028\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:6\tReward total:0\tRunning mean: 0.5629\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:7\tReward total:17\tRunning mean: 14.81\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:8\tReward total:17\tRunning mean: 16.88\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Learner:9\tReward total:115.0\tRunning mean: 103.3\tNum agents crossed: 1\tRunning mean: 0.9927\n",
      "Max Norms =  ['60.35', '148.03', '41.67', '4.97', '62.99', '0.00', '0.00', '0.49', '0.18', '148.98']\n",
      "....................\n",
      "Episode 1980 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3744\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:1\tReward total:1\tRunning mean: 1.252\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:2\tReward total:0\tRunning mean: 3.993\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:3\tReward total:0\tRunning mean: 0.01701\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:5\tReward total:0\tRunning mean: 4.445\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:6\tReward total:0\tRunning mean: 0.6173\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:7\tReward total:17\tRunning mean: 14.74\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:8\tReward total:17\tRunning mean: 16.92\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Learner:9\tReward total:107.0\tRunning mean: 104.9\tNum agents crossed: 1\tRunning mean: 0.994\n",
      "Max Norms =  ['0.00', '85.17', '12.60', '15.38', '102.68', '0.00', '0.00', '0.39', '0.23', '128.33']\n",
      "....................\n",
      "Episode 2000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.3062\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:1\tReward total:0\tRunning mean: 1.121\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:2\tReward total:1\tRunning mean: 3.413\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1773\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:5\tReward total:0\tRunning mean: 3.635\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:6\tReward total:0\tRunning mean: 0.5049\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:7\tReward total:17\tRunning mean: 15.01\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:8\tReward total:17\tRunning mean: 16.79\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Learner:9\tReward total:123.0\tRunning mean: 106.2\tNum agents crossed: 1\tRunning mean: 0.9865\n",
      "Max Norms =  ['48.40', '14.76', '54.09', '25.35', '101.99', '0.00', '0.00', '9.99', '0.17', '90.60']\n",
      "....................\n",
      "Episode 2020 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.2505\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:1\tReward total:0\tRunning mean: 1.127\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:2\tReward total:0\tRunning mean: 3.024\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:3\tReward total:0\tRunning mean: 0.2401\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:5\tReward total:0\tRunning mean: 3.501\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:6\tReward total:0\tRunning mean: 0.4129\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:7\tReward total:17\tRunning mean: 14.86\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:8\tReward total:17\tRunning mean: 16.82\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Learner:9\tReward total:126.0\tRunning mean: 107.9\tNum agents crossed: 1\tRunning mean: 0.989\n",
      "Max Norms =  ['19.22', '64.09', '15.86', '33.94', '77.43', '0.00', '0.00', '0.50', '0.14', '26.33']\n",
      "....................\n",
      "Episode 2040 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.2049\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:1\tReward total:0\tRunning mean: 1.193\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:2\tReward total:0\tRunning mean: 2.553\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1964\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:5\tReward total:0\tRunning mean: 3.683\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:6\tReward total:0\tRunning mean: 0.4915\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:7\tReward total:17\tRunning mean: 14.57\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:8\tReward total:17\tRunning mean: 16.89\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Learner:9\tReward total:121.0\tRunning mean: 109.0\tNum agents crossed: 1\tRunning mean: 0.9814\n",
      "Max Norms =  ['16.75', '30.79', '23.87', '19.86', '85.56', '0.00', '0.00', '0.69', '15.34', '41.44']\n",
      "....................\n",
      "Episode 2060 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1676\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:1\tReward total:-9.0\tRunning mean: 0.8857\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:2\tReward total:0\tRunning mean: 2.088\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1606\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:5\tReward total:17\tRunning mean: 5.887\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:6\tReward total:0\tRunning mean: 0.4109\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:7\tReward total:17\tRunning mean: 15.01\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:8\tReward total:17\tRunning mean: 16.91\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Learner:9\tReward total:123.0\tRunning mean: 111.1\tNum agents crossed: 1\tRunning mean: 0.9848\n",
      "Max Norms =  ['24.61', '113.47', '33.05', '22.99', '143.51', '24.02', '0.00', '0.47', '0.37', '120.97']\n",
      "....................\n",
      "Episode 2080 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.137\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:1\tReward total:0\tRunning mean: 0.7432\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:2\tReward total:0\tRunning mean: 1.708\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1403\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:5\tReward total:1\tRunning mean: 7.281\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:6\tReward total:14\tRunning mean: 0.7817\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:7\tReward total:17\tRunning mean: 15.37\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:8\tReward total:17\tRunning mean: 16.78\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Learner:9\tReward total:3.0\tRunning mean: 111.1\tNum agents crossed: 0\tRunning mean: 0.9868\n",
      "Max Norms =  ['22.93', '42.49', '75.75', '64.31', '67.32', '91.88', '86.63', '5.49', '0.23', '93.33']\n",
      "....................\n",
      "Episode 2100 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1121\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:1\tReward total:0\tRunning mean: 0.5994\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:2\tReward total:0\tRunning mean: 1.397\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1148\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:5\tReward total:0\tRunning mean: 8.525\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:6\tReward total:17\tRunning mean: 0.8276\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:7\tReward total:17\tRunning mean: 15.52\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:8\tReward total:17\tRunning mean: 16.82\tNum agents crossed: 1\tRunning mean: 0.9892\n",
      "Learner:9\tReward total:123.0\tRunning mean: 112.5\tNum agents crossed: 1\tRunning mean: 0.9892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['70.96', '132.73', '49.55', '38.53', '94.45', '0.00', '17.68', '0.66', '2.75', '105.52']\n",
      "....................\n",
      "Episode 2120 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.09168\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:1\tReward total:0\tRunning mean: 0.4902\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:2\tReward total:0\tRunning mean: 1.143\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:3\tReward total:0\tRunning mean: 0.09389\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:5\tReward total:17\tRunning mean: 9.781\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:6\tReward total:0\tRunning mean: 0.8202\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:7\tReward total:17\tRunning mean: 15.79\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:8\tReward total:17\tRunning mean: 16.85\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Learner:9\tReward total:127.0\tRunning mean: 114.3\tNum agents crossed: 1\tRunning mean: 0.9912\n",
      "Max Norms =  ['53.01', '45.54', '39.16', '19.84', '27.68', '24.89', '0.00', '0.91', '0.26', '2.92']\n",
      "....................\n",
      "Episode 2140 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07499\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:1\tReward total:0\tRunning mean: 0.4289\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9345\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1395\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:5\tReward total:18\tRunning mean: 10.73\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:6\tReward total:0\tRunning mean: 0.7354\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:7\tReward total:17\tRunning mean: 15.85\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:8\tReward total:6\tRunning mean: 16.73\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Learner:9\tReward total:124.0\tRunning mean: 114.8\tNum agents crossed: 1\tRunning mean: 0.9838\n",
      "Max Norms =  ['87.89', '67.58', '21.41', '0.83', '45.74', '29.54', '0.00', '0.76', '25.17', '87.25']\n",
      "....................\n",
      "Episode 2160 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.06133\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:1\tReward total:0\tRunning mean: 0.3679\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:2\tReward total:0\tRunning mean: 0.7643\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:3\tReward total:0\tRunning mean: 0.2658\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:5\tReward total:17\tRunning mean: 11.75\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:6\tReward total:0\tRunning mean: 0.8183\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:7\tReward total:17\tRunning mean: 15.79\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:8\tReward total:17\tRunning mean: 16.8\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Learner:9\tReward total:115.0\tRunning mean: 112.6\tNum agents crossed: 1\tRunning mean: 0.9771\n",
      "Max Norms =  ['80.49', '33.27', '51.21', '0.73', '151.77', '11.66', '0.00', '0.62', '0.13', '66.43']\n",
      "....................\n",
      "Episode 2180 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.05016\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:1\tReward total:0\tRunning mean: 0.3009\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:2\tReward total:0\tRunning mean: 0.6252\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:3\tReward total:0\tRunning mean: 0.2174\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:5\tReward total:17\tRunning mean: 12.11\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:6\tReward total:0\tRunning mean: 1.124\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:7\tReward total:17\tRunning mean: 15.87\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:8\tReward total:17\tRunning mean: 16.84\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Learner:9\tReward total:107.0\tRunning mean: 111.9\tNum agents crossed: 1\tRunning mean: 0.9813\n",
      "Max Norms =  ['28.34', '78.67', '17.43', '3.20', '42.52', '13.55', '0.00', '0.44', '0.10', '79.67']\n",
      "....................\n",
      "Episode 2200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04103\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:1\tReward total:0\tRunning mean: 0.2461\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:2\tReward total:0\tRunning mean: 0.549\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1778\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:5\tReward total:17\tRunning mean: 12.53\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:6\tReward total:17\tRunning mean: 1.089\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:7\tReward total:0\tRunning mean: 15.75\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:8\tReward total:17\tRunning mean: 16.87\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Learner:9\tReward total:103.0\tRunning mean: 111.0\tNum agents crossed: 1\tRunning mean: 0.9847\n",
      "Max Norms =  ['17.18', '46.74', '36.55', '0.68', '98.24', '8.78', '11.61', '0.00', '0.09', '199.41']\n",
      "....................\n",
      "Episode 2220 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.03356\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1647\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:2\tReward total:0\tRunning mean: 0.449\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1454\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:5\tReward total:17\tRunning mean: 13.08\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:6\tReward total:0\tRunning mean: 1.158\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:7\tReward total:17\tRunning mean: 15.97\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:8\tReward total:17\tRunning mean: 16.89\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Learner:9\tReward total:106.0\tRunning mean: 109.0\tNum agents crossed: 1\tRunning mean: 0.9875\n",
      "Max Norms =  ['45.66', '197.31', '7.59', '4.84', '65.86', '4.74', '0.00', '0.36', '0.08', '171.03']\n",
      "....................\n",
      "Episode 2240 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.02745\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1347\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:2\tReward total:0\tRunning mean: 0.3673\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:3\tReward total:0\tRunning mean: 0.119\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:5\tReward total:17\tRunning mean: 13.63\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:6\tReward total:0\tRunning mean: 0.9474\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:7\tReward total:17\tRunning mean: 16.16\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:8\tReward total:17\tRunning mean: 16.91\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Learner:9\tReward total:89.0\tRunning mean: 105.1\tNum agents crossed: 1\tRunning mean: 0.9898\n",
      "Max Norms =  ['32.53', '60.42', '22.42', '2.09', '85.71', '10.60', '0.00', '0.33', '0.07', '123.97']\n",
      "....................\n",
      "Episode 2260 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.02245\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1759\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:2\tReward total:0\tRunning mean: 0.3004\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:3\tReward total:0\tRunning mean: 0.09729\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:5\tReward total:17\tRunning mean: 14.25\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:6\tReward total:0\tRunning mean: 0.7657\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:7\tReward total:17\tRunning mean: 16.31\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:8\tReward total:17\tRunning mean: 16.93\tNum agents crossed: 1\tRunning mean: 0.9916\n",
      "Learner:9\tReward total:109.0\tRunning mean: 103.4\tNum agents crossed: 1\tRunning mean: 0.9916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['44.81', '98.25', '17.60', '97.85', '71.56', '31.59', '0.00', '0.22', '0.07', '47.23']\n",
      "....................\n",
      "Episode 2280 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01836\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:1\tReward total:0\tRunning mean: -1.123\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:2\tReward total:0\tRunning mean: 0.2457\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:3\tReward total:0\tRunning mean: 0.07097\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:5\tReward total:17\tRunning mean: 14.42\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:6\tReward total:0\tRunning mean: 1.22\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:7\tReward total:17\tRunning mean: 16.27\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:8\tReward total:17\tRunning mean: 16.94\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Learner:9\tReward total:112.0\tRunning mean: 104.8\tNum agents crossed: 1\tRunning mean: 1.003\n",
      "Max Norms =  ['8.86', '37.63', '14.11', '12.08', '23.94', '7.82', '0.00', '0.24', '0.06', '134.50']\n",
      "....................\n",
      "Episode 2300 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01502\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:1\tReward total:0\tRunning mean: -0.9188\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:2\tReward total:0\tRunning mean: 0.2009\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:3\tReward total:0\tRunning mean: -0.01344\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:5\tReward total:0\tRunning mean: 14.04\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:6\tReward total:0\tRunning mean: 1.342\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:7\tReward total:16\tRunning mean: 16.3\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:8\tReward total:18\tRunning mean: 17.04\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Learner:9\tReward total:122.0\tRunning mean: 107.1\tNum agents crossed: 1\tRunning mean: 1.028\n",
      "Max Norms =  ['5.87', '54.66', '12.89', '38.12', '138.69', '0.00', '0.00', '11.39', '1.94', '60.77']\n",
      "....................\n",
      "Episode 2320 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01228\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:1\tReward total:0\tRunning mean: -0.7515\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1644\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:3\tReward total:0\tRunning mean: -0.01099\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:5\tReward total:17\tRunning mean: 14.43\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:6\tReward total:0\tRunning mean: 1.46\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:7\tReward total:17\tRunning mean: 16.43\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:8\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Learner:9\tReward total:126.0\tRunning mean: 109.0\tNum agents crossed: 1\tRunning mean: 1.041\n",
      "Max Norms =  ['6.81', '112.45', '24.52', '20.54', '21.20', '17.13', '0.00', '0.19', '0.05', '57.35']\n",
      "....................\n",
      "Episode 2340 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01005\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:1\tReward total:0\tRunning mean: -0.6146\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1344\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:3\tReward total:0\tRunning mean: -0.008992\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:5\tReward total:0\tRunning mean: 14.42\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:6\tReward total:0\tRunning mean: 1.545\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:7\tReward total:17\tRunning mean: 16.53\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:8\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Learner:9\tReward total:124.0\tRunning mean: 110.7\tNum agents crossed: 1\tRunning mean: 1.052\n",
      "Max Norms =  ['7.93', '43.10', '35.98', '7.61', '31.03', '0.00', '0.00', '6.77', '0.05', '84.30']\n",
      "....................\n",
      "Episode 2360 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.008217\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:1\tReward total:0\tRunning mean: -0.5027\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1099\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:3\tReward total:0\tRunning mean: -0.007355\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:5\tReward total:17\tRunning mean: 14.57\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:6\tReward total:18.0\tRunning mean: 1.661\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:7\tReward total:17\tRunning mean: 16.62\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:8\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Learner:9\tReward total:77.0\tRunning mean: 111.6\tNum agents crossed: 2\tRunning mean: 1.069\n",
      "Max Norms =  ['32.94', '24.43', '25.48', '68.89', '142.47', '4.59', '129.86', '0.37', '0.05', '186.66']\n",
      "....................\n",
      "Episode 2380 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.006721\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:1\tReward total:0\tRunning mean: -0.4022\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:2\tReward total:0\tRunning mean: 0.08993\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:3\tReward total:0\tRunning mean: -0.006015\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:5\tReward total:17\tRunning mean: 14.69\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:6\tReward total:0\tRunning mean: 1.69\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:7\tReward total:17\tRunning mean: 16.53\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:8\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:9\tReward total:114.0\tRunning mean: 111.8\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Max Norms =  ['40.75', '49.50', '43.97', '18.06', '63.48', '22.60', '0.00', '0.62', '0.04', '124.59']\n",
      "....................\n",
      "Episode 2400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.005497\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:1\tReward total:0\tRunning mean: -0.329\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:2\tReward total:0\tRunning mean: 0.07355\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:3\tReward total:0\tRunning mean: -0.00492\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:5\tReward total:17\tRunning mean: 14.96\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:6\tReward total:0\tRunning mean: 1.441\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:7\tReward total:17\tRunning mean: 16.62\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:8\tReward total:17\tRunning mean: 17.01\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Learner:9\tReward total:111.0\tRunning mean: 112.4\tNum agents crossed: 1\tRunning mean: 1.07\n",
      "Max Norms =  ['52.42', '72.09', '33.47', '0.58', '28.13', '6.09', '0.00', '0.53', '0.04', '60.56']\n",
      "....................\n",
      "Episode 2420 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01391\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:1\tReward total:0\tRunning mean: -0.2691\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1261\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:3\tReward total:0\tRunning mean: -0.004024\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:5\tReward total:17\tRunning mean: 15.17\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:6\tReward total:0\tRunning mean: 1.225\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:7\tReward total:17\tRunning mean: 16.69\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:8\tReward total:17\tRunning mean: 17.01\tNum agents crossed: 1\tRunning mean: 1.066\n",
      "Learner:9\tReward total:128.0\tRunning mean: 113.6\tNum agents crossed: 1\tRunning mean: 1.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['15.82', '52.70', '120.61', '0.61', '24.02', '9.26', '0.00', '0.53', '0.03', '501.05']\n",
      "....................\n",
      "Episode 2440 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1333\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:1\tReward total:0\tRunning mean: -0.2201\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1031\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:3\tReward total:0\tRunning mean: -0.003291\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:5\tReward total:17\tRunning mean: 15.05\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:6\tReward total:0\tRunning mean: 1.002\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:7\tReward total:17\tRunning mean: 16.73\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:8\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Learner:9\tReward total:137.0\tRunning mean: 117.4\tNum agents crossed: 1\tRunning mean: 1.054\n",
      "Max Norms =  ['68.15', '26.28', '41.07', '5.45', '23.52', '6.70', '0.00', '0.60', '0.03', '30.37']\n",
      "....................\n",
      "Episode 2460 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.109\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:1\tReward total:0\tRunning mean: -0.18\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:2\tReward total:0\tRunning mean: 0.08433\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:3\tReward total:0\tRunning mean: -0.002692\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:5\tReward total:17\tRunning mean: 15.21\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:6\tReward total:0\tRunning mean: 0.8197\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:7\tReward total:17\tRunning mean: 16.8\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:8\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Learner:9\tReward total:1.0\tRunning mean: 118.4\tNum agents crossed: 0\tRunning mean: 1.026\n",
      "Max Norms =  ['177.91', '42.78', '20.88', '91.39', '14.23', '7.73', '0.00', '0.46', '0.03', '124.66']\n",
      "....................\n",
      "Episode 2480 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.09869\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:1\tReward total:0\tRunning mean: -0.09346\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:2\tReward total:0\tRunning mean: 0.06898\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:3\tReward total:0\tRunning mean: 0.006486\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:5\tReward total:17\tRunning mean: 15.38\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:6\tReward total:2\tRunning mean: 1.091\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:7\tReward total:17\tRunning mean: 16.88\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:8\tReward total:0\tRunning mean: 16.52\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Learner:9\tReward total:3\tRunning mean: 115.2\tNum agents crossed: 0\tRunning mean: 1.048\n",
      "Max Norms =  ['19.30', '15.47', '0.57', '146.76', '44.54', '3.05', '70.52', '0.58', '0.00', '140.86']\n",
      "....................\n",
      "Episode 2500 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.08072\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:1\tReward total:0\tRunning mean: -0.03386\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05642\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:3\tReward total:0\tRunning mean: 0.005305\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:5\tReward total:17\tRunning mean: 15.37\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:6\tReward total:0\tRunning mean: 1.386\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:7\tReward total:17\tRunning mean: 16.91\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:8\tReward total:17\tRunning mean: 16.18\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Learner:9\tReward total:137.0\tRunning mean: 114.3\tNum agents crossed: 1\tRunning mean: 1.067\n",
      "Max Norms =  ['32.74', '34.41', '0.00', '45.69', '15.94', '15.89', '0.00', '0.36', '0.03', '86.94']\n",
      "....................\n",
      "Episode 2520 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.08439\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0458\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04614\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:3\tReward total:0\tRunning mean: 0.004339\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:5\tReward total:17\tRunning mean: 15.36\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:6\tReward total:11.0\tRunning mean: 1.781\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:7\tReward total:17\tRunning mean: 16.93\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:8\tReward total:17\tRunning mean: 15.87\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Learner:9\tReward total:63.0\tRunning mean: 114.3\tNum agents crossed: 2\tRunning mean: 1.138\n",
      "Max Norms =  ['87.24', '37.41', '0.37', '303.51', '14.62', '7.82', '139.59', '0.24', '0.03', '45.43']\n",
      "....................\n",
      "Episode 2540 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.06903\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03746\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03774\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:3\tReward total:-6.0\tRunning mean: -0.1234\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:5\tReward total:17\tRunning mean: 15.66\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:6\tReward total:0\tRunning mean: 2.612\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:7\tReward total:17\tRunning mean: 16.96\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:8\tReward total:17\tRunning mean: 15.93\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Learner:9\tReward total:136.0\tRunning mean: 114.5\tNum agents crossed: 1\tRunning mean: 1.187\n",
      "Max Norms =  ['11.58', '26.79', '0.45', '98.20', '54.53', '5.84', '0.00', '0.22', '0.02', '103.08']\n",
      "....................\n",
      "Episode 2560 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.05646\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:1\tReward total:0\tRunning mean: 0.08566\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03087\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:3\tReward total:0\tRunning mean: -0.1009\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:5\tReward total:17\tRunning mean: 15.43\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:6\tReward total:0\tRunning mean: 3.498\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:7\tReward total:17\tRunning mean: 16.97\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:8\tReward total:17\tRunning mean: 16.13\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Learner:9\tReward total:125.0\tRunning mean: 114.8\tNum agents crossed: 1\tRunning mean: 1.241\n",
      "Max Norms =  ['28.81', '28.37', '18.53', '42.00', '23.45', '8.95', '0.00', '0.16', '0.02', '168.67']\n",
      "....................\n",
      "Episode 2580 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04618\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:1\tReward total:0\tRunning mean: 0.07006\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02525\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:3\tReward total:0\tRunning mean: -0.08256\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:5\tReward total:17\tRunning mean: 15.71\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:6\tReward total:16.0\tRunning mean: 3.618\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:7\tReward total:17\tRunning mean: 16.83\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:8\tReward total:17\tRunning mean: 16.29\tNum agents crossed: 2\tRunning mean: 1.264\n",
      "Learner:9\tReward total:111.0\tRunning mean: 115.0\tNum agents crossed: 2\tRunning mean: 1.264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['46.02', '86.62', '3.46', '82.89', '11.35', '25.69', '80.51', '0.20', '0.02', '66.02']\n",
      "....................\n",
      "Episode 2600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.03777\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:1\tReward total:0\tRunning mean: 0.07711\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02065\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:3\tReward total:0\tRunning mean: -0.06752\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:5\tReward total:17\tRunning mean: 15.78\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:6\tReward total:0\tRunning mean: 4.391\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:7\tReward total:17\tRunning mean: 16.86\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:8\tReward total:17\tRunning mean: 16.42\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Learner:9\tReward total:124.0\tRunning mean: 115.7\tNum agents crossed: 1\tRunning mean: 1.278\n",
      "Max Norms =  ['23.30', '63.77', '0.82', '20.96', '16.11', '32.38', '0.00', '0.26', '0.02', '97.60']\n",
      "....................\n",
      "Episode 2620 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.09558\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04346\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01689\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:3\tReward total:0\tRunning mean: -0.05523\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:5\tReward total:6\tRunning mean: 15.89\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:6\tReward total:20.0\tRunning mean: 4.696\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:7\tReward total:17\tRunning mean: 16.58\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:8\tReward total:17\tRunning mean: 16.53\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Learner:9\tReward total:105.0\tRunning mean: 116.0\tNum agents crossed: 2\tRunning mean: 1.274\n",
      "Max Norms =  ['178.86', '27.33', '0.61', '68.18', '73.78', '75.24', '126.68', '0.29', '0.03', '89.06']\n",
      "....................\n",
      "Episode 2640 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07818\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03555\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01381\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:3\tReward total:0\tRunning mean: -0.04517\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:5\tReward total:17\tRunning mean: 16.05\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:6\tReward total:0\tRunning mean: 3.933\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:7\tReward total:17\tRunning mean: 16.65\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:8\tReward total:17\tRunning mean: 16.61\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Learner:9\tReward total:137.0\tRunning mean: 119.4\tNum agents crossed: 1\tRunning mean: 1.233\n",
      "Max Norms =  ['148.91', '35.80', '1.29', '13.56', '42.30', '7.92', '0.00', '0.20', '0.04', '26.44']\n",
      "....................\n",
      "Episode 2660 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.09053\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02908\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0113\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:3\tReward total:0\tRunning mean: -0.03695\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:5\tReward total:17\tRunning mean: 16.22\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:6\tReward total:0\tRunning mean: 3.299\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:7\tReward total:17\tRunning mean: 16.62\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:8\tReward total:17\tRunning mean: 16.71\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Learner:9\tReward total:135.0\tRunning mean: 122.2\tNum agents crossed: 1\tRunning mean: 1.2\n",
      "Max Norms =  ['80.55', '60.52', '0.67', '61.31', '101.91', '6.34', '0.00', '0.16', '0.02', '105.58']\n",
      "....................\n",
      "Episode 2680 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07405\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:1\tReward total:0\tRunning mean: 0.005502\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:2\tReward total:0\tRunning mean: 0.009241\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:3\tReward total:0\tRunning mean: -0.03022\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:5\tReward total:17\tRunning mean: 16.45\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:6\tReward total:0\tRunning mean: 2.783\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:7\tReward total:19\tRunning mean: 16.91\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:8\tReward total:18\tRunning mean: 16.88\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Learner:9\tReward total:131.0\tRunning mean: 123.9\tNum agents crossed: 1\tRunning mean: 1.172\n",
      "Max Norms =  ['18.81', '20.03', '0.60', '4.64', '66.43', '4.89', '0.00', '0.40', '0.03', '67.26']\n",
      "....................\n",
      "Episode 2700 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.06056\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:1\tReward total:0\tRunning mean: 0.004501\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:2\tReward total:0\tRunning mean: 0.007559\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:3\tReward total:0\tRunning mean: -0.02472\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:5\tReward total:17\tRunning mean: 16.56\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:6\tReward total:0\tRunning mean: 2.607\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:7\tReward total:19\tRunning mean: 17.2\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:8\tReward total:18\tRunning mean: 17.05\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Learner:9\tReward total:132.0\tRunning mean: 124.4\tNum agents crossed: 1\tRunning mean: 1.149\n",
      "Max Norms =  ['163.28', '58.83', '0.50', '52.89', '31.06', '11.87', '0.00', '0.30', '0.03', '131.25']\n",
      "....................\n",
      "Episode 2720 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04954\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:1\tReward total:0\tRunning mean: 0.003681\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:2\tReward total:0\tRunning mean: 0.006182\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:3\tReward total:0\tRunning mean: -0.02022\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:5\tReward total:17\tRunning mean: 16.49\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:6\tReward total:0\tRunning mean: 2.133\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:7\tReward total:17\tRunning mean: 17.28\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:8\tReward total:17\tRunning mean: 17.1\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Learner:9\tReward total:129.0\tRunning mean: 124.6\tNum agents crossed: 1\tRunning mean: 1.113\n",
      "Max Norms =  ['21.61', '25.58', '0.10', '0.30', '9.56', '8.71', '0.00', '0.07', '0.02', '69.06']\n",
      "....................\n",
      "Episode 2740 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04052\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:1\tReward total:0\tRunning mean: 0.003011\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:2\tReward total:0\tRunning mean: 0.005056\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:3\tReward total:0\tRunning mean: -0.01653\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:5\tReward total:17\tRunning mean: 16.58\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:6\tReward total:0\tRunning mean: 1.69\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:7\tReward total:17\tRunning mean: 17.11\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:8\tReward total:17\tRunning mean: 17.09\tNum agents crossed: 1\tRunning mean: 1.092\n",
      "Learner:9\tReward total:137.0\tRunning mean: 126.0\tNum agents crossed: 1\tRunning mean: 1.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['14.83', '24.19', '0.09', '0.33', '9.37', '15.36', '0.00', '0.06', '0.01', '45.99']\n",
      "....................\n",
      "Episode 2760 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.03314\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:1\tReward total:0\tRunning mean: 0.002462\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:2\tReward total:0\tRunning mean: 0.004136\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:3\tReward total:0\tRunning mean: -0.01352\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:5\tReward total:17\tRunning mean: 16.52\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:6\tReward total:0\tRunning mean: 1.382\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:7\tReward total:17\tRunning mean: 17.09\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:8\tReward total:17\tRunning mean: 17.08\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Learner:9\tReward total:137.0\tRunning mean: 127.9\tNum agents crossed: 1\tRunning mean: 1.075\n",
      "Max Norms =  ['18.37', '35.25', '0.22', '5.44', '12.13', '6.80', '0.00', '0.06', '0.01', '119.20']\n",
      "....................\n",
      "Episode 2780 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0271\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:1\tReward total:0\tRunning mean: 0.002014\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:2\tReward total:0\tRunning mean: 0.003383\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:3\tReward total:0\tRunning mean: -0.01106\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:5\tReward total:17\tRunning mean: 16.46\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:6\tReward total:0\tRunning mean: 1.131\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:7\tReward total:17\tRunning mean: 17.07\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:8\tReward total:17\tRunning mean: 17.07\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Learner:9\tReward total:137.0\tRunning mean: 129.5\tNum agents crossed: 1\tRunning mean: 1.062\n",
      "Max Norms =  ['16.17', '32.70', '0.21', '13.05', '20.13', '5.19', '0.00', '0.05', '0.01', '1.90']\n",
      "....................\n",
      "Episode 2800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.02217\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:1\tReward total:0\tRunning mean: 0.001647\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:2\tReward total:0\tRunning mean: 0.002767\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:3\tReward total:0\tRunning mean: -0.009047\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:5\tReward total:17\tRunning mean: 16.56\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:6\tReward total:0\tRunning mean: 0.9247\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:7\tReward total:17\tRunning mean: 16.92\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:8\tReward total:17\tRunning mean: 17.06\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Learner:9\tReward total:137.0\tRunning mean: 130.8\tNum agents crossed: 1\tRunning mean: 1.051\n",
      "Max Norms =  ['25.95', '60.22', '0.25', '3.27', '28.46', '5.09', '0.00', '0.05', '0.01', '4.34']\n",
      "....................\n",
      "Episode 2820 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01813\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:1\tReward total:0\tRunning mean: 0.001347\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:2\tReward total:0\tRunning mean: 0.002263\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:3\tReward total:0\tRunning mean: -0.007399\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:5\tReward total:17\tRunning mean: 16.49\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:6\tReward total:0\tRunning mean: 0.7563\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:7\tReward total:19\tRunning mean: 17.03\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:8\tReward total:18\tRunning mean: 17.1\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Learner:9\tReward total:0\tRunning mean: 125.4\tNum agents crossed: 0\tRunning mean: 0.9941\n",
      "Max Norms =  ['5.64', '73.34', '0.15', '12.38', '23.51', '11.74', '0.00', '0.13', '0.02', '0.00']\n",
      "....................\n",
      "Episode 2840 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01483\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:1\tReward total:0\tRunning mean: 0.001102\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:2\tReward total:0\tRunning mean: 0.001851\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:3\tReward total:0\tRunning mean: -0.006052\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:5\tReward total:17\tRunning mean: 16.58\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:6\tReward total:0\tRunning mean: 0.6186\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:7\tReward total:17\tRunning mean: 17.07\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:8\tReward total:17\tRunning mean: 17.1\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Learner:9\tReward total:137.0\tRunning mean: 123.4\tNum agents crossed: 1\tRunning mean: 0.9696\n",
      "Max Norms =  ['1.84', '32.31', '0.05', '24.42', '20.35', '14.66', '0.00', '0.04', '0.01', '1.74']\n",
      "....................\n",
      "Episode 2860 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01213\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0009014\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:2\tReward total:0\tRunning mean: 0.001514\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:3\tReward total:0\tRunning mean: -0.00495\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:5\tReward total:17\tRunning mean: 16.66\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:6\tReward total:0\tRunning mean: 0.506\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:7\tReward total:17\tRunning mean: 17.06\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:8\tReward total:17\tRunning mean: 17.08\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Learner:9\tReward total:137.0\tRunning mean: 125.8\tNum agents crossed: 1\tRunning mean: 0.9751\n",
      "Max Norms =  ['16.77', '26.52', '0.04', '0.42', '10.61', '7.00', '0.00', '0.03', '0.01', '6.62']\n",
      "....................\n",
      "Episode 2880 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.009921\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0007372\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:2\tReward total:0\tRunning mean: 0.001238\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:3\tReward total:0\tRunning mean: -0.004049\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:5\tReward total:17\tRunning mean: 16.57\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:6\tReward total:0\tRunning mean: 0.4138\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:7\tReward total:17\tRunning mean: 17.05\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:8\tReward total:17\tRunning mean: 17.07\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Learner:9\tReward total:136.0\tRunning mean: 127.7\tNum agents crossed: 1\tRunning mean: 0.9797\n",
      "Max Norms =  ['19.84', '41.48', '0.05', '19.86', '9.78', '7.69', '0.00', '0.03', '0.01', '66.05']\n",
      "....................\n",
      "Episode 2900 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.008114\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:1\tReward total:0\tRunning mean: 0.000603\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:2\tReward total:0\tRunning mean: 0.001013\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:3\tReward total:0\tRunning mean: -0.003311\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:5\tReward total:17\tRunning mean: 16.64\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:6\tReward total:0\tRunning mean: 0.3385\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:7\tReward total:17\tRunning mean: 17.04\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:8\tReward total:17\tRunning mean: 17.06\tNum agents crossed: 1\tRunning mean: 0.9834\n",
      "Learner:9\tReward total:136.0\tRunning mean: 129.3\tNum agents crossed: 1\tRunning mean: 0.9834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['1.52', '63.34', '0.04', '0.22', '15.06', '7.76', '0.00', '0.03', '0.01', '27.56']\n",
      "....................\n",
      "Episode 2920 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.006637\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0004932\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0008283\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:3\tReward total:0\tRunning mean: -0.002708\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:5\tReward total:17\tRunning mean: 16.55\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:6\tReward total:0\tRunning mean: 0.2768\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:7\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:8\tReward total:17\tRunning mean: 17.05\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Learner:9\tReward total:137.0\tRunning mean: 130.6\tNum agents crossed: 1\tRunning mean: 0.9864\n",
      "Max Norms =  ['1.30', '41.60', '0.04', '0.21', '16.41', '10.78', '0.00', '0.03', '0.01', '2.17']\n",
      "....................\n",
      "Episode 2940 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.005428\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0004034\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0006775\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:3\tReward total:0\tRunning mean: -0.002215\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:5\tReward total:17\tRunning mean: 16.63\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:6\tReward total:0\tRunning mean: 0.2264\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:7\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:8\tReward total:17\tRunning mean: 17.04\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Learner:9\tReward total:137.0\tRunning mean: 131.8\tNum agents crossed: 1\tRunning mean: 0.9889\n",
      "Max Norms =  ['1.05', '22.80', '0.13', '0.22', '23.84', '4.27', '0.00', '0.03', '0.01', '1.36']\n",
      "....................\n",
      "Episode 2960 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.00444\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:1\tReward total:0\tRunning mean: -0.02878\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0005541\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:3\tReward total:0\tRunning mean: -0.001812\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:5\tReward total:17\tRunning mean: 16.7\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1852\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:7\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:8\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Learner:9\tReward total:134.0\tRunning mean: 132.6\tNum agents crossed: 1\tRunning mean: 0.9909\n",
      "Max Norms =  ['1.31', '32.70', '0.10', '0.36', '86.87', '16.54', '0.00', '0.03', '0.01', '43.19']\n",
      "....................\n",
      "Episode 2980 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.003631\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:1\tReward total:0\tRunning mean: -0.02354\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0004532\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:3\tReward total:0\tRunning mean: -0.001482\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:5\tReward total:17\tRunning mean: 16.6\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1515\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:7\tReward total:17\tRunning mean: 16.87\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:8\tReward total:17\tRunning mean: 17.03\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Learner:9\tReward total:137.0\tRunning mean: 133.4\tNum agents crossed: 1\tRunning mean: 0.9926\n",
      "Max Norms =  ['1.21', '84.49', '0.06', '0.16', '28.88', '7.77', '0.00', '0.03', '0.01', '1.05']\n",
      "....................\n",
      "Episode 3000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.00297\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:1\tReward total:0\tRunning mean: -0.03808\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0003707\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:3\tReward total:0\tRunning mean: -0.001212\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:5\tReward total:17\tRunning mean: 16.67\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:6\tReward total:0\tRunning mean: 0.1239\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:7\tReward total:17\tRunning mean: 16.89\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:8\tReward total:17\tRunning mean: 17.02\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Learner:9\tReward total:137.0\tRunning mean: 134.1\tNum agents crossed: 1\tRunning mean: 0.9939\n",
      "Max Norms =  ['1.18', '31.93', '0.05', '0.13', '12.75', '10.16', '0.00', '0.02', '0.01', '4.63']\n",
      "\n",
      "Training time: 523.30 min\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize environment\n",
    "game = \"Crossing\"\n",
    "num_crawler_actions = 8                     # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                      # Drones are capable of 12 actions\n",
    "experiment = '2T-5L/pac_droneleader/'    # 2 team of 5 agents; a team of pacifist with a drone leader \n",
    "\n",
    "# Map and Parameter sets\n",
    "map_name = \"food_d37_river_w1_d25\"  \n",
    "parameters =[ \n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300}\n",
    "            ]\n",
    "\n",
    "temp_end = 1.0   # temp parameter is annealed from the value stored in parameters['temp_start'] to 1.0 \n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 7      # environ observation consists of a list of stacked frames per agent\n",
    "max_episodes = 3000\n",
    "\n",
    "render = True    # This turns on rendering every save so that agents' behavior can be observed\n",
    "SPEED = 1/30\n",
    "second_pile_x = 50  # x-coordinate of the 2nd food pile\n",
    "\n",
    "log_interval = 20\n",
    "save_interval = 50\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   10 agents - 10 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 10\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "       \n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    start = time.clock()  # time the training\n",
    "    \n",
    "    situation = 'pac_vs_pac_apples_teamreward'\n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "    \n",
    "    # Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "    teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "        {'name': 'Franks', 'color': 'red', \n",
    "         'culture': {'name':'pacifist','laser_penalty':-1.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}\n",
    "    ]\n",
    "    agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',     \\\n",
    "         'role': 'follower', 'start': (1,7)},  # Use a different color for Leader\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9)},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8)},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7)},\n",
    "        # Leader of Team Viking is a drone and has a different color\n",
    "        {'id': 4, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)},\n",
    "        {'id': 5, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,1)},\n",
    "        {'id': 6, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,3)},\n",
    "        {'id': 7, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,2)},\n",
    "        {'id': 8, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,1)},\n",
    "        {'id': 9, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,3)}\n",
    "    ]\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            \n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            \n",
    "            # Initialize agent policy based on type\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            elif agents_params[i]['type'] is 'drone':\n",
    "                agents.append(Drone_Policy(num_frames, num_drone_actions, i)) \n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "            \n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "        \n",
    "        # Keep track of num learners who has crossed over to the 2nd food pile\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "        running_crossed = None         # running average\n",
    "        running_crossed_hist = []   # history of running averages\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Crawler_Policy(input_channels=num_frames, num_actions=num_crawler_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "    # Attach agents to their teams\n",
    "    # 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "    teams = []\n",
    "    # Team Vikings\n",
    "    teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'], \\\n",
    "                  culture=teams_params[0]['culture'], roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0], agents[1], agents[2], agents[3], agents[4]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:5]]))\n",
    "    # Team Franks\n",
    "    teams.append(Team(name=teams_params[1]['name'],color=teams_params[1]['color'], \\\n",
    "                  culture=teams_params[1]['culture'], roles=teams_params[1]['roles'], \\\n",
    "                  agent_policies=[agents[5], agents[6], agents[7], agents[8], agents[9]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[5:10]]))\n",
    "    \n",
    "    env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with Crawler_Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize reward and agents crossed counters\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                \n",
    "                # Only crawlers can fire lasers\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                        \n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with Crawler_Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            \n",
    "            load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "            \n",
    "            if render and (ep % save_interval == 0):   # render 1 episode every save\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "\n",
    "        # Keep track num of agents who gather from 2nd food pile. Note that env.consumption tracks the \n",
    "        # agent index and location of apple gathered\n",
    "        for (i, loc) in env.consumption:\n",
    "            if loc[0] > second_pile_x:   # If x-cood of gathered apple is beyond a preset value, it is\n",
    "                                         # in the 2nd pile\n",
    "                crossed[i] = 1\n",
    "        episode_crossed = sum(crossed)   # sum up the num agents who crossed to 2nd pile for the episode\n",
    "                \n",
    "        # Update reward and crossed statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "            \n",
    "        if running_crossed is None:\n",
    "            running_crossed = episode_crossed\n",
    "        running_crossed = running_crossed * 0.99 + episode_crossed * 0.01\n",
    "        running_crossed_hist.append(running_crossed)\n",
    "                \n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                verbose_str += '\\tNum agents crossed: {}'.format(episode_crossed)\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_crossed)\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(teams, agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "        \n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/' + experiment + map_name\n",
    "                results_dir = 'results/' + experiment + map_name\n",
    "\n",
    "                model_file = model_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    save_model(f, ep, agents[i], optimizers[i])\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "             \n",
    "            crossed_file = results_dir+'/{}/t{}_rp{}_{}gs/Crossed.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(crossed_file), exist_ok=True)\n",
    "            with open(crossed_file, 'wb') as f:\n",
    "                    pickle.dump(running_crossed_hist, f)\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))\n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'pacifist_leadfollow', 'laser_penalty': -1.0, 'target_reward': 2.0}\n",
      "{'name': 'pacifist', 'laser_penalty': -1.0}\n",
      "[Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Drone_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=1344, out_features=12, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      ")]\n",
      "['crawler', 'crawler', 'crawler', 'crawler', 'drone', 'crawler', 'crawler', 'crawler', 'crawler', 'crawler']\n",
      "[7, 5, 4, 1, 1, 5, 0, 2, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print (teams[0].culture)\n",
    "print (teams[1].culture)\n",
    "print (agents)\n",
    "print ([agent.type for agent in agents])\n",
    "print (actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
