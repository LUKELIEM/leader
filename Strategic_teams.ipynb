{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategic Teams\n",
    "\n",
    "After experimenting with a policy-gradient based leader agent in [Crossing_Leader_PG.ipynb](Crossing_Leader_PG.ipynb), we arrived at the conclusion that RL (policy-gradient or Q-learning) are not effective at strategic decision making. Specifically, the leader agent with complete observation of the game space is not able to overcome the problems of local optima and sparce reward.\n",
    "\n",
    "It is therefore necessary to seperate the behavioral aspect of a team from its strategic decision making.\n",
    "\n",
    "## Strategist Class\n",
    "\n",
    "A strategist analyzes the strategic position of the teams of agents it is responsible for directing, based on observable game space and game metrics provided by the Environment. Implemented as a black box, it outputs a Task/Objective for each team.\n",
    "\n",
    "## Team Class\n",
    "\n",
    "The Team class has a Mission and a Culture.\n",
    "\n",
    "A team's Culture is used to shape an agent's behavior regardless of role or type. It does so by adding an \"imaginary\" behavioral reward/penalty to the reward given to the agent by the environment during training. Doing so shape the agent's policy NN so that it conforms to the cultural behaviors expected by the team.\n",
    "\n",
    "A team's Mission/Goal converts the strategist's Task/Objective into a \"imaginary\" mission reward. This mission reward is added to the reward given to an agent by the environment and the team's Culture during training. Doing so shape the agent's policy NN so that it achieves the task/objective demanded of the team. \n",
    "\n",
    "A team doles out behavioral and mission reward to its agents regardless of type or role so that it can accomplish the task/objective set by its strategist.\n",
    "\n",
    "## Strategist directing multiple Teams\n",
    "\n",
    "We envision a multi-agent organization whereby:\n",
    "\n",
    "* A Strategist directs multiple Teams through Tasks/Objectives.\n",
    "\n",
    "* Each Team is made up of agents of different types (drones and crawlers) and roles (leaders and followers).\n",
    "\n",
    "* A Team uses its Culture to shape its agents' behaviors by doling out behavorial rewards during training. It uses its Mission to help its agents learn abilities to accomplish individual or group objectives by doling out mission rewards during training.\n",
    "\n",
    "* In this way, a Strategist that is optimized for strategic decision making can analyze the games space and direct multiple Teams to accomplish more complex and strategic tasks that require more than behavioral skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from xteams_env import CrossingEnv\n",
    "from xteams_model import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategist Class\n",
    "\n",
    "(Wikipedia) A strategist is responsible for the formulation and implementation of a strategy. Strategy generally involves setting goals, determining actions to achieve the goals, and mobilizing resources to execute the actions. It describes how the ends (goals) will be achieved by the means (resources).\n",
    "\n",
    "An agent belonging to the Strategist class performs the following:\n",
    "\n",
    "(1) It accepts and abdicates responsibilities for directing teams of agents\n",
    "\n",
    "(2) It receives game space and metrics from the Environment\n",
    "\n",
    "(3) It analyzes the game space and metrics to arrive at a \"strategic position\" for its teams. e.g. a topological map and/or a set of game stats\n",
    "\n",
    "(4) Based on the strategic position, it decides on a set of goals that need to be accomplished\n",
    "\n",
    "(5) It surveys its teams of agents and their location in the games space\n",
    "\n",
    "(6) For each goal, it picks the best team and assign it the goal\n",
    "\n",
    "(7) If necessary, it reorganize the teams and the agents\n",
    "\n",
    "(8) It measures the effectiveness of the teams in accomplishing the assigned goals, and whether the \"strategic position\" has improved for its teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Strategist():\n",
    "    \n",
    "    teams = []\n",
    "    eyes = []  # Each team has a drone agent that serves as an eye for the strategist\n",
    "    game_spaces = []\n",
    "    game_metrics = []\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Strategist, self).__init__()\n",
    "        \n",
    "        # Teams parameters\n",
    "        self.teams = []\n",
    "        self.eyes = []\n",
    "        \n",
    "        # Teams' game spaces  \n",
    "        self.game_spaces = []\n",
    "        \n",
    "        # Teams' game metrics\n",
    "        self.game_metrics = []\n",
    "        \n",
    "        \n",
    "        # zone parameters\n",
    "\n",
    "        # episode history\n",
    "\n",
    "        return\n",
    "    \n",
    "    # This method accepts directorship of a team of agents, but only if the team has a drone agent\n",
    "    # that can act as eye for the strategist.\n",
    "    def accept(self, team):\n",
    "        \n",
    "        eye_found = False\n",
    "        \n",
    "        # Look for drone agent in team\n",
    "        for agent in team.members:\n",
    "            if agent.type is \"drone\":\n",
    "                self.eyes.append(agent)  # assign agent as team eye\n",
    "                eye_found = True\n",
    "                break\n",
    "        \n",
    "        # Only accept directorship of a team if there is a team eye\n",
    "        if eye_found:\n",
    "            self.teams.append(team)  \n",
    "        else:\n",
    "            raise Exception('Cannot accept team directorship! Team {} has no drone.'.format(team.name))\n",
    "            \n",
    "        return\n",
    "    \n",
    "    # This method abdicates directorship of a team of agents\n",
    "    def abdicate(self, team):\n",
    "        try:\n",
    "            self.teams.remove(team)\n",
    "        except ValueError:\n",
    "            print(\"Cannot abdicate team directorship! Team {} is not under strategist's direction.\".format(team.name))\n",
    "        return\n",
    "\n",
    "    \n",
    "    # This method generates a favorability topological map from the game space \n",
    "    def _topology(self, game_space):\n",
    "        \n",
    "        space = game_space.numpy()\n",
    "        _,_,x,y = space.shape\n",
    "        \n",
    "        topology = np.zeros((x,y))\n",
    "        \n",
    "        # Generate favorability topology based on food units in 5x5 target zone\n",
    "        for ix,iy in np.ndindex(x,y):\n",
    "            topology[ix,iy] = np.sum(space[0,0,ix:ix+5, iy:iy+5])\n",
    "\n",
    "        return topology\n",
    "\n",
    "\n",
    "    # This \"black box\" method generates a set of goals after analyzing the game space and metrics\n",
    "    def generate_goals(self, game_space):\n",
    "        \n",
    "        # Create a topology of favorability\n",
    "        topology = self._topology(game_space)\n",
    "        \n",
    "        # Find the coordinate of highest favorability\n",
    "        i,j = np.unravel_index(topology.argmax(), topology.shape)\n",
    "        \n",
    "        goals = [(i,j)]  # The goal is to move a team to the coordinate of highest favorability\n",
    "        \n",
    "        return goals, topology\n",
    "    \n",
    "        \n",
    "    def _assign_goal(self, goal, team):\n",
    "        \n",
    "        # TBD\n",
    "        \n",
    "        return  \n",
    "    \n",
    "    # This method flush the strategist's history at the end of a game episode    \n",
    "    def clear_history(self):\n",
    "        \n",
    "        return\n",
    "\n",
    "    # This method resets strategist by abdicating all team directorships\n",
    "    def reset(self):\n",
    "        # Abdicate directorship for all teams\n",
    "        self.teams = []\n",
    "        self.eyes = []\n",
    "        self.game_spaces = []\n",
    "        self.game_metrics = []\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Game with Strategist\n",
    "\n",
    "For now, the strategist can only direct 1 team with a drone agent. It access the game space through the complete obs space of the drone agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    # Agents trained in map = food_d37\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/',   # scenario=1\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/',   # scenario=2\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=3\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/',   # scenario=4\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/',   # scenario=5\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/',   # scenario=6\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/',   # scenario=7\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/',   # scenario=8\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/',   # scenario=9\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/',   # scenario=10\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/',   # scenario=11\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/',   # scenario=12\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/',   # scenario=13\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/',   # scenario=14\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/',   # scenario=15\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/',   # scenario=16\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/',   # scenario=17\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/',   # scenario=18\n",
    "\n",
    "    # Agents trained in map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=19\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=20 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=21\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\",   # scenario=22\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37\n",
    "    \"models/2T-5L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/\",     # scenario=23\n",
    "    \"models/2T-5L/baseline/food_d37/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=24\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "    # Team Viking (Pacifist w/o leader) vs Team Frank (Cooperative)\n",
    "    \"models/2T-5L/baseline/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=25\n",
    "    \n",
    "    # Team Viking (Pacifist w/ leader) vs Team Frank (No Leader)\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\", # scenario=26\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t2.0_rp-1.0_300gs/\",   # scenario=27\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_coop/t1.5_rp-1.0_300gs/\",   # scenario=28\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t1.5_rp-1.0_300gs/\",   # scenario=29 \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t2.0_rp-1.0_300gs/\",   # scenario=30  \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples/t2.0_rp-1.0_300gs/\",   # scenario=31\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples_teamreward/t2.0_rp-1.0_300gs/\",    # scenario=32\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/pac_simple_droneleader/t2.0_rp-1.0_300gs/\",   # scenario=33\n",
    "    \"models/1T-11L/strategist/food_d37_river_w1_d25/pac_simple_droneleader_followers/t2.1_rp-1.0_300gs/\",   # scenario=34\n",
    "]\n",
    "\n",
    "# Parameter sets pertaining to the trained models in the folders above (not used in the code)\n",
    "parameters =[ \n",
    "        # Temperature for explore/exploit; penalty per step in river; game steps per episode\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37\n",
    "            {'temp_start':0.4, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':0.8, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37_river_w1_d25    \n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "        # Team Viking (Pacifist w/ leader) vs Team Frank (Cooperative)\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},    \n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},   \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':1.0, 'game_steps':300},     \n",
    "            {'temp_start':2.1, 'river_penalty':-1.0, 'target_reward':1.0, 'game_steps':300}, \n",
    "            ]\n",
    "\n",
    "print (len(parameters))\n",
    "print (len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load saved model for agent 0\n",
      "Load Drone Leader.\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Total rewards gathered = 0\n",
      "Av. rewards per agent = 0.00\n",
      "Num laser fired = 0\n",
      "Total US Hit (friendly fire) = 0\n",
      "Total THEM Hit = 0\n",
      "friendly fire (%) = 0.000\n",
      "Num agents gathering from 2nd food pile: 0\n",
      "\n",
      "Statistics by Team\n",
      "===================\n",
      "Team Vikings has total reward of 0\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "# map_name = \"food_d37_river_w1_d25_v2\"\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "# map_name \"food_d37\"\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "scenario = 34\n",
    "dir_name = folders[scenario-1]\n",
    "parameter = parameters[scenario-1]\n",
    "episodes = 1000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 1 agents - 1 teams of 1 AI agents each and 0 random agent\n",
    "num_ai_agents = 1\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "# Scenario 26-33\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},        \n",
    "]\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)}\n",
    "]\n",
    "\n",
    "\n",
    "def calc_deltas(goal, current):\n",
    "    # Calculate delta between the current and the target coordinates\n",
    "    target_x, target_y = goal\n",
    "    current_x, current_y = current\n",
    "    delta_x = (target_x - current_x)/60   # normalize\n",
    "    delta_y = (target_y - current_y)/20    # normalize\n",
    "    deltas = torch.Tensor([delta_x,delta_y])\n",
    "    deltas = deltas.view(1, -1)\n",
    "        \n",
    "    # print(deltas)\n",
    "    return deltas   \n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 200\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    print(\"Load Drone Leader.\")\n",
    "                    agent = DroneLeader_Advanced(num_frames, num_goal_params, num_drone_actions, i)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:5]]))\n",
    "\n",
    "# 5-29-2019  Strategist accepts directorship of a team\n",
    "suntzu = Strategist()\n",
    "suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "    game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "    goals, topology = suntzu.generate_goals(game_space)\n",
    "    deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                actions[i], _ = select_action_strat(agents[i], agents_obs[i], deltas, cuda=False)\n",
    "            else:    \n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])        \n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/(frame+1e-7)))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_ai_agents))\n",
    "print ('Num laser fired = {}'.format(total_tags))\n",
    "print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_team = None\n",
    "top_team_reward = 0\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    if team.name is not 'Crazies':\n",
    "        reward = sum(team.sum_rewards())\n",
    "        print ('Team {} has total reward of {}'.format(team.name, reward))\n",
    "                           \n",
    "        if reward > top_team_reward:   # Keep track of dominating team\n",
    "            top_team_reward = reward\n",
    "            top_team = team.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(teams) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_team))\n",
    "    dominance = top_team_reward/((total_rewards-top_team_reward+1.1e-7)/(len(teams)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[72.6833, 22.6000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SunTzu is strategist for Team Vikings.\n",
      "Agent 0 (drone leader) is acting as eye for SunTzu.\n",
      "Display the Big Picture!\n",
      "torch.Size([1, 7, 100, 60])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f870f879940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87080f0400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAHVCAYAAABG/rbjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADtJJREFUeJzt21+MZgdZx/HfY5fKP7UtYFO71ZbQSBojYDZYAhdYxFQkwAUhJVxsTJO9QS1CAq0mJl6SGP5cGJMNRXtB+GPBtOmFWEtJvFrYpUXaLqUrUuimZSFSMV4olceLOYvTunTend2ZeSbz+SSTec95zzvnyebsd885c7a6OwCT/cxODwCwEaECxhMqYDyhAsYTKmA8oQLGEypgPKECxjunUFXV9VX1cFWdqKqbz9dQAOvVZp9Mr6oLknwjyRuTPJbky0ne2d0PPctnPAYPrPf97n7JRhudyxnVq5Oc6O5vdvd/J/lUkreew88D9p5HV9noXEJ1eZLvrFt+bFn3NFV1qKqOVtXRc9gXsIft2+oddPfhJIcTl37A5pzLGdXJJFesW96/rAM4r84lVF9OcnVVXVVVFya5Icmd52csgP+z6Uu/7n6qqv4gyeeTXJDk49394HmbDGCx6ccTNrUz96iApzvW3Qc22siT6cB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYy3Yaiq6oqqureqHqqqB6vqpmX9JVV1d1U9sny/eOvHBfaiVc6onkryvu6+Jsm1Sd5dVdckuTnJPd19dZJ7lmWA827DUHX34939leX1fyQ5nuTyJG9Nctuy2W1J3rZVQwJ7276z2biqrkzyqiRHklza3Y8vbz2R5NKf8plDSQ5tfkRgr1v5ZnpVvTDJZ5O8p7t/uP697u4kfabPdffh7j7Q3QfOaVJgz1opVFX1nKxF6hPd/bll9Xer6rLl/cuSnNqaEYG9bpXf+lWSW5Mc7+4PrXvrziQHl9cHk9xx/scDSGrtqu1ZNqh6XZJ/SvK1JD9eVv9J1u5TfSbJLyd5NMk7uvvfNvhZz74zYK85tsptoQ1DdT4JFfAMK4XKk+nAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMt3KoquqCqrqvqu5alq+qqiNVdaKqPl1VF27dmMBedjZnVDclOb5u+YNJPtzdL0vygyQ3ns/BAE5bKVRVtT/J7yX52LJcSa5LcvuyyW1J3rYVAwKsekb1kSTvT/LjZflFSZ7s7qeW5ceSXH6mD1bVoao6WlVHz2lSYM/aMFRV9eYkp7r72GZ20N2Hu/tAdx/YzOcB9q2wzWuTvKWq3pTkuUl+PslHk1xUVfuWs6r9SU5u3ZjAXrbhGVV339Ld+7v7yiQ3JPlCd78ryb1J3r5sdjDJHVs2JbCnnctzVB9I8t6qOpG1e1a3np+RAJ6uunv7dla1fTsDdoNjq9y/9mQ6MJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB460Uqqq6qKpur6qvV9XxqnpNVV1SVXdX1SPL94u3elhgb1r1jOqjSf6+u1+e5BVJjie5Ock93X11knuWZYDzrrr72Teo+oUk9yd5aa/buKoeTvL67n68qi5L8sXu/tUNftaz7wzYa45194GNNlrljOqqJN9L8tdVdV9VfayqXpDk0u5+fNnmiSSXnunDVXWoqo5W1dFVJwdYb5VQ7UvyG0n+qrtfleQ/84zLvOVM64xnS919uLsPrFJNgDNZJVSPJXmsu48sy7dnLVzfXS75snw/tTUjAnvdhqHq7ieSfKeqTt9/ekOSh5LcmeTgsu5gkju2ZEJgz9u34nZ/mOQTVXVhkm8m+f2sRe4zVXVjkkeTvGNrRgT2ug1/63ded+a3fsDTnbff+gHsKKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGWylUVfXHVfVgVT1QVZ+squdW1VVVdaSqTlTVp6vqwq0eFtibNgxVVV2e5I+SHOjuX0tyQZIbknwwyYe7+2VJfpDkxq0cFNi7Vr3025fkeVW1L8nzkzye5Lokty/v35bkbed/PIAVQtXdJ5P8RZJvZy1Q/57kWJInu/upZbPHklx+ps9X1aGqOlpVR8/PyMBes8ql38VJ3prkqiS/lOQFSa5fdQfdfbi7D3T3gU1PCexpq1z6/XaSf+3u73X3j5J8Lslrk1y0XAomyf4kJ7doRmCPWyVU305ybVU9v6oqyRuSPJTk3iRvX7Y5mOSOrRkR2OtWuUd1JGs3zb+S5GvLZw4n+UCS91bViSQvSnLrFs4J7GHV3du3s6rt2xmwGxxb5f61J9OB8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8fZt8/6+n+TRJC9eXu8Gu2nWZHfNu5tmTXbXvLtl1l9ZZaPq7q0e5P/vtOpodx/Y9h1vwm6aNdld8+6mWZPdNe9umnUVLv2A8YQKGG+nQnV4h/a7Gbtp1mR3zbubZk1217y7adYN7cg9KoCz4dIPGE+ogPG2NVRVdX1VPVxVJ6rq5u3c9yqq6uNVdaqqHli37pKquruqHlm+X7yTM55WVVdU1b1V9VBVPVhVNy3rp8773Kr6UlV9dZn3z5f1V1XVkeWY+HRVXbjTs55WVRdU1X1VddeyPHnWb1XV16rq/qo6uqwbeSxsxraFqqouSPKXSX43yTVJ3llV12zX/lf0N0muf8a6m5Pc091XJ7lnWZ7gqSTv6+5rklyb5N3Ln+fUef8ryXXd/Yokr0xyfVVdm+SDST7c3S9L8oMkN+7gjM90U5Lj65Ynz5okv9Xdr1z3/NTUY+Hsdfe2fCV5TZLPr1u+Jckt27X/s5jzyiQPrFt+OMlly+vLkjy80zP+lLnvSPLG3TBvkucn+UqS38za09P7znSM7PCM+7P2l/u6JHclqamzLvN8K8mLn7Fu/LGw6td2XvpdnuQ765YfW9ZNd2l3P768fiLJpTs5zJlU1ZVJXpXkSAbPu1xK3Z/kVJK7k/xLkie7+6llk0nHxEeSvD/Jj5flF2XurEnSSf6hqo5V1aFl3dhj4Wxt9//129W6u6tq1PMcVfXCJJ9N8p7u/mFV/eS9afN29/8keWVVXZTk75K8fIdHOqOqenOSU919rKpev9PzrOh13X2yqn4xyd1V9fX1b047Fs7Wdp5RnUxyxbrl/cu66b5bVZclyfL91A7P8xNV9ZysReoT3f25ZfXYeU/r7ieT3Ju1y6eLqur0P5hTjonXJnlLVX0ryaeydvn30cycNUnS3SeX76ey9o/Aq7MLjoVVbWeovpzk6uU3JxcmuSHJndu4/826M8nB5fXBrN0L2nG1dup0a5Lj3f2hdW9Nnfcly5lUqup5WbufdjxrwXr7stmIebv7lu7e391XZu04/UJ3vysDZ02SqnpBVf3c6ddJfifJAxl6LGzKNt/we1OSb2Tt3sSf7vQNujPM98kkjyf5UdbuQdyYtXsT9yR5JMk/Jrlkp+dcZn1d1u5L/HOS+5evNw2e99eT3LfM+0CSP1vWvzTJl5KcSPK3SX52p2d9xtyvT3LX5FmXub66fD14+u/W1GNhM1/+Cw0wnifTgfGEChhPqIDxhAoYT6iA8YQKGE+ogPH+F+MGuRv8jA/aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8708094b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAHVCAYAAABG/rbjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADzxJREFUeJzt212sZQdZx+H/a4fKp7YFbGqn2hIaSS8EzAQhcIFFTEUDXBAC4WI0TXqjpogJtJqYeCeJ4ePCYCYU7QXhw1LTpjFiLTXRm8JMW6TtUDoihTYtA4GKMUatvF6cNXg6Dj27Z87HezjPk5ycvdZe+6w3++zzm7XXXlPdHYDJfmy3BwDYiFAB4wkVMJ5QAeMJFTCeUAHjCRUwnlAB451VqKrqqqp6sKpOVNV1WzUUwHq12SvTq+qcJF9J8sYkjyT5QpJ3dvcDT/MYl8ED6327u1+80UZnc0T1qiQnuvur3f1fST6Z5C1n8fOA/efhVTY6m1BdnOQb65YfWdY9RVVdU1VHq+roWewL2McObPcOuvtIkiOJt37A5pzNEdWjSS5Zt3xwWQewpc4mVF9IcnlVXVZV5yZ5R5Jbt2YsgP+z6bd+3f1kVf12ks8mOSfJx7r7/i2bDGCx6csTNrUz56iApzrW3Yc22siV6cB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeAd2ewA2p7t3e4R9q6p2e4R9xxEVMJ5QAeNtGKqquqSq7qyqB6rq/qq6dll/QVXdXlUPLd/P3/5xgf2oNjrXUVUXJbmou++uqhckOZbkrUl+I8l3uvuPq+q6JOd39/s2+FlOrGyR039vzptsH8/1tjrW3Yc22mjDI6rufqy7715u/1uS40kuTvKWJDcum92YtXgBbLln9KlfVV2a5JVJ7kpyYXc/ttz1eJILf8hjrklyzeZHBPa7lU+mV9Xzk3wmybu7+3vr7+u1Y+Mzvq3r7iPdfWiVwzuAM1kpVFX1rKxF6uPdffOy+pvL+atT57FObs+IwH63yqd+leSGJMe7+wPr7ro1yeHl9uEkt2z9eACrfer3uiT/kORLSb6/rP79rJ2n+nSSn0nycJK3d/d3NvhZPvXbIj6J2jme62210qd+G4ZqKwnV1vHHs3M819tqay5PANhtQgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYy3cqiq6pyquqeqbluWL6uqu6rqRFV9qqrO3b4xgf3smRxRXZvk+Lrl9yf5YHe/NMl3k1y9lYMBnLJSqKrqYJJfS/LRZbmSXJnkpmWTG5O8dTsGBFj1iOpDSd6b5PvL8guTPNHdTy7LjyS5+EwPrKprqupoVR09q0mBfWvDUFXVryc52d3HNrOD7j7S3Ye6+9BmHg9wYIVtXpvkzVX1piTPTvITST6c5LyqOrAcVR1M8uj2jQnsZxseUXX39d19sLsvTfKOJJ/r7ncluTPJ25bNDie5ZdumBPa1s7mO6n1J3lNVJ7J2zuqGrRkJ4Kmqu3duZ1U7t7Mfcaf/3tY+iGU7eK631bFVzl+7Mh0YT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxhAoYT6iA8YQKGE+ogPGEChhPqIDxVgpVVZ1XVTdV1Zer6nhVvaaqLqiq26vqoeX7+ds9LLA/rXpE9eEkf9PdL0vy8iTHk1yX5I7uvjzJHcsywJar7n76Dap+Msm9SV7S6zauqgeTvL67H6uqi5L8fXf/3AY/6+l3xspO/71V1S5N8qPPc72tjnX3oY02WuWI6rIk30ry51V1T1V9tKqel+TC7n5s2ebxJBee6cFVdU1VHa2qo6tODrDeKqE6kOQXknyku1+Z5N9z2tu85UjrjEdL3X2kuw+tUk2AM1klVI8keaS771qWb8pauL65vOXL8v3k9owI7Hcbhqq7H0/yjao6df7pDUkeSHJrksPLusNJbtmWCYF978CK2/1Oko9X1blJvprkN7MWuU9X1dVJHk7y9u0ZEdjvNvzUb0t35lO/LeOTqJ3jud5WW/apH8CuEipgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmA8oQLGEypgPKECxhMqYDyhAsYTKmC8lUJVVb9bVfdX1X1V9YmqenZVXVZVd1XViar6VFWdu93DAvtTdffTb1B1cZJ/THJFd/9HVX06yV8neVOSm7v7k1X1Z0m+2N0f2eBnPf3OWNlGvze2T1Xt9gg/So5196GNNlr1rd+BJM+pqgNJnpvksSRXJrlpuf/GJG/dzJQAG9kwVN39aJI/SfL1rAXqX5McS/JEdz+5bPZIkovP9PiquqaqjlbV0a0ZGdhvNgxVVZ2f5C1JLkvy00mel+SqVXfQ3Ue6+9Aqh3cAZ3JghW1+Ocm/dPe3kqSqbk7y2iTnVdWB5ajqYJJHt29MTuc8CfvJKueovp7k1VX13Fr763hDkgeS3Jnkbcs2h5Pcsj0jAvvdKueo7sraSfO7k3xpecyRJO9L8p6qOpHkhUlu2MY5gX1sw8sTtnRnLk8AnmpLL08A2DVCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHhCBYwnVMB4QgWMJ1TAeEIFjCdUwHgHdnh/307ycJIXLbf3gr00a7K35t1LsyZ7a969MuvPrrJRdfd2D/L/d1p1tLsP7fiON2EvzZrsrXn30qzJ3pp3L826Cm/9gPGEChhvt0J1ZJf2uxl7adZkb827l2ZN9ta8e2nWDe3KOSqAZ8JbP2A8oQLG29FQVdVVVfVgVZ2oqut2ct+rqKqPVdXJqrpv3boLqur2qnpo+X7+bs54SlVdUlV3VtUDVXV/VV27rJ8677Or6vNV9cVl3j9a1l9WVXctr4lPVdW5uz3rKVV1TlXdU1W3LcuTZ/1aVX2pqu6tqqPLupGvhc3YsVBV1TlJ/jTJrya5Isk7q+qKndr/iv4iyVWnrbsuyR3dfXmSO5blCZ5M8nvdfUWSVyf5reX5nDrvfya5srtfnuQVSa6qqlcneX+SD3b3S5N8N8nVuzjj6a5Ncnzd8uRZk+SXuvsV666fmvpaeOa6e0e+krwmyWfXLV+f5Pqd2v8zmPPSJPetW34wyUXL7YuSPLjbM/6QuW9J8sa9MG+S5ya5O8kvZu3q6QNneo3s8owHs/bHfWWS25LU1FmXeb6W5EWnrRv/Wlj1ayff+l2c5Bvrlh9Z1k13YXc/ttx+PMmFuznMmVTVpUlemeSuDJ53eSt1b5KTSW5P8s9JnujuJ5dNJr0mPpTkvUm+vyy/MHNnTZJO8rdVdayqrlnWjX0tPFM7/X/99rTu7qoadT1HVT0/yWeSvLu7v1dVP7hv2rzd/T9JXlFV5yX5qyQv2+WRzqiqfj3Jye4+VlWv3+15VvS67n60qn4qye1V9eX1d057LTxTO3lE9WiSS9YtH1zWTffNqrooSZbvJ3d5nh+oqmdlLVIf7+6bl9Vj5z2lu59IcmfW3j6dV1Wn/sGc8pp4bZI3V9XXknwya2//PpyZsyZJuvvR5fvJrP0j8KrsgdfCqnYyVF9Icvnyycm5Sd6R5NYd3P9m3Zrk8HL7cNbOBe26Wjt0uiHJ8e7+wLq7ps774uVIKlX1nKydTzuetWC9bdlsxLzdfX13H+zuS7P2Ov1cd78rA2dNkqp6XlW94NTtJL+S5L4MfS1syg6f8HtTkq9k7dzEH+z2CbozzPeJJI8l+e+snYO4OmvnJu5I8lCSv0tywW7Pucz6uqydl/inJPcuX28aPO/PJ7lnmfe+JH+4rH9Jks8nOZHkL5P8+G7Petrcr09y2+RZl7m+uHzdf+pva+prYTNf/gsNMJ4r04HxhAoYT6iA8YQKGE+ogPGEChhPqIDx/hcf2vxdk5I46gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87002c39b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f870037db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87002f8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f870021bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Favorability Topology!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87002c3b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, 24)]\n"
     ]
    }
   ],
   "source": [
    "for team, eye in zip(suntzu.teams, suntzu.eyes):\n",
    "    print(\"SunTzu is strategist for Team {}.\".format(team.name))\n",
    "    print (\"Agent {} ({} {}) is acting as eye for SunTzu.\".format(eye.idx, eye.type, eye.role))\n",
    "\n",
    "print (\"Display the Big Picture!\")\n",
    "print (game_space.shape)\n",
    "for i in range(7):\n",
    "    plt.imshow(game_space[0,i,:,:])\n",
    "    plt.show()\n",
    "\n",
    "print (\"Display Favorability Topology!\")    \n",
    "plt.imshow(topology)\n",
    "plt.show()\n",
    "\n",
    "print (goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Team directed by Strategist\n",
    "\n",
    "For now, a strategist can only direct 1 team with a drone agent, which acts as the \"eye\" for the strategist. The strategist access the game space through the complete obs space of the drone agent.\n",
    "\n",
    "The code below run training on 2 teams of 5 Agents each. Both team Viking and Franks have Pacifist cultures so they are unagressive (do not fire their lasers). The Vikings have a drone leader and a strategist. The Franks do not.\n",
    "\n",
    "Our strategist is able to take in the game space provided by its eye and output a goal in the form of a coordinate. \n",
    "\n",
    "The Team class must now take this goal (\"move the team to this coordinate\") and generate the mission reward such that its leader agent learns to move to that coordinate, thus taking many of its followers along in its target zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"Simple\" DroneLeader\n",
    "\n",
    "The DroneLeader_Simple is a 2-layer fully-connected NN that accepts the deltas between the drone leader's current coordinate vs the coordinate of max favorability as input, and outputs an action.\n",
    "\n",
    "The policy works amazingly well at reaching the target coordinate. \n",
    "\n",
    "It is also general - meaning it will always reach the target coordinate even when we change the drone's starting coordinate or the target coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DroneLeader_Simple(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=32, out_features=12, bias=True)\n",
      ")\n",
      "tensor([[0.0913, 0.1244, 0.0976, 0.1187, 0.0684, 0.0527, 0.1807, 0.0377, 0.0504,\n",
      "         0.0578, 0.0952, 0.0252]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_drone_actions = 12\n",
    "num_goal_params = 2\n",
    "\n",
    "drone_leader = DroneLeader_Simple(num_goal_params, num_drone_actions, 0)\n",
    "print (drone_leader)\n",
    "\n",
    "batch_size = 1\n",
    "x = torch.randn(batch_size, 2)\n",
    "output = drone_leader(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training (1 Team with 1 Drone Leader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Load Drone Leader.\n",
      "...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4659ab37da9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;31m# 5-30-2019 Strategist uses the obs space of its team eye as the big picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mgame_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuntzu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meyes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mgoals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuntzu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_goals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_locations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Store a history of deltas for generating mission rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef89594b2780>\u001b[0m in \u001b[0;36mgenerate_goals\u001b[0;34m(self, game_space)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Create a topology of favorability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtopology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Find the coordinate of highest favorability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef89594b2780>\u001b[0m in \u001b[0;36m_topology\u001b[0;34m(self, game_space)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Generate favorability topology based on food units in 5x5 target zone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mtopology\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0miy\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtopology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1882\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize environment\n",
    "torch.manual_seed(0)\n",
    "game = \"Crossing\"\n",
    "num_crawler_actions = 8                     # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                      # Drones are capable of 12 actions\n",
    "num_goal_params = 2    # Goal has 2 parameter\n",
    "\n",
    "experiment = '1T-1L/strategist/'    # 1 team of 1 drone leader directed by a strategist\n",
    "\n",
    "# Map and Parameter sets\n",
    "map_name = \"food_d37_river_w1_d25\"  \n",
    "parameters =[ \n",
    "            {'temp_start':2.1, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300}\n",
    "            ]\n",
    "\n",
    "temp_end = 1.0   # temp parameter is annealed from the value stored in parameters['temp_start'] to 1.0 \n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 7      # environ observation consists of a list of stacked frames per agent\n",
    "max_episodes = 2000\n",
    "\n",
    "render = True    # This turns on rendering every save so that agents' behavior can be observed\n",
    "SPEED = 1/30\n",
    "second_pile_x = 50  # x-coordinate of the 2nd food pile\n",
    "\n",
    "log_interval = 10\n",
    "save_interval = 20\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   1 agents - 1 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 1\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "\n",
    "\n",
    "def calc_deltas(goal, current):\n",
    "    # Calculate delta between the current and the target coordinates\n",
    "    target_x, target_y = goal\n",
    "    current_x, current_y = current\n",
    "    delta_x = target_x - current_x\n",
    "    delta_y = target_y - current_y\n",
    "    deltas = torch.Tensor([delta_x,delta_y])\n",
    "    deltas = deltas.view(1, -1)\n",
    "        \n",
    "    # print(deltas)\n",
    "    return deltas   \n",
    "       \n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    start = time.clock()  # time the training\n",
    "    \n",
    "    situation = 'pac_simple_droneleader'\n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "    \n",
    "    # Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "    teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "    ]\n",
    "    agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)},\n",
    "    ]\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            \n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            \n",
    "            # Initialize agent policy based on type\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                agents.append(Drone_Policy(num_frames, num_drone_actions, i)) \n",
    "            elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                print(\"Load Drone Leader.\")\n",
    "                agents.append(DroneLeader_Advanced(num_frames, num_goal_params, num_drone_actions, i)) \n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "            \n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "        \n",
    "        # Keep track of num learners who has crossed over to the 2nd food pile\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "        running_crossed = None         # running average\n",
    "        running_crossed_hist = []   # history of running averages\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Crawler_Policy(input_channels=num_frames, num_actions=num_crawler_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "    # Attach agents to their teams\n",
    "    # 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "    teams = []\n",
    "    # Team Vikings\n",
    "    teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'], \\\n",
    "                  culture=teams_params[0]['culture'], roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:1]]))\n",
    "    \n",
    "    # 5-30-2019  Strategist accepts directorship of a team\n",
    "    suntzu = Strategist()\n",
    "    suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "    \n",
    "    env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with Crawler_Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        \n",
    "        # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize reward and agents crossed counters\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    actions[i], log_probs[i] = select_action_strat(agents[i], agents_obs[i], deltas, cuda)\n",
    "                else:    \n",
    "                    actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                \n",
    "                # Only crawlers can fire lasers\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                        \n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with Crawler_Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            \n",
    "            load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "            \n",
    "            # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "            game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "            goals, topology = suntzu.generate_goals(game_space)\n",
    "            deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "            agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "            \n",
    "            if render and (ep % save_interval == 0):   # render 1 episode every save\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "\n",
    "        # Keep track num of agents who gather from 2nd food pile. Note that env.consumption tracks the \n",
    "        # agent index and location of apple gathered\n",
    "        for (i, loc) in env.consumption:\n",
    "            if loc[0] > second_pile_x:   # If x-cood of gathered apple is beyond a preset value, it is\n",
    "                                         # in the 2nd pile\n",
    "                crossed[i] = 1\n",
    "        episode_crossed = sum(crossed)   # sum up the num agents who crossed to 2nd pile for the episode\n",
    "                \n",
    "        # Update reward and crossed statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "            \n",
    "        if running_crossed is None:\n",
    "            running_crossed = episode_crossed\n",
    "        running_crossed = running_crossed * 0.99 + episode_crossed * 0.01\n",
    "        running_crossed_hist.append(running_crossed)\n",
    "                \n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                verbose_str += '\\tNum agents crossed: {}'.format(episode_crossed)\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_crossed)\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(teams, agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "            print('Drone Agent coordinate: {}'.format(deltas))\n",
    "\n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/' + experiment + map_name\n",
    "                results_dir = 'results/' + experiment + map_name\n",
    "\n",
    "                model_file = model_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    save_model(f, ep, agents[i], optimizers[i])\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "             \n",
    "            crossed_file = results_dir+'/{}/t{}_rp{}_{}gs/Crossed.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(crossed_file), exist_ok=True)\n",
    "            with open(crossed_file, 'wb') as f:\n",
    "                    pickle.dump(running_crossed_hist, f)\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))\n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Training (1 Team with 1 Drone Leader + 10 Followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Load Drone Leader.\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "..........\n",
      "Episode 10 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['18.55', '142.52', '204.46', '260.11', '155.72']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.0500]])\n",
      "..........\n",
      "Episode 20 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['26.89', '168.32', '148.65', '411.21', '135.05']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.6500]])\n",
      "..........\n",
      "Episode 30 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['24.35', '117.98', '166.84', '88.68', '104.11']\n",
      "Drone Agent coordinate: tensor([[ 0.8833, -0.6500]])\n",
      "..........\n",
      "Episode 40 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05536\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['30.19', '170.50', '138.36', '201.60', '80.01']\n",
      "Drone Agent coordinate: tensor([[ 0.8833, -0.1500]])\n",
      "..........\n",
      "Episode 50 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05007\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['39.30', '110.95', '126.40', '113.66', '67.19']\n",
      "Drone Agent coordinate: tensor([[ 0.7667, -0.5500]])\n",
      "..........\n",
      "Episode 60 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04528\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['16.35', '127.75', '110.17', '150.18', '76.58']\n",
      "Drone Agent coordinate: tensor([[ 0.8500, -0.7000]])\n",
      "..........\n",
      "Episode 70 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04095\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['31.08', '120.02', '131.45', '85.62', '180.63']\n",
      "Drone Agent coordinate: tensor([[ 0.7167, -0.7000]])\n",
      "..........\n",
      "Episode 80 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03704\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['31.89', '113.34', '118.22', '98.93', '137.04']\n",
      "Drone Agent coordinate: tensor([[ 0.7167, -0.1500]])\n",
      "..........\n",
      "Episode 90 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0335\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['12.91', '225.30', '131.58', '76.30', '87.31']\n",
      "Drone Agent coordinate: tensor([[ 0.9000, -0.6000]])\n",
      "..........\n",
      "Episode 100 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03029\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['25.61', '137.26', '121.39', '84.04', '72.88']\n",
      "Drone Agent coordinate: tensor([[ 0.8000, -0.5000]])\n",
      "..........\n",
      "Episode 110 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0274\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['28.66', '116.74', '124.52', '91.62', '76.64']\n",
      "Drone Agent coordinate: tensor([[ 0.9000, -0.3500]])\n",
      "..........\n",
      "Episode 120 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02478\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03881\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['32.32', '103.49', '115.89', '69.27', '83.50']\n",
      "Drone Agent coordinate: tensor([[ 0.8167, -0.2500]])\n",
      "..........\n",
      "Episode 130 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02241\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0351\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['20.82', '113.14', '138.06', '83.73', '86.39']\n",
      "Drone Agent coordinate: tensor([[ 0.8500, -0.5500]])\n",
      "..........\n",
      "Episode 140 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.02027\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03174\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['19.21', '128.15', '140.85', '105.07', '74.71']\n",
      "Drone Agent coordinate: tensor([[ 0.9000, -0.3000]])\n",
      "..........\n",
      "Episode 150 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01833\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02871\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['25.91', '80.15', '120.28', '85.89', '76.65']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.7500]])\n",
      "..........\n",
      "Episode 160 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01658\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02596\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['24.25', '118.32', '141.19', '77.97', '67.95']\n",
      "Drone Agent coordinate: tensor([[ 0.8833, -0.2000]])\n",
      "..........\n",
      "Episode 170 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05449\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.05288\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.0297\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['37.67', '92.93', '124.45', '97.17', '70.47']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.2000]])\n",
      "..........\n",
      "Episode 180 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04928\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04783\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.02686\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['19.09', '106.54', '136.71', '131.64', '67.58']\n",
      "Drone Agent coordinate: tensor([[ 0.8333, -0.1000]])\n",
      "..........\n",
      "Episode 190 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04457\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04325\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.02429\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['28.55', '140.48', '145.96', '92.79', '67.86']\n",
      "Drone Agent coordinate: tensor([[ 0.8167, -0.2000]])\n",
      "..........\n",
      "Episode 200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04031\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03912\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.02197\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['22.31', '92.87', '121.10', '93.96', '79.40']\n",
      "Drone Agent coordinate: tensor([[ 0.8000, -0.0500]])\n",
      "..........\n",
      "Episode 210 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03645\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03538\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.008949\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['20.32', '102.62', '165.40', '100.83', '83.79']\n",
      "Drone Agent coordinate: tensor([[ 0.8167, -0.3000]])\n",
      "..........\n",
      "Episode 220 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03297\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.032\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.02692\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['26.14', '110.87', '123.90', '75.14', '87.57']\n",
      "Drone Agent coordinate: tensor([[ 0.5167, -0.4500]])\n",
      "..........\n",
      "Episode 230 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03923\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02894\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.02435\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['28.74', '106.09', '134.38', '84.19', '86.24']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.6500]])\n",
      "..........\n",
      "Episode 240 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01941\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03548\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.05441\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.02202\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['32.05', '141.55', '102.99', '100.94', '65.31']\n",
      "Drone Agent coordinate: tensor([[ 0.8333, -0.6000]])\n",
      "..........\n",
      "Episode 250 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02716\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.04131\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04921\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.01992\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['18.94', '139.76', '121.39', '106.77', '79.33']\n",
      "Drone Agent coordinate: tensor([[ 0.6000, -0.4000]])\n",
      "..........\n",
      "Episode 260 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02456\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.008369\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04451\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.01801\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['27.62', '103.53', '110.22', '121.37', '56.03']\n",
      "Drone Agent coordinate: tensor([[ 0.7500, -0.5000]])\n",
      "..........\n",
      "Episode 270 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02221\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.03553\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04025\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.05301\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['21.05', '126.72', '133.74', '82.53', '76.49']\n",
      "Drone Agent coordinate: tensor([[ 0.8333, -0.1500]])\n",
      "..........\n",
      "Episode 280 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02009\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.06066\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.05504\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.04794\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['34.10', '107.51', '158.01', '84.14', '61.02']\n",
      "Drone Agent coordinate: tensor([[0.9000, 0.1500]])\n",
      "..........\n",
      "Episode 290 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01817\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.05486\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04978\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.04336\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['24.96', '88.41', '132.30', '55.59', '78.96']\n",
      "Drone Agent coordinate: tensor([[ 0.9000, -0.4500]])\n",
      "..........\n",
      "Episode 300 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01643\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.06883\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04502\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: -0.01941\tNum agents crossed: 0\tRunning mean: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['34.01', '105.84', '100.97', '108.69', '92.85']\n",
      "Drone Agent coordinate: tensor([[0.7667, 0.1000]])\n",
      "..........\n",
      "Episode 310 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01486\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.07157\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.05004\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.01013\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['31.77', '105.59', '133.95', '94.80', '72.69']\n",
      "Drone Agent coordinate: tensor([[ 0.7500, -0.0500]])\n",
      "..........\n",
      "Episode 320 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01344\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.1541\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.09456\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:1\tRunning mean: 0.07738\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['34.55', '101.15', '141.79', '88.08', '88.35']\n",
      "Drone Agent coordinate: tensor([[0.7667, 0.2000]])\n",
      "..........\n",
      "Episode 330 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01215\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.1394\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.08552\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.07911\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['36.93', '91.50', '91.03', '83.57', '83.20']\n",
      "Drone Agent coordinate: tensor([[0.8667, 0.2000]])\n",
      "..........\n",
      "Episode 340 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01099\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.2938\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.07734\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.07155\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['32.51', '127.77', '112.46', '86.19', '85.22']\n",
      "Drone Agent coordinate: tensor([[0.8667, 0.0500]])\n",
      "..........\n",
      "Episode 350 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.00994\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4404\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.06994\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.06471\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['29.53', '85.78', '135.19', '75.40', '54.40']\n",
      "Drone Agent coordinate: tensor([[0.9000, 0.0500]])\n",
      "..........\n",
      "Episode 360 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03752\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4908\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.06326\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05852\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['34.12', '113.45', '106.24', '81.92', '78.76']\n",
      "Drone Agent coordinate: tensor([[ 0.7667, -0.0500]])\n",
      "..........\n",
      "Episode 370 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03393\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4439\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.05721\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05292\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['37.64', '108.22', '98.43', '101.59', '105.15']\n",
      "Drone Agent coordinate: tensor([[ 0.7333, -0.1000]])\n",
      "..........\n",
      "Episode 380 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03069\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4696\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.03253\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05709\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['33.78', '91.01', '95.22', '76.77', '123.77']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.1000]])\n",
      "..........\n",
      "Episode 390 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02775\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4516\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: -0.02942\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.08036\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['24.55', '109.63', '121.88', '90.57', '93.02']\n",
      "Drone Agent coordinate: tensor([[0.9000, 0.0500]])\n",
      "..........\n",
      "Episode 400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0251\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4453\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:1\tRunning mean: -0.006803\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.08218\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['34.40', '109.89', '122.60', '126.62', '74.74']\n",
      "Drone Agent coordinate: tensor([[ 0.6167, -0.0500]])\n",
      "..........\n",
      "Episode 410 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.009321\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0227\tNum agents crossed: 0\tRunning mean: 0.009321\n",
      "Learner:2\tReward total:0\tRunning mean: -0.3843\tNum agents crossed: 0\tRunning mean: 0.009321\n",
      "Learner:3\tReward total:0\tRunning mean: 0.01345\tNum agents crossed: 0\tRunning mean: 0.009321\n",
      "Learner:4\tReward total:0\tRunning mean: 0.07433\tNum agents crossed: 0\tRunning mean: 0.009321\n",
      "Max Norms =  ['32.50', '93.94', '144.00', '74.91', '82.64']\n",
      "Drone Agent coordinate: tensor([[ 0.8500, -0.7000]])\n",
      "..........\n",
      "Episode 420 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.008429\n",
      "Learner:1\tReward total:0\tRunning mean: 0.07534\tNum agents crossed: 0\tRunning mean: 0.008429\n",
      "Learner:2\tReward total:-2.0\tRunning mean: -0.4348\tNum agents crossed: 0\tRunning mean: 0.008429\n",
      "Learner:3\tReward total:0\tRunning mean: 0.01216\tNum agents crossed: 0\tRunning mean: 0.008429\n",
      "Learner:4\tReward total:0\tRunning mean: 0.06722\tNum agents crossed: 0\tRunning mean: 0.008429\n",
      "Max Norms =  ['31.25', '101.57', '141.19', '108.62', '76.83']\n",
      "Drone Agent coordinate: tensor([[0.8333, 0.1000]])\n",
      "..........\n",
      "Episode 430 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.007623\n",
      "Learner:1\tReward total:0\tRunning mean: 0.06814\tNum agents crossed: 0\tRunning mean: 0.007623\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4135\tNum agents crossed: 0\tRunning mean: 0.007623\n",
      "Learner:3\tReward total:0\tRunning mean: 0.011\tNum agents crossed: 0\tRunning mean: 0.007623\n",
      "Learner:4\tReward total:0\tRunning mean: 0.06079\tNum agents crossed: 0\tRunning mean: 0.007623\n",
      "Max Norms =  ['29.08', '102.95', '82.40', '79.69', '68.90']\n",
      "Drone Agent coordinate: tensor([[0.9000, 0.1500]])\n",
      "..........\n",
      "Episode 440 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.006894\n",
      "Learner:1\tReward total:0\tRunning mean: 0.06162\tNum agents crossed: 0\tRunning mean: 0.006894\n",
      "Learner:2\tReward total:0\tRunning mean: -0.3938\tNum agents crossed: 0\tRunning mean: 0.006894\n",
      "Learner:3\tReward total:3\tRunning mean: 0.04946\tNum agents crossed: 0\tRunning mean: 0.006894\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05498\tNum agents crossed: 0\tRunning mean: 0.006894\n",
      "Max Norms =  ['30.68', '118.25', '155.25', '105.84', '80.26']\n",
      "Drone Agent coordinate: tensor([[0.7667, 0.1500]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Episode 450 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.006235\n",
      "Learner:1\tReward total:0\tRunning mean: 0.05573\tNum agents crossed: 0\tRunning mean: 0.006235\n",
      "Learner:2\tReward total:0\tRunning mean: -0.7613\tNum agents crossed: 0\tRunning mean: 0.006235\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04473\tNum agents crossed: 0\tRunning mean: 0.006235\n",
      "Learner:4\tReward total:0\tRunning mean: 0.04972\tNum agents crossed: 0\tRunning mean: 0.006235\n",
      "Max Norms =  ['14.92', '109.27', '125.14', '84.62', '58.26']\n",
      "Drone Agent coordinate: tensor([[0.6500, 0.0000]])\n",
      "..........\n",
      "Episode 460 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01525\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0504\tNum agents crossed: 0\tRunning mean: 0.01525\n",
      "Learner:2\tReward total:0\tRunning mean: -0.7277\tNum agents crossed: 0\tRunning mean: 0.01525\n",
      "Learner:3\tReward total:0\tRunning mean: 0.04045\tNum agents crossed: 0\tRunning mean: 0.01525\n",
      "Learner:4\tReward total:0\tRunning mean: 0.06437\tNum agents crossed: 0\tRunning mean: 0.01525\n",
      "Max Norms =  ['31.38', '95.76', '121.91', '56.54', '61.82']\n",
      "Drone Agent coordinate: tensor([[0.6667, 0.2000]])\n",
      "..........\n",
      "Episode 470 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.02292\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04558\tNum agents crossed: 0\tRunning mean: 0.02292\n",
      "Learner:2\tReward total:0\tRunning mean: -0.6025\tNum agents crossed: 0\tRunning mean: 0.02292\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03658\tNum agents crossed: 0\tRunning mean: 0.02292\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05822\tNum agents crossed: 0\tRunning mean: 0.02292\n",
      "Max Norms =  ['30.36', '94.46', '86.14', '87.22', '75.52']\n",
      "Drone Agent coordinate: tensor([[ 0.6667, -0.4500]])\n",
      "..........\n",
      "Episode 480 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.02073\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04122\tNum agents crossed: 0\tRunning mean: 0.02073\n",
      "Learner:2\tReward total:0\tRunning mean: -0.5548\tNum agents crossed: 0\tRunning mean: 0.02073\n",
      "Learner:3\tReward total:0\tRunning mean: 0.03309\tNum agents crossed: 0\tRunning mean: 0.02073\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1007\tNum agents crossed: 0\tRunning mean: 0.02073\n",
      "Max Norms =  ['31.11', '104.16', '115.91', '88.82', '101.93']\n",
      "Drone Agent coordinate: tensor([[0.9000, 0.2000]])\n",
      "..........\n",
      "Episode 490 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01875\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03728\tNum agents crossed: 0\tRunning mean: 0.01875\n",
      "Learner:2\tReward total:0\tRunning mean: -0.5018\tNum agents crossed: 0\tRunning mean: 0.01875\n",
      "Learner:3\tReward total:0\tRunning mean: 0.08604\tNum agents crossed: 0\tRunning mean: 0.01875\n",
      "Learner:4\tReward total:2\tRunning mean: 0.1111\tNum agents crossed: 0\tRunning mean: 0.01875\n",
      "Max Norms =  ['30.50', '110.57', '70.45', '120.99', '86.76']\n",
      "Drone Agent coordinate: tensor([[ 0.7000, -0.0500]])\n",
      "..........\n",
      "Episode 500 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01696\n",
      "Learner:1\tReward total:0\tRunning mean: 0.06168\tNum agents crossed: 0\tRunning mean: 0.01696\n",
      "Learner:2\tReward total:0\tRunning mean: -0.5114\tNum agents crossed: 0\tRunning mean: 0.01696\n",
      "Learner:3\tReward total:0\tRunning mean: -0.1011\tNum agents crossed: 0\tRunning mean: 0.01696\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1004\tNum agents crossed: 0\tRunning mean: 0.01696\n",
      "Max Norms =  ['38.21', '103.96', '134.66', '103.11', '80.02']\n",
      "Drone Agent coordinate: tensor([[ 0.6667, -0.2000]])\n",
      "..........\n",
      "Episode 510 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01533\n",
      "Learner:1\tReward total:0\tRunning mean: 0.05578\tNum agents crossed: 0\tRunning mean: 0.01533\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4723\tNum agents crossed: 0\tRunning mean: 0.01533\n",
      "Learner:3\tReward total:5\tRunning mean: -0.0414\tNum agents crossed: 0\tRunning mean: 0.01533\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1091\tNum agents crossed: 0\tRunning mean: 0.01533\n",
      "Max Norms =  ['27.73', '76.65', '154.79', '79.16', '94.31']\n",
      "Drone Agent coordinate: tensor([[0.6500, 0.1000]])\n",
      "..........\n",
      "Episode 520 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.01387\n",
      "Learner:1\tReward total:0\tRunning mean: 0.05045\tNum agents crossed: 0\tRunning mean: 0.01387\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4173\tNum agents crossed: 0\tRunning mean: 0.01387\n",
      "Learner:3\tReward total:2\tRunning mean: -0.008307\tNum agents crossed: 0\tRunning mean: 0.01387\n",
      "Learner:4\tReward total:0\tRunning mean: 0.09867\tNum agents crossed: 0\tRunning mean: 0.01387\n",
      "Max Norms =  ['24.75', '105.44', '156.47', '102.58', '79.82']\n",
      "Drone Agent coordinate: tensor([[0.6833, 0.0500]])\n",
      "..........\n",
      "Episode 530 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.03156\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04562\tNum agents crossed: 0\tRunning mean: 0.03156\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4048\tNum agents crossed: 0\tRunning mean: 0.03156\n",
      "Learner:3\tReward total:0\tRunning mean: 0.002094\tNum agents crossed: 0\tRunning mean: 0.03156\n",
      "Learner:4\tReward total:1\tRunning mean: 0.1183\tNum agents crossed: 0\tRunning mean: 0.03156\n",
      "Max Norms =  ['40.47', '116.72', '98.42', '91.80', '78.53']\n",
      "Drone Agent coordinate: tensor([[ 0.8667, -0.3000]])\n",
      "..........\n",
      "Episode 540 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.04757\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04126\tNum agents crossed: 0\tRunning mean: 0.04757\n",
      "Learner:2\tReward total:3\tRunning mean: -1.824\tNum agents crossed: 0\tRunning mean: 0.04757\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0114\tNum agents crossed: 0\tRunning mean: 0.04757\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1536\tNum agents crossed: 0\tRunning mean: 0.04757\n",
      "Max Norms =  ['23.39', '127.06', '97.49', '93.16', '86.86']\n",
      "Drone Agent coordinate: tensor([[0.4667, 0.0000]])\n",
      "..........\n",
      "Episode 550 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.06175\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03732\tNum agents crossed: 0\tRunning mean: 0.06175\n",
      "Learner:2\tReward total:0\tRunning mean: -1.996\tNum agents crossed: 0\tRunning mean: 0.06175\n",
      "Learner:3\tReward total:0\tRunning mean: 0.01031\tNum agents crossed: 0\tRunning mean: 0.06175\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1389\tNum agents crossed: 0\tRunning mean: 0.06175\n",
      "Max Norms =  ['35.49', '99.24', '90.18', '118.72', '71.05']\n",
      "Drone Agent coordinate: tensor([[0.4167, 0.2000]])\n",
      "..........\n",
      "Episode 560 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.09496\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01415\tNum agents crossed: 1\tRunning mean: 0.09496\n",
      "Learner:2\tReward total:0.0\tRunning mean: -1.776\tNum agents crossed: 1\tRunning mean: 0.09496\n",
      "Learner:3\tReward total:-4.0\tRunning mean: -0.03067\tNum agents crossed: 1\tRunning mean: 0.09496\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1349\tNum agents crossed: 1\tRunning mean: 0.09496\n",
      "Max Norms =  ['35.11', '116.18', '75.79', '106.86', '68.09']\n",
      "Drone Agent coordinate: tensor([[0.6167, 0.0000]])\n",
      "..........\n",
      "Episode 570 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.09588\n",
      "Learner:1\tReward total:0\tRunning mean: -0.288\tNum agents crossed: 1\tRunning mean: 0.09588\n",
      "Learner:2\tReward total:0\tRunning mean: -1.625\tNum agents crossed: 1\tRunning mean: 0.09588\n",
      "Learner:3\tReward total:1.0\tRunning mean: 0.01108\tNum agents crossed: 1\tRunning mean: 0.09588\n",
      "Learner:4\tReward total:0\tRunning mean: 0.122\tNum agents crossed: 1\tRunning mean: 0.09588\n",
      "Max Norms =  ['33.10', '81.95', '44.80', '94.66', '83.56']\n",
      "Drone Agent coordinate: tensor([[0.7333, 0.1500]])\n",
      "..........\n",
      "Episode 580 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.1157\n",
      "Learner:1\tReward total:0\tRunning mean: -0.2605\tNum agents crossed: 1\tRunning mean: 0.1157\n",
      "Learner:2\tReward total:2.0\tRunning mean: -1.44\tNum agents crossed: 1\tRunning mean: 0.1157\n",
      "Learner:3\tReward total:1.0\tRunning mean: -0.1492\tNum agents crossed: 1\tRunning mean: 0.1157\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1103\tNum agents crossed: 1\tRunning mean: 0.1157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['26.77', '111.52', '57.38', '96.22', '90.17']\n",
      "Drone Agent coordinate: tensor([[0.8000, 0.0500]])\n",
      "..........\n",
      "Episode 590 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.1047\n",
      "Learner:1\tReward total:0\tRunning mean: -0.2356\tNum agents crossed: 0\tRunning mean: 0.1047\n",
      "Learner:2\tReward total:0\tRunning mean: -1.449\tNum agents crossed: 0\tRunning mean: 0.1047\n",
      "Learner:3\tReward total:0\tRunning mean: -0.5836\tNum agents crossed: 0\tRunning mean: 0.1047\n",
      "Learner:4\tReward total:0\tRunning mean: 0.09979\tNum agents crossed: 0\tRunning mean: 0.1047\n",
      "Max Norms =  ['39.78', '160.05', '82.32', '87.11', '90.35']\n",
      "Drone Agent coordinate: tensor([[ 0.7167, -0.0500]])\n",
      "..........\n",
      "Episode 600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.09467\n",
      "Learner:1\tReward total:0\tRunning mean: -0.213\tNum agents crossed: 0\tRunning mean: 0.09467\n",
      "Learner:2\tReward total:0\tRunning mean: -1.31\tNum agents crossed: 0\tRunning mean: 0.09467\n",
      "Learner:3\tReward total:2\tRunning mean: -0.4708\tNum agents crossed: 0\tRunning mean: 0.09467\n",
      "Learner:4\tReward total:0\tRunning mean: 0.09025\tNum agents crossed: 0\tRunning mean: 0.09467\n",
      "Max Norms =  ['38.59', '89.02', '80.80', '84.34', '97.37']\n",
      "Drone Agent coordinate: tensor([[0.5000, 0.1500]])\n",
      "..........\n",
      "Episode 610 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.09561\n",
      "Learner:1\tReward total:0\tRunning mean: -0.163\tNum agents crossed: 1\tRunning mean: 0.09561\n",
      "Learner:2\tReward total:0\tRunning mean: -1.195\tNum agents crossed: 1\tRunning mean: 0.09561\n",
      "Learner:3\tReward total:7.0\tRunning mean: -0.8991\tNum agents crossed: 1\tRunning mean: 0.09561\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1012\tNum agents crossed: 1\tRunning mean: 0.09561\n",
      "Max Norms =  ['39.65', '121.94', '44.16', '79.94', '91.49']\n",
      "Drone Agent coordinate: tensor([[0.5500, 0.0000]])\n",
      "..........\n",
      "Episode 620 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.08647\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1474\tNum agents crossed: 0\tRunning mean: 0.08647\n",
      "Learner:2\tReward total:0\tRunning mean: -1.081\tNum agents crossed: 0\tRunning mean: 0.08647\n",
      "Learner:3\tReward total:-2.0\tRunning mean: -1.47\tNum agents crossed: 0\tRunning mean: 0.08647\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1489\tNum agents crossed: 0\tRunning mean: 0.08647\n",
      "Max Norms =  ['24.63', '111.95', '93.37', '96.71', '75.14']\n",
      "Drone Agent coordinate: tensor([[0.6000, 0.2000]])\n",
      "..........\n",
      "Episode 630 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0782\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1137\tNum agents crossed: 0\tRunning mean: 0.0782\n",
      "Learner:2\tReward total:0\tRunning mean: -0.9773\tNum agents crossed: 0\tRunning mean: 0.0782\n",
      "Learner:3\tReward total:-3.0\tRunning mean: -1.368\tNum agents crossed: 0\tRunning mean: 0.0782\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1641\tNum agents crossed: 0\tRunning mean: 0.0782\n",
      "Max Norms =  ['31.74', '146.07', '73.82', '94.24', '117.30']\n",
      "Drone Agent coordinate: tensor([[0.3333, 0.2000]])\n",
      "..........\n",
      "Episode 640 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.07073\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1901\tNum agents crossed: 0\tRunning mean: 0.07073\n",
      "Learner:2\tReward total:0\tRunning mean: -0.8933\tNum agents crossed: 0\tRunning mean: 0.07073\n",
      "Learner:3\tReward total:0.0\tRunning mean: -1.17\tNum agents crossed: 0\tRunning mean: 0.07073\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1852\tNum agents crossed: 0\tRunning mean: 0.07073\n",
      "Max Norms =  ['26.82', '148.58', '90.59', '103.98', '72.71']\n",
      "Drone Agent coordinate: tensor([[0.6667, 0.2000]])\n",
      "..........\n",
      "Episode 650 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.06396\n",
      "Learner:1\tReward total:0\tRunning mean: -0.172\tNum agents crossed: 0\tRunning mean: 0.06396\n",
      "Learner:2\tReward total:0\tRunning mean: -0.817\tNum agents crossed: 0\tRunning mean: 0.06396\n",
      "Learner:3\tReward total:2\tRunning mean: -1.028\tNum agents crossed: 0\tRunning mean: 0.06396\n",
      "Learner:4\tReward total:2\tRunning mean: 0.2149\tNum agents crossed: 0\tRunning mean: 0.06396\n",
      "Max Norms =  ['34.75', '127.20', '51.21', '103.60', '74.25']\n",
      "Drone Agent coordinate: tensor([[0.8333, 0.0500]])\n",
      "..........\n",
      "Episode 660 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.05785\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1555\tNum agents crossed: 0\tRunning mean: 0.05785\n",
      "Learner:2\tReward total:0\tRunning mean: -0.7488\tNum agents crossed: 0\tRunning mean: 0.05785\n",
      "Learner:3\tReward total:0\tRunning mean: -0.9517\tNum agents crossed: 0\tRunning mean: 0.05785\n",
      "Learner:4\tReward total:1\tRunning mean: 0.01059\tNum agents crossed: 0\tRunning mean: 0.05785\n",
      "Max Norms =  ['50.40', '78.39', '124.10', '104.08', '96.53']\n",
      "Drone Agent coordinate: tensor([[0.6833, 0.1500]])\n",
      "..........\n",
      "Episode 670 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.05232\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1406\tNum agents crossed: 0\tRunning mean: 0.05232\n",
      "Learner:2\tReward total:0\tRunning mean: -0.6864\tNum agents crossed: 0\tRunning mean: 0.05232\n",
      "Learner:3\tReward total:0\tRunning mean: -0.8221\tNum agents crossed: 0\tRunning mean: 0.05232\n",
      "Learner:4\tReward total:0\tRunning mean: 0.05908\tNum agents crossed: 0\tRunning mean: 0.05232\n",
      "Max Norms =  ['33.00', '121.24', '59.72', '65.33', '57.90']\n",
      "Drone Agent coordinate: tensor([[ 0.5667, -0.1500]])\n",
      "..........\n",
      "Episode 680 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.04731\n",
      "Learner:1\tReward total:0\tRunning mean: -0.1272\tNum agents crossed: 0\tRunning mean: 0.04731\n",
      "Learner:2\tReward total:0\tRunning mean: -0.6208\tNum agents crossed: 0\tRunning mean: 0.04731\n",
      "Learner:3\tReward total:0\tRunning mean: -1.306\tNum agents crossed: 0\tRunning mean: 0.04731\n",
      "Learner:4\tReward total:0\tRunning mean: 0.09303\tNum agents crossed: 0\tRunning mean: 0.04731\n",
      "Max Norms =  ['29.50', '116.90', '30.58', '137.52', '75.39']\n",
      "Drone Agent coordinate: tensor([[0.6167, 0.1500]])\n",
      "..........\n",
      "Episode 690 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.04279\n",
      "Learner:1\tReward total:0\tRunning mean: -0.115\tNum agents crossed: 0\tRunning mean: 0.04279\n",
      "Learner:2\tReward total:0\tRunning mean: -0.5614\tNum agents crossed: 0\tRunning mean: 0.04279\n",
      "Learner:3\tReward total:0\tRunning mean: -1.56\tNum agents crossed: 0\tRunning mean: 0.04279\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1534\tNum agents crossed: 0\tRunning mean: 0.04279\n",
      "Max Norms =  ['34.95', '56.61', '34.72', '108.20', '94.06']\n",
      "Drone Agent coordinate: tensor([[0.3167, 0.1000]])\n",
      "..........\n",
      "Episode 700 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0484\n",
      "Learner:1\tReward total:0\tRunning mean: -0.104\tNum agents crossed: 0\tRunning mean: 0.0484\n",
      "Learner:2\tReward total:0\tRunning mean: -0.5169\tNum agents crossed: 0\tRunning mean: 0.0484\n",
      "Learner:3\tReward total:0\tRunning mean: -1.238\tNum agents crossed: 0\tRunning mean: 0.0484\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1844\tNum agents crossed: 0\tRunning mean: 0.0484\n",
      "Max Norms =  ['25.47', '105.09', '18.90', '88.12', '77.83']\n",
      "Drone Agent coordinate: tensor([[0.6833, 0.1500]])\n",
      "..........\n",
      "Episode 710 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.05309\n",
      "Learner:1\tReward total:0\tRunning mean: -0.09409\tNum agents crossed: 0\tRunning mean: 0.05309\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4674\tNum agents crossed: 0\tRunning mean: 0.05309\n",
      "Learner:3\tReward total:0\tRunning mean: -1.104\tNum agents crossed: 0\tRunning mean: 0.05309\n",
      "Learner:4\tReward total:0\tRunning mean: 0.2033\tNum agents crossed: 0\tRunning mean: 0.05309\n",
      "Max Norms =  ['39.92', '159.65', '13.72', '59.01', '74.92']\n",
      "Drone Agent coordinate: tensor([[ 0.2167, -0.3500]])\n",
      "..........\n",
      "Episode 720 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.06723\n",
      "Learner:1\tReward total:0\tRunning mean: -0.08509\tNum agents crossed: 0\tRunning mean: 0.06723\n",
      "Learner:2\tReward total:0\tRunning mean: -0.4227\tNum agents crossed: 0\tRunning mean: 0.06723\n",
      "Learner:3\tReward total:1\tRunning mean: -0.9601\tNum agents crossed: 0\tRunning mean: 0.06723\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1839\tNum agents crossed: 0\tRunning mean: 0.06723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['29.28', '87.16', '21.46', '78.16', '100.20']\n",
      "Drone Agent coordinate: tensor([[ 0.5833, -0.1000]])\n",
      "..........\n",
      "Episode 730 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 1\tRunning mean: 0.0708\n",
      "Learner:1\tReward total:0\tRunning mean: -0.07696\tNum agents crossed: 1\tRunning mean: 0.0708\n",
      "Learner:2\tReward total:0\tRunning mean: -0.3823\tNum agents crossed: 1\tRunning mean: 0.0708\n",
      "Learner:3\tReward total:9.0\tRunning mean: -0.7693\tNum agents crossed: 1\tRunning mean: 0.0708\n",
      "Learner:4\tReward total:0\tRunning mean: 0.2046\tNum agents crossed: 1\tRunning mean: 0.0708\n",
      "Max Norms =  ['17.04', '143.97', '1.40', '88.94', '95.61']\n",
      "Drone Agent coordinate: tensor([[0.5333, 0.1000]])\n",
      "..........\n",
      "Episode 740 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.06403\n",
      "Learner:1\tReward total:0\tRunning mean: -0.0696\tNum agents crossed: 0\tRunning mean: 0.06403\n",
      "Learner:2\tReward total:0\tRunning mean: -0.3458\tNum agents crossed: 0\tRunning mean: 0.06403\n",
      "Learner:3\tReward total:0\tRunning mean: -0.6859\tNum agents crossed: 0\tRunning mean: 0.06403\n",
      "Learner:4\tReward total:0\tRunning mean: 0.185\tNum agents crossed: 0\tRunning mean: 0.06403\n",
      "Max Norms =  ['36.92', '101.87', '12.78', '88.16', '71.71']\n",
      "Drone Agent coordinate: tensor([[0.3833, 0.2000]])\n",
      "."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize environment\n",
    "torch.manual_seed(100)\n",
    "game = \"Crossing\"\n",
    "num_crawler_actions = 8                     # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                      # Drones are capable of 12 actions\n",
    "num_goal_params = 2    # Goal has 2 parameter\n",
    "\n",
    "experiment = '1T-11L/strategist/'    # 1 team of 1 drone leader + 10 crawler followers directed by a strategist\n",
    "\n",
    "# Map and Parameter sets\n",
    "map_name = \"food_d37_river_w1_d25\"  \n",
    "parameters =[ \n",
    "            {'temp_start':1.8, 'river_penalty':-1.0, 'target_reward':1.0, 'game_steps':300}\n",
    "            ]\n",
    "\n",
    "temp_end = 1.0   # temp parameter is annealed from the value stored in parameters['temp_start'] to 1.0 \n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 7      # environ observation consists of a list of stacked frames per agent\n",
    "max_episodes = 1000\n",
    "\n",
    "render = True    # This turns on rendering every save so that agents' behavior can be observed\n",
    "SPEED = 1/30\n",
    "second_pile_x = 50  # x-coordinate of the 2nd food pile\n",
    "\n",
    "log_interval = 10\n",
    "save_interval = 20\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   1 agents - 5 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 5\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "\n",
    "\n",
    "def calc_deltas(goal, current):\n",
    "    # Calculate delta between the current and the target coordinates\n",
    "    target_x, target_y = goal\n",
    "    current_x, current_y = current\n",
    "    delta_x = (target_x - current_x)/60   # normalize\n",
    "    delta_y = (target_y - current_y)/20    # normalize\n",
    "    deltas = torch.Tensor([delta_x,delta_y])\n",
    "    deltas = deltas.view(1, -1)\n",
    "        \n",
    "    # print(deltas)\n",
    "    return deltas   \n",
    "       \n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    start = time.clock()  # time the training\n",
    "    \n",
    "    situation = 'pac_simple_droneleader_followers'\n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "    \n",
    "    # Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "    teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "    ]\n",
    "    agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (4,8)},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9)},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8)},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7)},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9)}\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8)},\n",
    "        {'id': 6, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (5,7)},\n",
    "        {'id': 7, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (5,9)},\n",
    "        {'id': 8, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (6,8)},\n",
    "        {'id': 9, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (7,7)},\n",
    "        {'id': 10, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (7,9)}\n",
    "\n",
    "    ]\n",
    "    \"\"\"\n",
    "        \n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            \n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            \n",
    "            # Initialize agent policy based on type\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                agents.append(Drone_Policy(num_frames, num_drone_actions, i)) \n",
    "            elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                print(\"Load Drone Leader.\")\n",
    "                agents.append(DroneLeader_Advanced(num_frames, num_goal_params, num_drone_actions, i)) \n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "            \n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "        \n",
    "        # Keep track of num learners who has crossed over to the 2nd food pile\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "        running_crossed = None         # running average\n",
    "        running_crossed_hist = []   # history of running averages\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Crawler_Policy(input_channels=num_frames, num_actions=num_crawler_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "    # Attach agents to their teams\n",
    "    # 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "    teams = []\n",
    "    # Team Vikings\n",
    "    teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'], \\\n",
    "                  culture=teams_params[0]['culture'], roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0],agents[1],agents[2],agents[3],agents[4]], \\\n",
    "                  #   agents[5],agents[6],agents[7],agents[8],agents[9],agents[10]],  \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:1]]))\n",
    "    \n",
    "    # 5-30-2019  Strategist accepts directorship of a team\n",
    "    suntzu = Strategist()\n",
    "    suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "    \n",
    "    env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = True, debug_agent=1)   \n",
    "\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with Crawler_Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        \n",
    "        # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize reward and agents crossed counters\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    actions[i], log_probs[i] =  select_action_strat(agents[i], agents_obs[i], deltas, cuda)\n",
    "                else:    \n",
    "                    actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                \n",
    "                # Only crawlers can fire lasers\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                        \n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with Crawler_Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            \n",
    "            load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "            \n",
    "            # 5-30-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "            game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "            goals, topology = suntzu.generate_goals(game_space)\n",
    "            deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "            agents[0].deltas.append(deltas)   # Store a history of deltas for generating mission rewards\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "            \n",
    "            if render and (ep % save_interval == 0):   # render 1 episode every save\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "\n",
    "        # Keep track num of agents who gather from 2nd food pile. Note that env.consumption tracks the \n",
    "        # agent index and location of apple gathered\n",
    "        for (i, loc) in env.consumption:\n",
    "            if loc[0] > second_pile_x:   # If x-cood of gathered apple is beyond a preset value, it is\n",
    "                                         # in the 2nd pile\n",
    "                crossed[i] = 1\n",
    "        episode_crossed = sum(crossed)   # sum up the num agents who crossed to 2nd pile for the episode\n",
    "                \n",
    "        # Update reward and crossed statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "            \n",
    "        if running_crossed is None:\n",
    "            running_crossed = episode_crossed\n",
    "        running_crossed = running_crossed * 0.99 + episode_crossed * 0.01\n",
    "        running_crossed_hist.append(running_crossed)\n",
    "                \n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                verbose_str += '\\tNum agents crossed: {}'.format(episode_crossed)\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_crossed)\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(teams, agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "            print('Drone Agent coordinate: {}'.format(deltas))\n",
    "\n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/' + experiment + map_name\n",
    "                results_dir = 'results/' + experiment + map_name\n",
    "\n",
    "                model_file = model_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    save_model(f, ep, agents[i], optimizers[i])\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "             \n",
    "            crossed_file = results_dir+'/{}/t{}_rp{}_{}gs/Crossed.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(crossed_file), exist_ok=True)\n",
    "            with open(crossed_file, 'wb') as f:\n",
    "                    pickle.dump(running_crossed_hist, f)\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))\n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
