{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leader\n",
    "\n",
    "In this notebook, we develop a leader policy that learns to lead followers to accomplish a mission by maximizing rewards linked to mission metrics.\n",
    "\n",
    "We also modify the environment so that it recognizes multiple roles in a team of agents - leader and followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python version:  3.6.8\n",
      "Pytorch version: 1.0.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from xteams_env import CrossingEnv\n",
    "from xteams_model import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    # Agents trained in map = food_d37\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/',   # scenario=1\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/',   # scenario=2\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=3\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/',   # scenario=4\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/',   # scenario=5\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/',   # scenario=6\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/',   # scenario=7\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/',   # scenario=8\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/',   # scenario=9\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/',   # scenario=10\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/',   # scenario=11\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/',   # scenario=12\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/',   # scenario=13\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/',   # scenario=14\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/',   # scenario=15\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/',   # scenario=16\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/',   # scenario=17\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/',   # scenario=18\n",
    "\n",
    "    # Agents trained in map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=19\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=20 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=21\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\",   # scenario=22\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37\n",
    "    \"models/2T-5L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/\",     # scenario=23\n",
    "    \"models/2T-5L/baseline/food_d37/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=24\n",
    "    \n",
    "    # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "    # Team Viking (Pacifist w/o leader) vs Team Frank (Cooperative)\n",
    "    \"models/2T-5L/baseline/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\",   # scenario=25\n",
    "    # Team Viking (Pacifist w/ leader) vs Team Frank (Cooperative)\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t1.25_rp-1.0_300gs/\", # scenario=26\n",
    "    \"models/2T-5L/pac_leader/food_d37_river_w1_d25/pac_vs_coop/t2.0_rp-1.0_300gs/\",   # scenario=27\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_coop/t1.5_rp-1.0_300gs/\",   # scenario=28\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t1.5_rp-1.0_300gs/\",   # scenario=29 \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t2.0_rp-1.0_300gs/\",   # scenario=30     \n",
    "]\n",
    "\n",
    "# Parameter sets pertaining to the trained models in the folders above (not used in the code)\n",
    "parameters =[ \n",
    "        # Temperature for explore/exploit; penalty per step in river; game steps per episode\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37\n",
    "            {'temp_start':0.4, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':0.8, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "    \n",
    "        # 1 Team of 10 Agents trained in map = food_d37_river_w1_d25    \n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "    \n",
    "        # 2 Teams of 5 Agents trained in map = food_d37_river_w1_d25\n",
    "        # Team Viking (Pacifist w/ leader) vs Team Frank (Cooperative)\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},    \n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},   \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300}\n",
    "            ]\n",
    "\n",
    "print (len(parameters))\n",
    "print (len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teams + Agents --> Env\n",
    "\n",
    "The new Crossing environment accepts a list of parameters of the participating agents and teams.\n",
    "\n",
    "**Team parameters**\n",
    "* name - name of team\n",
    "* color - color of the team (can be over-ridden by agent colors)\n",
    "* culture - use to calculate and dole out team rewards to agents during training\n",
    "* roles - a list of possible roles in the team\n",
    "* target_zone - an area where agents are rewarded to assemble within\n",
    "* banned_zone - an area where agents are penalized for staying within\n",
    "\n",
    "**Agent parameters**\n",
    "* id - id of the agent\n",
    "* team - the team the agent is attached to\n",
    "* color - color of the agent when rendering (can over-ride default team color\n",
    "* type - 'crawler' or 'drone' - affecting the agent's policy NN, action and obs spaces\n",
    "* role - the agent's role in the team\n",
    "\n",
    "The code below plays a game using the new environment using Agents previously trained in Crossing_Baseline. Agent 0 of Team Viking has the 'leader' role. \n",
    "\n",
    "If an agent is a 'leader', the Environment sets the target_zone for its team based on the agent's location. During rendering, the target zone is represented by a green box.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "Agent0 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent1 reward is 0\n",
      "Agent1 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent2 reward is 0\n",
      "Agent2 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent3 reward is 0\n",
      "Agent3 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 1\n",
      "Agent4 reward is 0\n",
      "Agent5 reward is 0\n",
      "Agent5 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent6 reward is 29\n",
      "Agent6 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent7 reward is 28\n",
      "Agent7 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent8 reward is 33\n",
      "Agent8 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent9 reward is 0\n",
      "Agent9 aggressiveness is 0.00\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Total rewards gathered = 90\n",
      "Av. rewards per agent = 9.00\n",
      "Num laser fired = 1\n",
      "Total US Hit (friendly fire) = 0\n",
      "Total THEM Hit = 1\n",
      "friendly fire (%) = 0.000\n",
      "Num agents gathering from 2nd food pile: 0\n",
      "\n",
      "Statistics by Team\n",
      "===================\n",
      "Team Vikings has total reward of 0\n",
      "Team Franks has total reward of 90\n",
      "Dominating Team: Franks\n",
      "Team dominance: 818181818.18x\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "# map_name \"food_d37\"\n",
    "\n",
    "device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "\n",
    "scenario = 30\n",
    "dir_name = folders[scenario-1]\n",
    "parameter = parameters[scenario-1]\n",
    "episodes = 3000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "        {'name': 'Franks', 'color': 'red',\n",
    "         'culture': {'name':'pacifist','laser_penalty':-1.0},\n",
    "         # 'culture': {'name':'cooperative','coop_factor':5.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}\n",
    "]\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',     \\\n",
    "         'role': 'follower', 'start': (1,7)},  # Use a different color for Leader\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9)},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8)},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7)},\n",
    "        # Leader of Team Viking is a drone and has a different color\n",
    "        {'id': 4, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)},\n",
    "        {'id': 5, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,1)},\n",
    "        {'id': 6, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,3)},\n",
    "        {'id': 7, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,2)},\n",
    "        {'id': 8, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,1)},\n",
    "        {'id': 9, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,3)}\n",
    "]\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 500\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, 0)\n",
    "                elif agents_params[i]['type'] is 'drone':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, 0)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0], agents[1], agents[2], agents[3], agents[4]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:5]]))\n",
    "# Team Franks\n",
    "teams.append(Team(name=teams_params[1]['name'],color=teams_params[1]['color'],   \\\n",
    "                  culture=teams_params[1]['culture'], roles=teams_params[1]['roles'], \\\n",
    "                  agent_policies=[agents[5], agents[6], agents[7], agents[8], agents[9]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[5:10]]))\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/frame))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_ai_agents))\n",
    "print ('Num laser fired = {}'.format(total_tags))\n",
    "print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_team = None\n",
    "top_team_reward = 0\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    if team.name is not 'Crazies':\n",
    "        reward = sum(team.sum_rewards())\n",
    "        print ('Team {} has total reward of {}'.format(team.name, reward))\n",
    "                           \n",
    "        if reward > top_team_reward:   # Keep track of dominating team\n",
    "            top_team_reward = reward\n",
    "            top_team = team.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(teams) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_team))\n",
    "    dominance = top_team_reward/((total_rewards-top_team_reward+1.1e-7)/(len(teams)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model_file = \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_coop/t1.5_rp-1.0_300gs/MA0_Crossing_ep1000.p\"\n",
    "\n",
    "checkpoint = torch.load(model_file, map_location=device)\n",
    "episode = checkpoint['epoch']\n",
    "agent.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print (episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1092da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.sum(env.initial_food[30:40, 20:30]))\n",
    "\n",
    "plt.imshow(env.initial_food)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 60)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(100, 60, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "(10, 20, 7)\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 100, 60])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "torch.Size([1, 7, 10, 20])\n",
      "100\n",
      "60\n",
      "[(33, 37), (32, 20), (33, 36), (32, 37), (31, 33), (58, 21), (22, 39), (21, 21), (67, 20), (79, 23)]\n",
      "['Vikings', 'Franks']\n",
      "[0, 0, 0, 0, None, 0, 0, 0, 0, 0]\n",
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c73ded1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADKCAYAAACFWKrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAC9hJREFUeJzt3W2sZdVdx/HvzxlGhCIMktTpQFpomiZqYjuZEFqxIaEiRdKppjHTWMXWZNIoEUyahtjYNCbG1KdYjdGMiKIhLSmlShragtqobxgZpsPDzFCYIpYZB6jSQI0vWuTvi7Nvudw5D/sOZ59z1/j9JDd3n73XPuefNev+7jpr7zM3VYUkqR3fs+wCJEnrY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGrN5iCdN4scxJWmdqip92jnjlqTGGNyS1BiDW5IaY3BLUmN6BXeSq5N8NcnRJDcNXZQkabLM+v+4k2wCHgN+AjgG3A+8r6oOTznHu0okaZ3meVfJpcDRqnqiqr4NfBrY9WqKkySduj7BvR14atXjY90+SdISzO0DOEn2AHvm9XySpPH6BPdx4KJVjy/s9r1CVe0F9oJr3JI0pD5LJfcDb0pycZItwG7grmHLkiRNMnPGXVUvJrke+BKwCbilqg4NXpkkaayZtwOe0pO6VCJJ6+Z/MiVJpymDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWrMzL/yLmljGuIPff9/lfT6G70bhjNuSWqMwS1JjTG4JakxBrckNWZmcCe5KMmXkxxOcijJDYsoTJI0XmZdmU6yDdhWVQeSnAM8ALynqg5POcfL3dLAvKtkfjbKXSVV1auQmTPuqjpRVQe67W8BR4Dtr648SdKpWtd93EneALwV2Dfm2B5gz1yqkiRNNHOp5LsNk9cA/wT8VlXdOaOt7+GkgblUMj+n3VIJQJIzgM8Ct80KbUnSsPpcnAxwK/BcVd3Y60mdcUuDc8Y9P63NuPsE9+XAvwAPAy91u3+9qu6eco4jShqYwT0/p11wnwqDWxqewT0/rQW3n5yUpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1pndwJ9mU5CtJPj9kQZKk6dYz474BODJUIZKkfnoFd5ILgZ8Cbh62HEnSLH1n3H8IfAR4acBaJEk9zAzuJNcCz1bVAzPa7UmyP8n+uVUnSTpJqmp6g+S3gZ8HXgTOBL4fuLOq3j/lnOlPKulVm/Wzq/6SLLsEAKqqVyEzg/sVjZMrgA9X1bUz2jmipIEZ3PPTWnB7H7ckNWZdM+7eT+qMWxqcM+75ccYtSRqUwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWpMr+BOcl6SO5I8muRIkrcNXZgkabzNPdt9EvhiVb03yRbgrAFrkiRNkaqa3iA5FzgIXFKzGr98Tq92kk5dzx9H9ZBk2SUAUFW9CumzVHIx8A3gL5N8JcnNSc5e2yjJniT7k+xfZ62SpHXoM+PeCdwH/FhV7UvySeCFqvqNKec4FZAG5ox7fk7HGfcx4FhV7ese3wHsONXCJEmvzszgrqqngaeSvLnbdSVweNCqJEkTzVwqAUjyFuBmYAvwBPCBqvrmlPa+h5MG5lLJ/LS2VNIruNfL4JaGZ3DPT2vB7ScnJakxBrckNcbglqTG9P3Iu6QNZqOsy2rxnHFLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxvYI7ya8lOZTkkSSfSnLm0IVJksabGdxJtgO/Cuysqh8BNgG7hy5MkjRe36WSzcD3JdkMnAX8x3AlSZKmmRncVXUc+D3g68AJ4PmqumfowiRJ4/VZKtkK7AIuBl4HnJ3k/WPa7UmyP8n++ZcpSVrRZ6nkncC/VdU3quo7wJ3A29c2qqq9VbWzqnbOu0hJ0sv6BPfXgcuSnJUkwJXAkWHLkiRN0meNex9wB3AAeLg7Z+/AdUmSJkhVzf9Jk/k/qSSd5qoqfdr5yUlJaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDVm80DP+5/Av085fkHXZqOzzvlpoUawznmzzv5e37fhIH/lfeaLJvuraufCX3idrHN+WqgRrHPerHMYLpVIUmMMbklqzLKCe++SXne9rHN+WqgRrHPerHMAS1njliSdOpdKJKkxgwZ3kquTfDXJ0SQ3jTn+vUlu747vS/KGIeuZUONFSb6c5HCSQ0luGNPmiiTPJznYfX1s0XV2dTyZ5OGuhv1jjifJH3X9+VCSHQuu782r+uhgkheS3LimzVL6MsktSZ5N8siqfecnuTfJ4933rRPOva5r83iS65ZQ5+8mebT7N/1ckvMmnDt1fCygzo8nOb7q3/aaCedOzYUF1Hn7qhqfTHJwwrkL6891q6pBvoBNwNeAS4AtwIPAD61p88vAn3Xbu4Hbh6pnSp3bgB3d9jnAY2PqvAL4/KJrG1Prk8AFU45fA3wBCHAZsG+JtW4CngZevxH6EngHsAN4ZNW+3wFu6rZvAj4x5rzzgSe671u77a0LrvMqYHO3/YlxdfYZHwuo8+PAh3uMi6m5MHSda47/PvCxZffner+GnHFfChytqieq6tvAp4Fda9rsAm7ttu8ArkySAWs6SVWdqKoD3fa3gCPA9kXWMEe7gL+ukfuA85JsW1ItVwJfq6ppH8RamKr6Z+C5NbtXj79bgfeMOfUngXur6rmq+iZwL3D1Iuusqnuq6sXu4X3AhUO9fl8T+rOPPrkwN9Pq7LLmZ4FPDfX6QxkyuLcDT616fIyTA/G7bbqB+TzwAwPWNFW3VPNWYN+Yw29L8mCSLyT54YUW9rIC7knyQJI9Y4736fNF2c3kH4iN0JcAr62qE93208Brx7TZSH0K8EFG76rGmTU+FuH6bknnlglLTxupP38ceKaqHp9wfCP051henOwkeQ3wWeDGqnphzeEDjN7y/yjwx8DfLrq+zuVVtQN4F/ArSd6xpDqmSrIFeDfwmTGHN0pfvkKN3htv6FusknwUeBG4bUKTZY+PPwXeCLwFOMFoGWIjex/TZ9vL7s+Jhgzu48BFqx5f2O0b2ybJZuBc4L8GrGmsJGcwCu3bqurOtcer6oWq+u9u+27gjCQXLLhMqup49/1Z4HOM3nau1qfPF+FdwIGqembtgY3Sl51nVpaSuu/PjmmzIfo0yS8C1wI/1/2SOUmP8TGoqnqmqv63ql4C/nzC62+U/twM/Axw+6Q2y+7PaYYM7vuBNyW5uJuB7QbuWtPmLmDlKv17gX+cNCiH0q1z/QVwpKr+YEKbH1xZe09yKaN+W+gvmCRnJzlnZZvRBatH1jS7C/iF7u6Sy4DnVy0FLNLEmcxG6MtVVo+/64C/G9PmS8BVSbZ2b/2v6vYtTJKrgY8A766q/5nQps/4GNSa6yk/PeH1++TCIrwTeLSqjo07uBH6c6ohr3wyusvhMUZXkT/a7ftNRgMQ4ExGb6ePAv8KXLLoq7PA5YzeIj8EHOy+rgE+BHyoa3M9cIjRFfD7gLcvoc5Lutd/sKtlpT9X1xngT7r+fhjYuYQ6z2YUxOeu2rf0vmT0i+QE8B1G66q/xOh6yj8AjwN/D5zftd0J3Lzq3A92Y/Qo8IEl1HmU0brwyvhcuRPrdcDd08bHguv8m27cPcQojLetrbN7fFIuLLLObv9frYzJVW2X1p/r/fKTk5LUGC9OSlJjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhrzf2wo4jbqpMgbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c6c030390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c648d2438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c64866eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 60, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c647f4d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADKCAYAAACFWKrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAC4pJREFUeJzt3X/sXfVdx/Hny5aKMIQiyewK2cAsS9TErWkIm7iQMJEhWadZTBenuJk0ixLBZFmIi8tiYsz8FacxmoooGrIRGVOyMAfqov5DpXTlR1sGHeJoLTBlgRn/2JC3f9zT8eXb++Pccs+93w95PpKb77nnvM+573x6+vqe+zn3tqkqJEnt+K5VNyBJmo/BLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWrM5iEOmsSvY0rSnKoqfeq84pakxhjcktQYg1uSGmNwS1JjegV3kquTfCXJ0SQ3Dd2UJGmyzPr3uJNsAh4Dfhw4BtwPvL+qDk/Zx0+VSNKcFvmpkkuBo1X1RFV9C/gMsOvVNCdJOn19gns78NSa58e6dZKkFVjYF3CS7AH2LOp4kqTx+gT3ceCiNc8v7Na9QlXtBfaCc9ySNKQ+UyX3A29OcnGSLcBu4K5h25IkTTLziruqXkxyPfBFYBNwS1UdGrwzSdJYMz8OeFoHdapEkubmPzIlSa9RBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxM4M7yUVJvpTkcJJDSW5YRmOSpPFSVdMLkm3Atqo6kOQc4AHgvVV1eMo+0w8qSTpFVaVP3cwr7qo6UVUHuuVvAkeA7a+uPUnS6do8T3GSNwFvA/aN2bYH2LOQriRJE82cKvlOYfI64J+B36yqO2fUOlUiSXNa2FQJQJIzgM8Ct80KbUnSsPrcnAxwK/BcVd3Y66BecUvS3PpecfcJ7suBfwUeBl7qVv9aVd09ZR+DW5LmtLDgPh0GtyTNb6Fz3JKkjcPglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxvYM7yaYkX07y+SEbkiRNN88V9w3AkaEakST10yu4k1wI/CRw87DtSJJm6XvF/QfAR4GXBuxFktTDzOBOci3wbFU9MKNuT5L9SfYvrDtJ0ilSVdMLkt8Cfg54ETgT+F7gzqr6wJR9ph9UknSKqkqfupnB/Yri5ArgI1V17Yw6g1uS5tQ3uP0ctyQ1Zq4r7t4H9YpbkubmFbckvUYZ3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMb0Cu4k5yW5I8mjSY4kefvQjUmSxtvcs+5TwN9X1fuSbAHOGrAnSdIUqarpBcm5wEHgkppV/PI+veokSS+rqvSp6zNVcjHwdeAvknw5yc1Jzl5flGRPkv1J9s/ZqyRpDn2uuHcC9wE/WlX7knwKeKGqfn3KPl5xS9KcFnnFfQw4VlX7uud3ADtOtzFJ0qszM7ir6mngqSRv6VZdCRwetCtJ0kQzp0oAkrwVuBnYAjwBfLCqvjGl3qkSSZpT36mSXsE9L4Nbkua3yDluSdIGYnBLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1JhewZ3kV5McSvJIkk8nOXPoxiRJ480M7iTbgV8BdlbVDwObgN1DNyZJGq/vVMlm4HuSbAbOAv5zuJYkSdPMDO6qOg78LvA14ATwfFXdM3RjkqTx+kyVbAV2ARcDbwDOTvKBMXV7kuxPsn/xbUqSTuozVfIu4N+r6utV9W3gTuAd64uqam9V7ayqnYtuUpL0sj7B/TXgsiRnJQlwJXBk2LYkSZP0mePeB9wBHAAe7vbZO3BfkqQJUlWLP2iy+INK0mtcVaVPnd+clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYzYPdNz/Av5jyvYLupqNzj4Xp4UewT4XzT77e2PfwkH+l/eZL5rsr6qdS3/hOdnn4rTQI9jnotnnMJwqkaTGGNyS1JhVBffeFb3uvOxzcVroEexz0exzACuZ45YknT6nSiSpMYMGd5Krk3wlydEkN43Z/t1Jbu+270vypiH7mdDjRUm+lORwkkNJbhhTc0WS55Mc7B4fX3afXR9PJnm462H/mO1J8ofdeD6UZMeS+3vLmjE6mOSFJDeuq1nJWCa5JcmzSR5Zs+78JPcmebz7uXXCvtd1NY8nuW4Fff5Okke7P9PPJTlvwr5Tz48l9PmJJMfX/NleM2HfqbmwhD5vX9Pjk0kOTth3aeM5t6oa5AFsAr4KXAJsAR4EfnBdzS8Bf9ot7wZuH6qfKX1uA3Z0y+cAj43p8wrg88vubUyvTwIXTNl+DfAFIMBlwL4V9roJeBp440YYS+CdwA7gkTXrfhu4qVu+CfjkmP3OB57ofm7tlrcuuc+rgM3d8ifH9dnn/FhCn58APtLjvJiaC0P3uW777wEfX/V4zvsY8or7UuBoVT1RVd8CPgPsWlezC7i1W74DuDJJBuzpFFV1oqoOdMvfBI4A25fZwwLtAv6qRu4DzkuybUW9XAl8taqmfRFraarqX4Dn1q1ee/7dCrx3zK4/AdxbVc9V1TeAe4Grl9lnVd1TVS92T+8DLhzq9fuaMJ599MmFhZnWZ5c1PwN8eqjXH8qQwb0deGrN82OcGojfqelOzOeB7xuwp6m6qZq3AfvGbH57kgeTfCHJDy21sZcVcE+SB5LsGbO9z5gvy24m/4XYCGMJ8PqqOtEtPw28fkzNRhpTgA8xelc1zqzzYxmu76Z0bpkw9bSRxvPHgGeq6vEJ2zfCeI7lzclOktcBnwVurKoX1m0+wOgt/48AfwT87bL761xeVTuAdwO/nOSdK+pjqiRbgPcAfzNm80YZy1eo0XvjDf0RqyQfA14EbptQsurz40+AHwDeCpxgNA2xkb2f6Vfbqx7PiYYM7uPARWueX9itG1uTZDNwLvDfA/Y0VpIzGIX2bVV15/rtVfVCVf1Pt3w3cEaSC5bcJlV1vPv5LPA5Rm871+oz5svwbuBAVT2zfsNGGcvOMyenkrqfz46p2RBjmuQXgGuBn+1+yZyix/kxqKp6pqr+r6peAv5swutvlPHcDPw0cPukmlWP5zRDBvf9wJuTXNxdge0G7lpXcxdw8i79+4B/mnRSDqWb5/pz4EhV/f6Emu8/Ofee5FJG47bUXzBJzk5yzsllRjesHllXdhfw892nSy4Dnl8zFbBME69kNsJYrrH2/LsO+LsxNV8ErkqytXvrf1W3bmmSXA18FHhPVf3vhJo+58eg1t1P+akJr98nF5bhXcCjVXVs3MaNMJ5TDXnnk9GnHB5jdBf5Y92632B0AgKcyejt9FHg34BLln13Fric0Vvkh4CD3eMa4MPAh7ua64FDjO6A3we8YwV9XtK9/oNdLyfHc22fAf64G++HgZ0r6PNsRkF87pp1Kx9LRr9ITgDfZjSv+ouM7qf8I/A48A/A+V3tTuDmNft+qDtHjwIfXEGfRxnNC588P09+EusNwN3Tzo8l9/nX3Xn3EKMw3ra+z+75KbmwzD679X958pxcU7uy8Zz34TcnJakx3pyUpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNeb/Afg7uy9hD/E/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c648a8a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c647fc278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c64832908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c646e43c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c64803630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.food.shape)\n",
    "for obs in env_obs:\n",
    "    print(obs.shape)\n",
    "for obs in agents_obs:\n",
    "    print(obs.shape)\n",
    "print(env.gamespace_width)\n",
    "print(env.gamespace_height)\n",
    "print(env.agent_locations)\n",
    "print(env.teams)\n",
    "print(env.tagged)\n",
    "\n",
    "for state in env.state_n:\n",
    "    print(state.shape)\n",
    "    plt.imshow(state[:,:,6])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Vikings\n",
      "name Vikings\n",
      "color deepskyblue\n",
      "culture {'name': 'pacifist_leadfollow', 'laser_penalty': -1.0}\n",
      "roles ['leader', 'follower']\n",
      "target_zone None\n",
      "banned_zone None\n",
      "\n",
      "\n",
      "Team Franks\n",
      "name Franks\n",
      "color red\n",
      "culture {'name': 'cooperative', 'coop_factor': 5.0}\n",
      "roles ['leader', 'follower']\n",
      "target_zone None\n",
      "banned_zone None\n",
      "\n",
      "\n",
      "Agent 0\n",
      "id 0\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 7)\n",
      "(33, 37)\n",
      "\n",
      "\n",
      "Agent 1\n",
      "id 1\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 9)\n",
      "(32, 20)\n",
      "\n",
      "\n",
      "Agent 2\n",
      "id 2\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (2, 8)\n",
      "(33, 36)\n",
      "\n",
      "\n",
      "Agent 3\n",
      "id 3\n",
      "team Vikings\n",
      "color deepskyblue\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 7)\n",
      "(32, 37)\n",
      "\n",
      "\n",
      "Agent 4\n",
      "id 4\n",
      "team Vikings\n",
      "color royalblue\n",
      "type drone\n",
      "role leader\n",
      "start (3, 9)\n",
      "(31, 33)\n",
      "\n",
      "\n",
      "Agent 5\n",
      "id 5\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 1)\n",
      "(58, 21)\n",
      "\n",
      "\n",
      "Agent 6\n",
      "id 6\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (1, 3)\n",
      "(22, 39)\n",
      "\n",
      "\n",
      "Agent 7\n",
      "id 7\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (2, 2)\n",
      "(21, 21)\n",
      "\n",
      "\n",
      "Agent 8\n",
      "id 8\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 1)\n",
      "(67, 20)\n",
      "\n",
      "\n",
      "Agent 9\n",
      "id 9\n",
      "team Franks\n",
      "color red\n",
      "type crawler\n",
      "role follower\n",
      "start (3, 3)\n",
      "(79, 23)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for team in teams_params:\n",
    "    print (\"Team {}\".format(team['name']) )\n",
    "    for k, v in team.items():\n",
    "        print (k, v)\n",
    "    print ('\\n')\n",
    "\n",
    "for i, agent in enumerate(agents_params):\n",
    "    print (\"Agent {}\".format(agent['id']) )\n",
    "    for k, v in agent.items():\n",
    "        print (k, v)\n",
    "    print (env.agent_locations[i])\n",
    "    print ('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agents with New Env\n",
    "\n",
    "The code below run training on 2 teams of 5 Agents each using the new environment.\n",
    "\n",
    "Team Viking has a Pacifist culture and they are unagressive (do not fire their lasers). Team Franks has a Cooperative culture and their agents specialize into 'shooters' and 'gatherers' even though these roles are not formally defined. Thus the Team Viking is highly disadvantaged in this situation. \n",
    "\n",
    "We will investigate how the introduction of a 'leader' can change the dynamic of this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "Learner agent 5\n",
      "Learner agent 6\n",
      "Learner agent 7\n",
      "Learner agent 8\n",
      "Learner agent 9\n",
      "..........\n",
      "Episode 10 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:4\tRunning mean: 0.04\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['267.11', '166.59', '261.05', '240.41', '272.04', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 20 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03618\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['346.21', '184.70', '220.06', '146.84', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 30 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.03272\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['139.61', '227.89', '217.11', '152.20', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 40 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02959\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['273.55', '178.43', '282.82', '220.82', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 50 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.009321\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.05472\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['133.89', '170.92', '153.37', '153.94', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 60 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.008429\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04949\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03691\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['156.10', '181.79', '154.88', '183.71', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 70 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.007623\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.04476\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.05279\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['143.55', '144.94', '131.32', '101.90', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 80 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.06282\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01137\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01883\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.04774\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['131.63', '209.25', '150.02', '137.12', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 90 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.05681\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01028\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01703\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.04317\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['140.22', '158.30', '0.00', '167.11', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 100 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.05138\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.009298\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01845\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0154\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03905\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['153.11', '161.10', '165.89', '138.34', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 110 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04647\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01851\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01669\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01393\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03531\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['163.63', '166.71', '0.00', '278.07', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 120 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04202\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:-9.0\tRunning mean: -0.08316\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.01509\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0126\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.04174\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['164.41', '183.89', '0.00', '0.00', '36.21', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..........\n",
      "Episode 130 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.03801\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: -0.07521\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.009606\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:1\tRunning mean: 0.02365\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01139\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03775\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['0.00', '155.25', '145.56', '113.22', '0.00', '234.54', '212.58', '158.95', '125.08', '154.92']\n",
      "..........\n",
      "Episode 140 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.03437\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:1\tReward total:0\tRunning mean: -0.06802\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:2\tReward total:0\tRunning mean: 0.008687\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:5\tReward total:0\tRunning mean: 0.06059\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0103\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:8\tReward total:0\tRunning mean: 0.03414\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Learner:9\tReward total:0\tRunning mean: 0.0\tNum agents crossed: 0\tRunning mean: 0.0\n",
      "Max Norms =  ['0.00', '150.45', '144.30', '130.22', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6aa2aaad1bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Perform step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0menv_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \"\"\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/leader/xteams_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBANISH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# go through the list of agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/leader/xteams_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBANISH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# go through the list of agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Initialize environment\n",
    "game = \"Crossing\"\n",
    "num_crawler_actions = 8                     # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                      # Drones are capable of 12 actions\n",
    "experiment = '2T-5L/pac_droneleader/'    # 2 team of 5 agents; a team of pacifist with a drone leader \n",
    "\n",
    "# Map and Parameter sets\n",
    "map_name = \"food_d37_river_w1_d25\"  \n",
    "parameters =[ \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300}\n",
    "            ]\n",
    "\n",
    "temp_end = 1.0   # temp parameter is annealed from the value stored in parameters['temp_start'] to 1.0 \n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 7      # environ observation consists of a list of stacked frames per agent\n",
    "max_episodes = 3000\n",
    "\n",
    "render = True    # This turns on rendering every save so that agents' behavior can be observed\n",
    "SPEED = 1/30\n",
    "second_pile_x = 50  # x-coordinate of the 2nd food pile\n",
    "\n",
    "log_interval = 10\n",
    "save_interval = 20\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   10 agents - 10 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 10\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "       \n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    start = time.clock()  # time the training\n",
    "    \n",
    "    situation = 'pac_vs_coop'\n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "    \n",
    "    # Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "    teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},\n",
    "        {'name': 'Franks', 'color': 'red', \n",
    "         'culture': {'name':'cooperative','coop_factor':5.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}\n",
    "    ]\n",
    "    agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',     \\\n",
    "         'role': 'follower', 'start': (1,7)},  # Use a different color for Leader\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9)},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8)},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7)},\n",
    "        # Leader of Team Viking is a drone and has a different color\n",
    "        {'id': 4, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (3,9)},\n",
    "        {'id': 5, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,1)},\n",
    "        {'id': 6, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,3)},\n",
    "        {'id': 7, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,2)},\n",
    "        {'id': 8, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,1)},\n",
    "        {'id': 9, 'team': 'Franks', 'color': teams_params[1]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,3)}\n",
    "    ]\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            \n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            \n",
    "            # Initialize agent policy based on type\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            elif agents_params[i]['type'] is 'drone':\n",
    "                agents.append(Drone_Policy(num_frames, num_drone_actions, i)) \n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "            \n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "        \n",
    "        # Keep track of num learners who has crossed over to the 2nd food pile\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "        running_crossed = None         # running average\n",
    "        running_crossed_hist = []   # history of running averages\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Crawler_Policy(input_channels=num_frames, num_actions=num_crawler_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "    # Attach agents to their teams\n",
    "    # 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "    teams = []\n",
    "    # Team Vikings\n",
    "    teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'], \\\n",
    "                  culture=teams_params[0]['culture'], roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0], agents[1], agents[2], agents[3], agents[4]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:5]]))\n",
    "    # Team Franks\n",
    "    teams.append(Team(name=teams_params[1]['name'],color=teams_params[1]['color'], \\\n",
    "                  culture=teams_params[1]['culture'], roles=teams_params[1]['roles'], \\\n",
    "                  agent_policies=[agents[5], agents[6], agents[7], agents[8], agents[9]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[5:10]]))\n",
    "    \n",
    "    env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_agent=0)   \n",
    "\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with Crawler_Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize reward and agents crossed counters\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        crossed = [0 for i in range(num_learners)]      # whether an agent has crossed to the 2nd food pile  \n",
    "        episode_crossed = 0                             # num learners who has crossed for an episode\n",
    "\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                actions[i], log_probs[i] = select_action(agents[i], agents_obs[i], cuda)\n",
    "                \n",
    "                # Only crawlers can fire lasers\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                        \n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with Crawler_Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            \n",
    "            load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "            \n",
    "            if render and (ep % save_interval == 0):   # render 1 episode every save\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "\n",
    "        # Keep track num of agents who gather from 2nd food pile. Note that env.consumption tracks the \n",
    "        # agent index and location of apple gathered\n",
    "        for (i, loc) in env.consumption:\n",
    "            if loc[0] > second_pile_x:   # If x-cood of gathered apple is beyond a preset value, it is\n",
    "                                         # in the 2nd pile\n",
    "                crossed[i] = 1\n",
    "        episode_crossed = sum(crossed)   # sum up the num agents who crossed to 2nd pile for the episode\n",
    "                \n",
    "        # Update reward and crossed statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "            \n",
    "        if running_crossed is None:\n",
    "            running_crossed = episode_crossed\n",
    "        running_crossed = running_crossed * 0.99 + episode_crossed * 0.01\n",
    "        running_crossed_hist.append(running_crossed)\n",
    "                \n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                verbose_str += '\\tNum agents crossed: {}'.format(episode_crossed)\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_crossed)\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(teams, agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "        \n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/' + experiment + map_name\n",
    "                results_dir = 'results/' + experiment + map_name\n",
    "\n",
    "                model_file = model_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'/{}/t{}_rp{}_{}gs/MA{}_{}.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames, \\\n",
    "                        i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    save_model(f, ep, agents[i], optimizers[i])\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "             \n",
    "            crossed_file = results_dir+'/{}/t{}_rp{}_{}gs/Crossed.p'.format(situation, \\\n",
    "                        temp_start, river_penalty, max_frames)\n",
    "            os.makedirs(os.path.dirname(crossed_file), exist_ok=True)\n",
    "            with open(crossed_file, 'wb') as f:\n",
    "                    pickle.dump(running_crossed_hist, f)\n",
    "    \n",
    "    end = time.clock()\n",
    "    print('\\nTraining time: {:.2f} min'.format((end-start)/60.0))\n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'pacifist_leadfollow', 'laser_penalty': -1.0}\n",
      "{'name': 'cooperative', 'coop_factor': 5.0}\n",
      "[Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Drone_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=1344, out_features=12, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      "), Crawler_Policy(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(16, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (action_head): Linear(in_features=384, out_features=8, bias=True)\n",
      ")]\n",
      "['crawler', 'crawler', 'crawler', 'crawler', 'drone', 'crawler', 'crawler', 'crawler', 'crawler', 'crawler']\n",
      "[0, 4, 5, 0, 2, 2, 2, 5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print (teams[0].culture)\n",
    "print (teams[1].culture)\n",
    "print (agents)\n",
    "print ([agent.type for agent in agents])\n",
    "print (actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Leader_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1, -1), (21, 24), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)]\n",
      "[(-1, -1), (21, 24), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)]\n"
     ]
    }
   ],
   "source": [
    "print(env.agents)\n",
    "print(env.spawn_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
