{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategic Teams\n",
    "\n",
    "After experimenting with a policy-gradient based leader agent in [Crossing_Leader_PG.ipynb](Crossing_Leader_PG.ipynb), we arrived at the conclusion that RL (policy-gradient or Q-learning) is not effective at strategic decision making. Specifically, we discovered that a drone leader, even with complete observation of the game space, was not able to overcome the problems of local optima and sparce reward.\n",
    "\n",
    "It is therefore necessary to seperate the behavioral aspect of a team from its strategic decision making.\n",
    "\n",
    "## Strategist Class\n",
    "\n",
    "A strategist analyzes the strategic position of the teams of agents it is responsible for directing, based on observable game space and game metrics provided by the Environment. Implemented as a black box, it outputs a Goal for each team.\n",
    "\n",
    "## Team Class\n",
    "\n",
    "The Team class has a Mission and a Culture.\n",
    "\n",
    "A team's Culture is used to shape an agent's behavior regardless of role or type. It does so by adding an \"imaginary\" behavioral reward/penalty to the reward given to the agent by the environment during training. Doing so shape the agent's policy NN so that it conforms to the cultural behaviors expected by the team.\n",
    "\n",
    "A team's Mission/Goal converts the strategist's Goal into a \"imaginary\" mission reward. This mission reward is added to the reward given to an agent by the environment and the team's Culture during training. Doing so shape the agent's policy NN so that it achieves the Goal demanded of the team. \n",
    "\n",
    "A team doles out behavioral and mission reward to its agents regardless of type or role so that it can accomplish the Goal set by its strategist.\n",
    "\n",
    "## Strategist directing multiple Teams\n",
    "\n",
    "We envision a multi-agent organization whereby:\n",
    "\n",
    "* A Strategist directs multiple Teams through Tasks/Objectives.\n",
    "\n",
    "* Each Team is made up of agents of different types (drones and crawlers) and roles (leaders and followers).\n",
    "\n",
    "* A Team uses its Culture to shape its agents' behaviors by doling out behavorial rewards during training. It uses its Mission to help its agents learn abilities to accomplish individual or group objectives by doling out mission rewards during training.\n",
    "\n",
    "* In this way, a Strategist that is optimized for strategic decision making can analyze the games space and direct multiple Teams to accomplish more complex and strategic tasks that require more than behavioral skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from xteams_env import CrossingEnv\n",
    "from xteams_model import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategist Class\n",
    "\n",
    "(Wikipedia) A strategist is responsible for the formulation and implementation of a strategy. Strategy generally involves setting goals, determining actions to achieve the goals, and mobilizing resources to execute the actions. It describes how the ends (goals) will be achieved by the means (resources).\n",
    "\n",
    "An agent belonging to the Strategist class performs the following:\n",
    "\n",
    "(1) It accepts and abdicates responsibilities for directing teams of agents\n",
    "\n",
    "(2) It receives game space and metrics from the Environment\n",
    "\n",
    "(3) It analyzes the game space and metrics to arrive at a \"strategic position\" for its teams. e.g. a topological map and/or a set of game stats\n",
    "\n",
    "(4) Based on the strategic position, it decides on a set of goals that need to be accomplished\n",
    "\n",
    "(5) It surveys its teams of agents and their location in the games space\n",
    "\n",
    "(6) For each goal, it picks the best team and assign it the goal\n",
    "\n",
    "(7) If necessary, it reorganize the teams and the agents\n",
    "\n",
    "(8) It measures the effectiveness of the teams in accomplishing the assigned goals, and whether the \"strategic position\" has improved for its teams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Game with Strategist\n",
    "\n",
    "For now, the strategist can only direct 1 team with a drone agent. It access the game space through the complete obs space of the drone agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We seperate out the models of drone leaders and crawler followers so that we can mix and match trained models\n",
    "in game plays.\n",
    "\"\"\"\n",
    "\n",
    "droneleader_PG_models = [\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_coop/t1.5_rp-1.0_300gs/\",  # model 1\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t1.5_rp-1.0_300gs/\",   # model 2 \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac/t2.0_rp-1.0_300gs/\",   # model 3  \n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples/t2.0_rp-1.0_300gs/\",   # model 4\n",
    "    \"models/2T-5L/pac_droneleader/food_d37_river_w1_d25/pac_vs_pac_apples_teamreward/t2.0_rp-1.0_300gs/\",   # model 5\n",
    "    \"models/1T-1L/pg_droneleader/food_d37_river_w1_d25/pg_droneleader_seed_7/t2.0_rp-1.0_300gs/\", # model 6\n",
    "    \"models/1T-1L/pg_droneleader/food_d37_river_w1_d25/pg_droneleader_seed_8/t2.0_rp-1.0_300gs/\", # model 7\n",
    "    \"models/1T-1L/pg_droneleader_2000/food_d37_river_w1_d25/pg_droneleader_seed_7/t2.0_rp-1.0_300gs/\", # model 8\n",
    "    \"models/1T-1L/pg_droneleader_test/food_d37_river_w1_d25/pg_droneleader_seed_7/t2.0_rp-1.0_300gs/\", # model 9\n",
    "]\n",
    "\n",
    "droneleader_PG_params = [\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 8},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "]\n",
    "\n",
    "droneleader_fc32_models = [\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc32_seed_0/t2.0_rp-1.0_300gs/\",   # model 1\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc32_seed_7/t2.0_rp-1.0_300gs/\",   # model 2\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc32_seed_54/t2.0_rp-1.0_300gs/\",  # model 3\n",
    "]\n",
    "\n",
    "droneleader_fc32_params = [\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, \\\n",
    "             'game_steps':300, 'seed': 0},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, \\\n",
    "             'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, \\\n",
    "             'game_steps':300, 'seed': 54}\n",
    "]\n",
    "\n",
    "droneleader_fc64_models = [\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc64_seed_0/t2.0_rp-1.0_300gs/\",   # model 1\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc64_seed_7/t2.0_rp-1.0_300gs/\",   # model 2\n",
    "    \"models/1T-1L/strategist/food_d37_river_w1_d25/droneleaderfc64_seed_54/t2.0_rp-1.0_300gs/\",   # model 3\n",
    "    \"models/1T-1L/strategist_big_reward/food_d37_river_w1_d25/droneleaderfc64_seed_7/t2.0_rp-1.0_300gs/\",   # model 4\n",
    "    \"models/1T-1L/strategist_big_reward/food_d37_river_w1_d25/droneleaderfc64_seed_54/t2.0_rp-1.0_300gs/\",  # model 5\n",
    "    \"models/1T-1L/strategist_big_reward/food_d37_river_w1_d25/droneleaderfc64_seed_80/t2.0_rp-1.0_300gs/\",   # model 6 \n",
    "]\n",
    "\n",
    "droneleader_fc64_params = [\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 0},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 54},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 54},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':0.5, 'game_steps':300, 'seed': 80},\n",
    "]\n",
    "\n",
    "crawler_models = [\n",
    "    # Agents trained with a static target zone\n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t2.0_rp-1.0_300gs_s1/',   # scenario=1\n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t2.0_rp-1.0_300gs_s2/',   # scenario=2\n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t2.0_rp-1.0_300gs_s3/',   # scenario=3    \n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t1.5_rp-1.0_300gs_s1/',   # scenario=4\n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t1.5_rp-1.0_300gs_s2/',   # scenario=5\n",
    "    'models/1T-10L/followers_static/food_d37/pacifist_follower/tr5.0_t1.5_rp-1.0_300gs_s3/',   # scenario=6\n",
    "    # Agents trained with a moving target zone, map = food_d37\n",
    "    \"models/1T-10L/followers_trajectory/food_d37/pacifist_follower/tr5.0_t2.0_rp-1.0_600gs_s1/\",   # scenario=7\n",
    "    \"models/1T-10L/followers_trajectory/food_d37/pacifist_follower/tr5.0_t2.0_rp-1.0_500gs_s2/\",   # scenario=8 \n",
    "    \"models/1T-10L/followers_trajectory/food_d37/pacifist_follower/tr5.0_t1.5_rp-1.0_600gs_s1/\",   # scenario=9\n",
    "    \"models/1T-10L/followers_trajectory/food_d37/pacifist_follower/tr5.0_t1.5_rp-1.0_500gs_s2/\",   # scenario=10 \n",
    "    # Agents trained using moving target zone and map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/followers_trajectory/food_d37_river_w1_d25/pacifist_follower/tr5.0_t1.5_rp-1.0_600gs_s1/\",   # scenario=11\n",
    "    \"models/1T-10L/followers_trajectory/food_d37_river_w1_d25/pacifist_follower/tr5.0_t1.5_rp-1.0_500gs_s2/\",   # scenario=12\n",
    "    \"models/1T-10L/followers_trajectory/food_d37_river_w1_d25/pacifist_follower/tr5.0_t2.0_rp-1.0_600gs_s1/\",   # scenario=13\n",
    "    \"models/1T-10L/followers_trajectory/food_d37_river_w1_d25/pacifist_follower/tr5.0_t2.0_rp-1.0_500gs_s2/\",   # scenario=14\n",
    "    \n",
    "    # Agents trained with DroneLeader_FC64 \n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_7/t2.0_tr1.0_300gs/\",   # model 15\n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_7/t2.0_tr2.0_300gs/\",   # model 16\n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_7/t2.0_tr5.0_300gs/\",   # model 17\n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_54/t2.0_tr1.0_300gs/\",   # model 18\n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_54/t2.0_tr2.0_300gs/\",   # model 19\n",
    "    \"models/1T-6L/strategist/food_d37_river_w1_d25/crawler_followers_seed_54/t2.0_tr5.0_300gs/\",   # model 20\n",
    "]\n",
    "\n",
    "crawler_params = [\n",
    "    # Agents trained with static or moving trajectories\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 1},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 2},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 3},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 1},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 2},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'set': 3},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':600, 'set': 1},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':500, 'set': 2},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':600, 'set': 1},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':500, 'set': 2},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':600, 'set': 1},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':500, 'set': 2},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':600, 'set': 1},\n",
    "    {'temp_start':1.5, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':500, 'set': 2},\n",
    "    \n",
    "    # Agents trained with DroneLeader_FC64   \n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':1.0, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'seed': 7},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':1.0, 'game_steps':300, 'seed': 54},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':2.0, 'game_steps':300, 'seed': 54},\n",
    "    {'temp_start':2.0, 'river_penalty':-1.0, 'target_reward':5.0, 'game_steps':300, 'seed': 54},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG-based DroneLeader\n",
    "\n",
    "We implement a DroneLeader using a 3-layer CNN using REINFORCE policy gradient RL algorithm. The drone leader has complete observation of the game space.\n",
    "\n",
    "we discovered that a drone leader based on very advanced NN with complete observation of the game space, is not able to overcome the problems of local optima and sparce reward. In addition, it learns a very brittle policy of always going back to the same general area of the 1st food pile, even when the map has completely changed (and the food pile is no longer at the same spot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone starts at (3, 9) in map food_d37_river_w1_d25.\n",
      "Load saved model for agent 0\n",
      "Load Drone Leader.\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Distance from Goal = 44\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "\n",
    "maps = [\n",
    "    \"food_d37_river_w1_d25\",\n",
    "    \"food_d37_river_w1_d25_v2\",\n",
    "    \"food_d37_river_w1_d25_v3\",\n",
    "    \"food_d37_river_w1_d25_v4\",\n",
    "]\n",
    "starts = [\n",
    "    (3,9),\n",
    "    (3,18),\n",
    "    (20,9),\n",
    "    (20,18),\n",
    "    (50,9),\n",
    "    (50,18),\n",
    "]\n",
    "\n",
    "# droneleader_start = starts[random.randint(0,len(starts)-1)]\n",
    "# map_name = maps[random.randint(0,len(maps)-1)]\n",
    "\n",
    "map_name = maps[0]\n",
    "droneleader_start = starts[0]\n",
    "\n",
    "print(\"Drone starts at {} in map {}.\".format(droneleader_start, map_name))\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "scenario = 15\n",
    "dir_name = droneleader_PG_models[scenario-1]\n",
    "parameter = droneleader_PG_params[scenario-1]\n",
    "episodes = 400  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 1 agents - 1 teams of 1 AI agents each and 0 random agent\n",
    "num_ai_agents = 1\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "# Scenario 26-33\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},        \n",
    "]\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': droneleader_start}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "num_goal_params = 2       # 2 parameters in Goal (delta coordinates)\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 100\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA0_{}_ep{}.p'.format(game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    print(\"Load Drone Leader.\")\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, i)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:1]]))\n",
    "\n",
    "# 5-29-2019  Strategist accepts directorship of a team\n",
    "suntzu = Strategist()\n",
    "suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = False)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "    game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "    goals, topology = suntzu.generate_goals(game_space)\n",
    "    deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                # PG_based droneleaders requires obs space as input\n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            else:    \n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])        \n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/(frame+1e-7)))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "\n",
    "# 6-02-2019 Update distance from goal for droneleader\n",
    "target_x, target_y = goals[0]\n",
    "current_x, current_y = env.agent_locations[0]\n",
    "episode_delta = abs(target_x - current_x) + abs(target_y - current_y)\n",
    "print ('Distance from Goal = {}'.format(episode_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DroneLeader_FC32\n",
    "\n",
    "For DroneLeader_FC32, different manual_seed can result in vastly different policies. For example, manual_seed(54) results in a very adaptable and general policy, while manual_seed(7) performs dismally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone starts at (50, 18) in map food_d37_river_w1_d25_v3.\n",
      "Load saved model for agent 0\n",
      "Load Drone Leader.\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Distance from Goal = 1\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "\n",
    "maps = [\n",
    "    \"food_d37_river_w1_d25\",\n",
    "    \"food_d37_river_w1_d25_v2\",\n",
    "    \"food_d37_river_w1_d25_v3\",\n",
    "    \"food_d37_river_w1_d25_v4\",\n",
    "]\n",
    "starts = [\n",
    "    (3,9),\n",
    "    (3,18),\n",
    "    (20,9),\n",
    "    (20,18),\n",
    "    (50,9),\n",
    "    (50,18),\n",
    "]\n",
    "\n",
    "droneleader_start = starts[random.randint(0,len(starts)-1)]\n",
    "map_name = maps[random.randint(0,len(maps)-1)]\n",
    "\n",
    "print(\"Drone starts at {} in map {}.\".format(droneleader_start, map_name))\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "scenario = 3\n",
    "dir_name = droneleader_fc32_models[scenario-1]\n",
    "parameter = droneleader_fc32_params[scenario-1]\n",
    "episodes = 2000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 1 agents - 1 teams of 1 AI agents each and 0 random agent\n",
    "num_ai_agents = 1\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "# Scenario 26-33\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},        \n",
    "]\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': droneleader_start}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "num_goal_params = 2       # 2 parameters in Goal (delta coordinates)\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 100\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    print(\"Load Drone Leader.\")\n",
    "                    agent = DroneLeader_FC32(num_goal_params, num_drone_actions, i)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=[agents[0]], \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:1]]))\n",
    "\n",
    "# 5-29-2019  Strategist accepts directorship of a team\n",
    "suntzu = Strategist()\n",
    "suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = False)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "    game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "    goals, topology = suntzu.generate_goals(game_space)\n",
    "    deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                # 6-02-2019 Simple droneleaders do not require obs space as input\n",
    "                actions[i], _ = select_action_strat_simple(agents[i], deltas, cuda=False)\n",
    "            else:    \n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])        \n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/(frame+1e-7)))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "\n",
    "# 6-02-2019 Update distance from goal for droneleader\n",
    "target_x, target_y = goals[0]\n",
    "current_x, current_y = env.agent_locations[0]\n",
    "episode_delta = abs(target_x - current_x) + abs(target_y - current_y)\n",
    "print ('Distance from Goal = {}'.format(episode_delta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DroneLeader_FC64\n",
    "\n",
    "DroneLeader_FC64 performs better than DroneLeader_FC32, the variance between policies trained from different manual_seed is smaller. Both manual_seed(7) and manual_seed(54) results in very adaptable and general policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone starts at (3, 18) in map food_d37_river_w1_d25_v2.\n",
      "Load saved model for agent 0\n",
      "Load Drone Leader.\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Distance from Goal = 1\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "\n",
    "maps = [\n",
    "    \"food_d37_river_w1_d25\",\n",
    "    \"food_d37_river_w1_d25_v2\",\n",
    "    \"food_d37_river_w1_d25_v3\",\n",
    "    \"food_d37_river_w1_d25_v4\",\n",
    "]\n",
    "starts = [\n",
    "    (3,9),\n",
    "    (3,18),\n",
    "    (20,9),\n",
    "    (20,18),\n",
    "    (50,9),\n",
    "    (50,18),\n",
    "]\n",
    "\n",
    "droneleader_start = starts[random.randint(0,len(starts)-1)]\n",
    "map_name = maps[random.randint(0,len(maps)-1)]\n",
    "\n",
    "print(\"Drone starts at {} in map {}.\".format(droneleader_start, map_name))\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "scenario = 6\n",
    "dir_name = droneleader_fc64_models[scenario-1]\n",
    "parameter = droneleader_fc64_params[scenario-1]\n",
    "episodes = 2000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 1 agents - 1 teams of 1 AI agents each and 0 random agent\n",
    "num_ai_agents = 1\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "# Scenario 26-33\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':parameter['target_reward']},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None},        \n",
    "]\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': droneleader_start}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "num_goal_params = 2       # 2 parameters in Goal (delta coordinates)\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 100\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                \n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                \n",
    "                # Load agent policy based on type\n",
    "                if agents_params[i]['type'] is 'crawler':\n",
    "                    agent = Crawler_Policy(num_frames, num_crawler_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'follower':\n",
    "                    agent = Drone_Policy(num_frames, num_drone_actions, i)\n",
    "                elif agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                    print(\"Load Drone Leader.\")\n",
    "                    agent = DroneLeader_FC64(num_goal_params, num_drone_actions, i)\n",
    "                else:\n",
    "                    raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f, device=device)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        if agents_params[i]['type'] is 'drone':\n",
    "            agents.append(Drone_Policy(num_frames, num_drone_actions, i))\n",
    "        elif agents_params[i]['type'] is 'crawler':\n",
    "            agents.append(Crawler_Policy(num_frames, num_crawler_actions, i))\n",
    "        else:\n",
    "            raise Exception('Invalid type for agent {}: {}'.format(i,agents_params[i]['type']))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "agents_list = [agents[0]]\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=agents_list, \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:len(agents_list)]]))\n",
    "\n",
    "# 5-29-2019  Strategist accepts directorship of a team\n",
    "suntzu = Strategist()\n",
    "suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = False)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "    game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "    goals, topology = suntzu.generate_goals(game_space)\n",
    "    deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        \n",
    "    for i in range(num_ai_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                # 6-02-2019 Simple droneleaders do not require obs space as input\n",
    "                actions[i], _ = select_action_strat_simple(agents[i], deltas, cuda=False)\n",
    "            else:    \n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])        \n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agents_params[i]['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agents_params[i]['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/(frame+1e-7)))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        print('US agents hit = {}'.format(agent_US_hits))\n",
    "        print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "\n",
    "# 6-02-2019 Update distance from goal for droneleader\n",
    "target_x, target_y = goals[0]\n",
    "current_x, current_y = env.agent_locations[0]\n",
    "episode_delta = abs(target_x - current_x) + abs(target_y - current_y)\n",
    "print ('Distance from Goal = {}'.format(episode_delta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategist - Game Space and Favorability Topology\n",
    "\n",
    "This is how the strategist see the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SunTzu is strategist for Team Vikings.\n",
      "Agent 0 (drone leader) is acting as eye for SunTzu.\n",
      "Display the Big Picture!\n",
      "torch.Size([1, 7, 100, 60])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAGfCAYAAABMVyc6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADcRJREFUeJzt3V2sZXV5x/HfU0bqazugLaEMLRhJjWkqNsRq8MJibag16oWxGJtMGxpu+kJfEoU2aeJFk5o0Khe9IWrLhfGlSAPhopYibXpTdBCswIhQ6wsEHBuh9sqW+vTirGmOFJ9z5sycvTfM55OcnL3WWfusJ+7hy1r/vRmruwPwg/zQugcANptIACORAEYiAYxEAhiJBDASCWAkEsDopCJRVZdX1QNV9VBVXXOqhgI2R+31E5dVdUaSLyd5Y5KHk3wuyTu7+/7hOT7eCRuku2unY07mSuLVSR7q7q90938l+XiSt57E7wM20MlE4rwk39i2/fCy7/tU1VVVdaSqjpzEuYA1ObDfJ+ju65Ncn7jdgGeik7mSeCTJ+du2Dy37gGeRk4nE55JcVFUXVtWZSa5IcsupGQvYFHu+3ejuJ6vqt5N8OskZST7S3fedssmAjbDnt0D3dDJrErBR9vstUOA0IBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJIDRjpGoqvOr6o6qur+q7quqq5f9Z1fVbVX14PL9rP0fF1i16u75gKpzk5zb3Z+vqhcluSvJ25L8epJvd/efVdU1Sc7q7vfs8LvmkwEr1d210zE7Xkl096Pd/fnl8X8mOZrkvCRvTXLDctgN2QoH8Cxz4EQOrqoLkrwqyZ1JzunuR5cfPZbknB/wnKuSXLX3EYF12vF24/8OrHphkn9M8qfdfVNVPdHdB7f9/PHuHtcl3G7AZjkltxtJUlXPSfKpJB/t7puW3d9c1iuOr1sc2+ugwObazbsbleTDSY529/u3/eiWJIeXx4eT3HzqxwPWbTfvbrwuyT8l+WKS7y27/yhb6xKfTPKTSb6W5B3d/e0dfpfbDdggu7nd2PWaxKkgErBZTtmaBHD6EglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGB9Y9AKvR3d+3XVVrmoRnGlcSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRj2WfJnwMm71yJQGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjHYdiao6o6rurqpbl+0Lq+rOqnqoqj5RVWfu35jAupzIlcTVSY5u235fkg9098uSPJ7kylM5GLAZdhWJqjqU5FeSfGjZriSXJblxOeSGJG/bjwGB9drtlcQHk7w7yfeW7RcneaK7n1y2H05y3tM9saquqqojVXXkpCYF1mLHSFTVm5Mc6+679nKC7r6+uy/p7kv28nxgvXbzn4pfmuQtVfWmJM9N8iNJrktysKoOLFcTh5I8sn9jAuuy45VEd1/b3Ye6+4IkVyT5THe/K8kdSd6+HHY4yc37NiWwNifzOYn3JPmDqnooW2sUHz41IwGbpJ76fyS7ryerWt3JgB11945/ZZlPXAIjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYx2FYmqOlhVN1bVl6rqaFW9tqrOrqrbqurB5ftZ+z0ssHq7vZK4LsnfdvfLk7wyydEk1yS5vbsvSnL7sg08y1R3zwdU/WiSe5K8tLcdXFUPJHl9dz9aVecm+Yfu/ukdftd8MmClurt2OmY3VxIXJvlWkr+sqrur6kNV9YIk53T3o8sxjyU55+meXFVXVdWRqjqy28GBzbGbK4lLkvxzkku7+86qui7Jd5L8Tncf3Hbc4909rku4koDNcqquJB5O8nB337ls35jk55J8c7nNyPL92F4HBTbXjpHo7seSfKOqjq83vCHJ/UluSXJ42Xc4yc37MiGwVjvebiRJVV2c5ENJzkzylSS/ka3AfDLJTyb5WpJ3dPe3d/g9bjdgg+zmdmNXkThVRAI2y6lakwBOYyIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLA6MC6B2A9uvv7tqtqTZOw6VxJACORAEYiAYysSZwmnroGsdPPrVFwnCsJYCQSwEgkgJE1iWexndYhTuS51ihOX64kgJFIACO3G89i228RTvTWw+0Fx7mSAEYiAYxEAhhZkzhNPHWNwVuc7JYrCWAkEsBIJICRNYnTlDUIdsuVBDDaVSSq6ver6r6qureqPlZVz62qC6vqzqp6qKo+UVVn7vewwOrtGImqOi/J7ya5pLt/JskZSa5I8r4kH+julyV5PMmV+zkosB67vd04kOR5VXUgyfOTPJrksiQ3Lj+/IcnbTv14wLrtGInufiTJnyf5erbi8B9J7kryRHc/uRz2cJLz9mtIYH12c7txVpK3JrkwyU8keUGSy3d7gqq6qqqOVNWRPU8JrM1u3gL9xST/1t3fSpKquinJpUkOVtWB5WriUJJHnu7J3X19kuuX5+79r0oC1mI3axJfT/Kaqnp+bb25/oYk9ye5I8nbl2MOJ7l5f0YE1ql285eRVNV7k/xqkieT3J3kN7O1BvHxJGcv+36tu7+7w+9xJQEbpLt3/FTdriJxqogEbJbdRMInLoGRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhgdWPH5/j3J15K8ZHm8acx1Ysx1YjZtrp/azUHV3fs9yP8/adWR7r5k5SfegblOjLlOzKbOtRO3G8BIJIDRuiJx/ZrOuxNznRhznZhNnWu0ljUJ4JnD7QYwWmkkquryqnqgqh6qqmtWee6nmeUjVXWsqu7dtu/sqrqtqh5cvp+14pnOr6o7qur+qrqvqq7ekLmeW1WfraovLHO9d9l/YVXdubyen6iqM1c517b5zqiqu6vq1k2Zq6q+WlVfrKp7qurIsm+tr+NerSwSVXVGkr9I8stJXpHknVX1ilWd/2n8VZLLn7LvmiS3d/dFSW5ftlfpySR/2N2vSPKaJL+1/G+07rm+m+Sy7n5lkouTXF5Vr0nyviQf6O6XJXk8yZUrnuu4q5Mc3ba9KXP9QndfvO1tz3W/jnvT3Sv5SvLaJJ/etn1tkmtXdf4fMNMFSe7dtv1AknOXx+cmeWDN892c5I2bNFeS5yf5fJKfz9YHgw483eu7wnkOZesfuMuS3JqkNmSuryZ5yVP2bczreCJfq7zdOC/JN7ZtP7zs2yTndPejy+PHkpyzrkGq6oIkr0pyZzZgruWS/p4kx5LcluRfkzzR3U8uh6zr9fxgkncn+d6y/eINmauT/F1V3VVVVy371v467sWqP5b9jNHdXVVreeunql6Y5FNJfq+7v1NVa5+ru/8nycVVdTDJ3yR5+apneKqqenOSY919V1W9ft3zPMXruvuRqvrxJLdV1Ze2/3Cdf75O1CqvJB5Jcv627UPLvk3yzao6N0mW78dWPUBVPSdbgfhod9+0KXMd191PJLkjW5fxB6vq+L9o1vF6XprkLVX11SQfz9Ytx3UbMFe6+5Hl+7FsRfXV2aDX8USsMhKfS3LRsvJ8ZpIrktyywvPvxi1JDi+PD2drTWBlauuS4cNJjnb3+zdorh9briBSVc/L1jrJ0WzF4u3rmqu7r+3uQ919Qbb+PH2mu9+17rmq6gVV9aLjj5P8UpJ7s+bXcc9WvJjzpiRfztb97B+vczEmyceSPJrkv7N133pltu5nb0/yYJK/T3L2imd6XbbuZf8lyT3L15s2YK6fTXL3Mte9Sf5k2f/SJJ9N8lCSv07yw2t8PV+f5NZNmGs5/xeWr/uO/1lf9+u41y+fuARGPnEJjEQCGIkEMBIJYCQSwEgkgJFIACORAEb/C0y6Ga1y7zpGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95f7521e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95f7477cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e86effd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAGfCAYAAABMVyc6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADcxJREFUeJzt2l2sZQdZx+H/a4fKp04L2tROsSU0EmKkmAYh5QKLmIoEuCBYxGQ0NXPjR/1IoNXEyIWJJAbohTFpAO0FoSDUtOmFWEs13jgwpUXaDqUVC8xkymBoxQuDVF4vzh5zqOXd52Pm7N32eZKTs9c6a5/1Zq8zv1lr7V3dHYDv5wdWPQCw3kQCGIkEMBIJYCQSwEgkgJFIACORAEa7ikRVXVlVD1TVQ1V17ekaClgftdNPXFbVWUm+lOQNSY4l+WySd3T3/cNzfLwT1kh317JtdnMm8aokD3X3l7v7v5PclOQtu/h9wBraTSQuSPK1TcvHFuu+R1UdqqojVXVkF/sCVmTfmd5Bd9+Q5IbE5QY8Fe3mTOJ4kgs3LR9YrAOeRnYTic8muaSqLq6qs5NcleTW0zMWsC52fLnR3Y9X1W8m+VSSs5J8uLvvO22TAWthx2+B7mhn7knAWjnTb4ECzwAiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAo32rHoDv1d2rHuFpo6pWPcLTgjMJYCQSwGhpJKrqwqq6s6rur6r7quqaxfpzq+r2qnpw8f2cMz8usNdq2TVwVZ2f5Pzu/lxVvSDJXUnemuRXk3yzu/+0qq5Nck53v3vJ73LBvcTm4+Gaevu8ftvT3UtfpKVnEt19ors/t3j8n0mOJrkgyVuS3LjY7MZshAN4mtnWuxtVdVGSVyY5nOS87j6x+NEjSc77Ps85lOTQzkcEVmnp5cb/bVj1/CT/mORPuvvmqnqsu/dv+vmj3T3el3C5sZzT5d3x+m3PabncSJKqelaSTyb5SHffvFj99cX9ilP3LU7udFBgfW3l3Y1K8qEkR7v7fZt+dGuSg4vHB5PccvrHA1ZtK+9uvDbJPyX5QpLvLlb/QTbuS3w8yYuTfCXJ27v7m0t+l8uNJZwu747Xb3u2crmx5XsSp4NILOePfHe8fttz2u5JAM9cIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhhtORJVdVZV3V1Vty2WL66qw1X1UFV9rKrOPnNjAquynTOJa5Ic3bT83iTv7+6XJnk0ydWnczBgPWwpElV1IMkvJvngYrmSXJHkE4tNbkzy1jMxILBaWz2T+ECSdyX57mL5hUke6+7HF8vHklzwZE+sqkNVdaSqjuxqUmAllkaiqt6U5GR337WTHXT3Dd19WXdftpPnA6u1bwvbXJ7kzVX1xiTPTvJDSa5Psr+q9i3OJg4kOX7mxgRWZemZRHdf190HuvuiJFcl+XR3vzPJnUnettjsYJJbztiUwMrs5nMS707ye1X1UDbuUXzo9IwErJPq7r3bWdXe7ewpavPx2HgTie3w+m1Pdy99kXziEhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYLSlSFTV/qr6RFV9saqOVtVrqurcqrq9qh5cfD/nTA8L7L2tnklcn+Rvu/tlSV6R5GiSa5Pc0d2XJLljsQw8zVR3zxtU/XCSe5K8pDdtXFUPJHldd5+oqvOT/EN3/8SS3zXvjGw+HlW1wkmemrx+29PdS1+krZxJXJzkG0n+sqrurqoPVtXzkpzX3ScW2zyS5Lwne3JVHaqqI1V1ZKuDA+tjK2cSlyX55ySXd/fhqro+ybeS/FZ379+03aPdPd6XcCaxnP8Jd8frtz2n60ziWJJj3X14sfyJJD+d5OuLy4wsvp/c6aDA+loaie5+JMnXqurU/YbXJ7k/ya1JDi7WHUxyyxmZEFippZcbSVJVlyb5YJKzk3w5ya9lIzAfT/LiJF9J8vbu/uaS3+NyYwmny7vj9duerVxubCkSp4tILOePfHe8fttzuu5JAM9gIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhhtKRJV9btVdV9V3VtVH62qZ1fVxVV1uKoeqqqPVdXZZ3pYYO8tjURVXZDkt5Nc1t0/meSsJFcleW+S93f3S5M8muTqMzkosBr7trHdc6rqO0mem+REkiuS/PLi5zcm+eMkf3G6B3wm6+5VjwDLzyS6+3iSP0vy1WzE4T+S3JXkse5+fLHZsSQXnKkhgdXZyuXGOUnekuTiJD+W5HlJrtzqDqrqUFUdqaojO54SWJmtXG78XJJ/6+5vJElV3Zzk8iT7q2rf4mziQJLjT/bk7r4hyQ2L5zp/hqeYrUTiq0leXVXPTfJfSV6f5EiSO5O8LclNSQ4mueVMDflMUlWrHgG+R23l5lhVvSfJLyV5PMndSX49G/cgbkpy7mLdr3T3t5f8HmcSsEa6e+n/SluKxOkiErBethIJn7gERiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgJBLASCSAkUgAI5EARiIBjEQCGIkEMBIJYCQSwEgkgJFIACORAEYiAYxEAhiJBDASCWAkEsBIJICRSAAjkQBGIgGMRAIYiQQwEglgtG+P9/fvSb6S5EWLx+vGXNtjru1Zt7l+fCsbVXef6UH+/06rjnT3ZXu+4yXMtT3m2p51nWsZlxvASCSA0aoiccOK9ruMubbHXNuzrnONVnJPAnjqcLkBjPY0ElV1ZVU9UFUPVdW1e7nvJ5nlw1V1sqru3bTu3Kq6vaoeXHw/Z49nurCq7qyq+6vqvqq6Zk3menZVfaaqPr+Y6z2L9RdX1eHF8fxYVZ29l3Ntmu+sqrq7qm5bl7mq6uGq+kJV3VNVRxbrVnocd2rPIlFVZyX58yS/kOTlSd5RVS/fq/0/ib9KcuUT1l2b5I7uviTJHYvlvfR4kt/v7pcneXWS31i8Rque69tJrujuVyS5NMmVVfXqJO9N8v7ufmmSR5NcvcdznXJNkqObltdlrp/t7ks3ve256uO4M929J19JXpPkU5uWr0ty3V7t//vMdFGSezctP5Dk/MXj85M8sOL5bknyhnWaK8lzk3wuyc9k44NB+57s+O7hPAey8Q/uiiS3Jak1mevhJC96wrq1OY7b+drLy40Lknxt0/Kxxbp1cl53n1g8fiTJeasapKouSvLKJIezBnMtTunvSXIyye1J/jXJY939+GKTVR3PDyR5V5LvLpZfuCZzdZK/q6q7qurQYt3Kj+NO7PXHsp8yururaiVv/VTV85N8MsnvdPe3qmrlc3X3/yS5tKr2J/mbJC/b6xmeqKrelORkd99VVa9b9TxP8NruPl5VP5rk9qr64uYfrvLva7v28kzieJILNy0fWKxbJ1+vqvOTZPH95F4PUFXPykYgPtLdN6/LXKd092NJ7szGafz+qjr1H80qjuflSd5cVQ8nuSkblxzXr8Fc6e7ji+8nsxHVV2WNjuN27GUkPpvkksWd57OTXJXk1j3c/1bcmuTg4vHBbNwT2DO1ccrwoSRHu/t9azTXjyzOIFJVz8nGfZKj2YjF21Y1V3df190HuvuibPw9fbq737nquarqeVX1glOPk/x8knuz4uO4Y3t8M+eNSb6UjevZP1zlzZgkH01yIsl3snHdenU2rmfvSPJgkr9Pcu4ez/TabFzL/kuSexZfb1yDuX4qyd2Lue5N8keL9S9J8pkkDyX56yQ/uMLj+bokt63DXIv9f37xdd+pv/VVH8edfvnEJTDyiUtgJBLASCSAkUgAI5EARiIBjEQCGIkEMPpfTX1AmsoERjoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e86b0358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e8594f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e8680828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e85dd080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Favorability Topology!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e86b00b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, 24)]\n"
     ]
    }
   ],
   "source": [
    "for team, eye in zip(suntzu.teams, suntzu.eyes):\n",
    "    print(\"SunTzu is strategist for Team {}.\".format(team.name))\n",
    "    print (\"Agent {} ({} {}) is acting as eye for SunTzu.\".format(eye.idx, eye.type, eye.role))\n",
    "\n",
    "print (\"Display the Big Picture!\")\n",
    "print (game_space.shape)\n",
    "for i in range(7):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(game_space[0,i,:,:])\n",
    "    plt.show()\n",
    "\n",
    "print (\"Display Favorability Topology!\") \n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(topology)\n",
    "plt.show()\n",
    "\n",
    "print (goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Team directed by Strategist\n",
    "\n",
    "A strategist can direct 1 team with a drone agent, which acts as the \"eye\" for the strategist. The strategist access the game space through the complete obs space of a drone agent.\n",
    "\n",
    "The code below run training on 1 teams of 1 drone leader and 5 crawler followers. The strategist takes in the game space provided by its eye (drone leader) and assign a goal to the team in the form of a target coordinate. The Team class takes this goal (\"move the team to this coordinate\") and generate the mission reward such that its drone leader learns to move to that coordinate, while taking as many of crawler followers along.\n",
    "\n",
    "The trainings of the drone leader and the crawler followers take place separately in [Drone_Leaders.ipynb](Drone_Leaders.ipynb) and [Crawler_Followers.ipynb](Crawler_Followers.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Play (1 Team with 1 Drone Leader + 5 Followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Drone Leader.\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 reward is 0\n",
      "Agent1 reward is 32\n",
      "Agent2 reward is 27\n",
      "Agent3 reward is 4\n",
      "Agent4 reward is 29\n",
      "Agent5 reward is 22\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Total rewards gathered = 114\n",
      "Av. rewards per agent = 22.80\n",
      "DroneLeader Distance from Goal = 44\n",
      "Num agents gathering from 2nd food pile: 4\n",
      "\n",
      "Statistics by Team\n",
      "===================\n",
      "Team Vikings has total reward of 114\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "game = 'Crossing'\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "# map_name \"food_d37\"\n",
    "# map_name = \"food_d37_river_w1_d25_v2\"\n",
    "\n",
    "# device = torch.device('cpu')   # for playing a game on the cpu-only laptop\n",
    "device = torch.device('cuda')   # for playing a game on the gpu-PC\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Set up parameters of agents and teams as inputs into CrossingEnv\n",
    "\n",
    "scenario = 3\n",
    "droneleader_dir = droneleader_fc64_models[scenario-1]\n",
    "scenario = 17\n",
    "# crawlers_dir = crawler_models[scenario-1]\n",
    "crawlers_dir = crawler_models[scenario-1]\n",
    "\n",
    "teams_params = [\n",
    "        {'name': 'Vikings', 'color': 'deepskyblue', \n",
    "         'culture': {'name':'pacifist_leadfollow','laser_penalty':-1.0,'target_reward':2.0},\n",
    "         'roles': ['leader','follower'],\n",
    "         'target_zone': None, 'banned_zone': None}        \n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "scenario = 17\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA1_Crossing_ep1000.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA2_Crossing_ep1000.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA3_Crossing_ep1000.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA4_Crossing_ep1000.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'}\n",
    "]\n",
    "\n",
    "agents_params = [  # Best\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'}\n",
    "]\n",
    "\n",
    "scenario = 20\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA1_Crossing_ep2000.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA2_Crossing_ep2000.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA3_Crossing_ep2000.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA4_Crossing_ep2000.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA5_Crossing_ep2000.p'}\n",
    "]\n",
    "\n",
    "scenario = 14\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA0_Crossing_ep1500.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA2_Crossing_ep1500.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA3_Crossing_ep1500.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA4_Crossing_ep1500.p'}\n",
    "]\n",
    "\n",
    "agents_params = [  # Best\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA1_Crossing_ep1500.p'}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "agents_params = [\n",
    "        {'id': 0, 'team': 'Vikings', 'color': 'royalblue', 'type': 'drone',    \\\n",
    "         'role': 'leader', 'start': (1,7), 'model': droneleader_dir+'MA0_Crossing_ep2000.p'},\n",
    "        {'id': 1, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (1,9), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 2, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (2,8), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 3, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,7), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 4, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (3,9), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'},\n",
    "        {'id': 5, 'team': 'Vikings', 'color': teams_params[0]['color'], 'type': 'crawler',    \\\n",
    "         'role': 'follower', 'start': (4,8), 'model': crawlers_dir+'MA5_Crossing_ep1000.p'}\n",
    "]\n",
    "\n",
    "num_agents = len(agents_params)\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_crawler_actions = 8                       # Crawlers are capable of 8 actions\n",
    "num_drone_actions = 12                       # Drones are capable of 12 actions\n",
    "num_goal_params = 2       # 2 parameters in Goal (delta coordinates)\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 300\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "agents= [[] for i in range(num_agents)]\n",
    "\n",
    "for i, agent_param in enumerate(agents_params):\n",
    "    model_file = agent_param['model']\n",
    "    try:\n",
    "        with open(model_file, 'rb') as f:         \n",
    "            if agents_params[i]['type'] is 'drone' and agents_params[i]['role'] is 'leader':\n",
    "                print(\"Load Drone Leader.\")\n",
    "                agent = DroneLeader_FC64(num_goal_params, num_drone_actions, i)\n",
    "            elif agents_params[i]['type'] is 'crawler':\n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                agent = Crawler_Policy(num_frames, num_crawler_actions, i)\n",
    "            else:\n",
    "                raise Exception('Unexpected agent type: {}'.format(agents_params[i]['type']))\n",
    "                    \n",
    "            optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "            # New way to save and load models - based on: \n",
    "            # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            _ = load_model(agent, optimizer, f, device=device)\n",
    "            agent.eval()\n",
    "            agents[i] = agent\n",
    "    except OSError:\n",
    "        print('Model file not found.')\n",
    "        raise\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Attach agents to their teams\n",
    "# 4-28-2019 Add roles and types to enable multi-role teams\n",
    "\n",
    "teams = []\n",
    "\n",
    "# Team Vikings\n",
    "agents_list = [agents[0], agents[1],agents[2],agents[3],agents[4],agents[5]]\n",
    "teams.append(Team(name=teams_params[0]['name'],color=teams_params[0]['color'],culture=teams_params[0]['culture'], \\\n",
    "                  roles=teams_params[0]['roles'], \\\n",
    "                  agent_policies=agents_list, \\\n",
    "                  agent_roles = [agent['role'] for agent in agents_params[0:len(agents_list)]]))\n",
    "\n",
    "# 5-29-2019  Strategist accepts directorship of a team\n",
    "suntzu = Strategist()\n",
    "suntzu.accept(teams[0])   # Strategist accepts directorship of Team Viking\n",
    "\n",
    "env = CrossingEnv(agents=agents_params, teams=teams_params, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty,  \\\n",
    "                  debug_window = True, debug_agent=1)   \n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "    game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "    goals, topology = suntzu.generate_goals(game_space)\n",
    "    deltas = calc_deltas(goals[0], env.agent_locations[0])\n",
    "        \n",
    "    for i in range(num_agents):    # Reset agent info - e.g. laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    # print (len(agents_obs))\n",
    "    # print (agents_obs[0].shape)    \n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i, agent_param in enumerate(agents_params):\n",
    "            if agent_param['type'] is 'drone' and agent_param['role'] is 'leader':\n",
    "                # 6-02-2019 Simple droneleaders do not require obs space as input\n",
    "                actions[i], _ = select_action_strat_simple(agents[i], deltas, cuda=False)\n",
    "            else:    \n",
    "                actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            \n",
    "            # Only crawlers can fire lasers\n",
    "            if agent_param['type'] is 'crawler':\n",
    "                if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "        \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, agents_params, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        # 5-29-2019 Strategist uses the obs space of its team eye as the big picture\n",
    "        game_space = agents_obs[suntzu.eyes[0].idx]\n",
    "        goals, topology = suntzu.generate_goals(game_space)\n",
    "        deltas = calc_deltas(goals[0], env.agent_locations[0])        \n",
    "        \n",
    "        for i, agent_param in enumerate(agents_params):\n",
    "            # Only crawlers can fire lasers\n",
    "            if agent_param['type'] is 'crawler':            \n",
    "                US_hits[i] += agents[i].US_hit\n",
    "                THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "         \n",
    "        total = 0\n",
    "        for i in range(num_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(SPEED)  # Change speed of video rendering        \n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i, agent_param in enumerate(agents_params):\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "    \n",
    "    # Only crawlers can fire lasers\n",
    "    if agent_param['type'] is 'crawler':     \n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        total_tags += agent_tags\n",
    "        # print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/(frame+1e-7)))\n",
    " \n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        total_US_hits += agent_US_hits\n",
    "        total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "        # print('US agents hit = {}'.format(agent_US_hits))\n",
    "        # print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "num_crawlers = sum([1 if param['type'] is 'crawler' else 0 for param in agents_params])\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_crawlers))\n",
    "# print ('Num laser fired = {}'.format(total_tags))\n",
    "# print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "# print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "# print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "# 6-02-2019 Update distance from goal for droneleader\n",
    "target_x, target_y = goals[0]\n",
    "current_x, current_y = env.agent_locations[0]\n",
    "episode_delta = abs(target_x - current_x) + abs(target_y - current_y)\n",
    "print ('DroneLeader Distance from Goal = {}'.format(episode_delta))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_team = None\n",
    "top_team_reward = 0\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    if team.name is not 'Crazies':\n",
    "        reward = sum(team.sum_rewards())\n",
    "        print ('Team {} has total reward of {}'.format(team.name, reward))\n",
    "                           \n",
    "        if reward > top_team_reward:   # Keep track of dominating team\n",
    "            top_team_reward = reward\n",
    "            top_team = team.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(teams) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_team))\n",
    "    dominance = top_team_reward/((total_rewards-top_team_reward+1.1e-7)/(len(teams)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
